{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        # MNIST中的图像默认为uint8（0-255的数字）。以下代码将其归一化到0-1之间的浮点数，并在最后增加一维作为颜色通道\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        # 从数据集中随机取出batch_size个元素并返回\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()    # Flatten层将除第一维（batch_size）以外的维度展平\n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):         # [batch_size, 28, 28, 1]\n",
    "        x = self.flatten(inputs)    # [batch_size, 784]\n",
    "        x = self.dense1(x)          # [batch_size, 100]\n",
    "        x = self.dense2(x)          # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 50\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1200/1200 [==============================] - 16s 11ms/step - loss: 0.5041 - sparse_categorical_accuracy: 0.8586\n",
      "Epoch 2/5\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1409 - sparse_categorical_accuracy: 0.9599\n",
      "Epoch 3/5\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.0921 - sparse_categorical_accuracy: 0.9733\n",
      "Epoch 4/5\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.0675 - sparse_categorical_accuracy: 0.9805\n",
      "Epoch 5/5\n",
      "1200/1200 [==============================] - 9s 8ms/step - loss: 0.0584 - sparse_categorical_accuracy: 0.9829\n"
     ]
    }
   ],
   "source": [
    "model = MLP()\n",
    "data_loader = MNISTLoader()\n",
    "model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "        metrics=[tf.keras.metrics.sparse_categorical_accuracy]\n",
    "    )\n",
    "history = model.fit(data_loader.train_data, data_loader.train_label, epochs=num_epochs, batch_size=batch_size, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 7ms/step - loss: 0.0786 - sparse_categorical_accuracy: 0.9754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07864271849393845, 0.9753999710083008]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(data_loader.test_data, data_loader.test_label)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.29623666405677795,\n",
       "  0.1310003697872162,\n",
       "  0.09184753149747849,\n",
       "  0.06998178362846375,\n",
       "  0.05615312233567238],\n",
       " 'sparse_categorical_accuracy': [0.9165499806404114,\n",
       "  0.9621333479881287,\n",
       "  0.9729666709899902,\n",
       "  0.9795833230018616,\n",
       "  0.9836000204086304]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "4/4 [==============================] - 2s 7ms/step - loss: 2.0795 - sparse_categorical_accuracy: 0.3271\n",
      "Epoch 2/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.4190 - sparse_categorical_accuracy: 0.3781\n",
      "Epoch 3/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.3234 - sparse_categorical_accuracy: 0.3523\n",
      "Epoch 4/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.2446 - sparse_categorical_accuracy: 0.3546\n",
      "Epoch 5/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.2172 - sparse_categorical_accuracy: 0.3746\n",
      "Epoch 6/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.1567 - sparse_categorical_accuracy: 0.3229\n",
      "Epoch 7/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.1293 - sparse_categorical_accuracy: 0.3454\n",
      "Epoch 8/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.1373 - sparse_categorical_accuracy: 0.3373\n",
      "Epoch 9/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.1425 - sparse_categorical_accuracy: 0.3756\n",
      "Epoch 10/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.0945 - sparse_categorical_accuracy: 0.4235\n",
      "Epoch 11/500\n",
      "4/4 [==============================] - ETA: 0s - loss: 1.0851 - sparse_categorical_accuracy: 0.343 - 0s 12ms/step - loss: 1.0759 - sparse_categorical_accuracy: 0.3831\n",
      "Epoch 12/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.0602 - sparse_categorical_accuracy: 0.4533\n",
      "Epoch 13/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.1268 - sparse_categorical_accuracy: 0.3731\n",
      "Epoch 14/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.0184 - sparse_categorical_accuracy: 0.4713\n",
      "Epoch 15/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.0640 - sparse_categorical_accuracy: 0.4763\n",
      "Epoch 16/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.1079 - sparse_categorical_accuracy: 0.3631\n",
      "Epoch 17/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.0432 - sparse_categorical_accuracy: 0.5767\n",
      "Epoch 18/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.0369 - sparse_categorical_accuracy: 0.5506\n",
      "Epoch 19/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.0549 - sparse_categorical_accuracy: 0.5683\n",
      "Epoch 20/500\n",
      "4/4 [==============================] - 1s 303ms/step - loss: 1.0163 - sparse_categorical_accuracy: 0.5773 - val_loss: 1.0532 - val_sparse_categorical_accuracy: 0.5667\n",
      "Epoch 21/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.0478 - sparse_categorical_accuracy: 0.4825\n",
      "Epoch 22/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.0326 - sparse_categorical_accuracy: 0.6258\n",
      "Epoch 23/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.9984 - sparse_categorical_accuracy: 0.7367\n",
      "Epoch 24/500\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.9968 - sparse_categorical_accuracy: 0.574 - 0s 31ms/step - loss: 1.0020 - sparse_categorical_accuracy: 0.5915\n",
      "Epoch 25/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.0786 - sparse_categorical_accuracy: 0.5456\n",
      "Epoch 26/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9716 - sparse_categorical_accuracy: 0.6875\n",
      "Epoch 27/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9673 - sparse_categorical_accuracy: 0.6650\n",
      "Epoch 28/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9635 - sparse_categorical_accuracy: 0.5715\n",
      "Epoch 29/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9817 - sparse_categorical_accuracy: 0.5942\n",
      "Epoch 30/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.0068 - sparse_categorical_accuracy: 0.5606\n",
      "Epoch 31/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9808 - sparse_categorical_accuracy: 0.6752\n",
      "Epoch 32/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9572 - sparse_categorical_accuracy: 0.7677\n",
      "Epoch 33/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8957 - sparse_categorical_accuracy: 0.7563\n",
      "Epoch 34/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.9088 - sparse_categorical_accuracy: 0.7175\n",
      "Epoch 35/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.0670 - sparse_categorical_accuracy: 0.5337\n",
      "Epoch 36/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0803 - sparse_categorical_accuracy: 0.6892\n",
      "Epoch 37/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.0369 - sparse_categorical_accuracy: 0.6646\n",
      "Epoch 38/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9363 - sparse_categorical_accuracy: 0.8294\n",
      "Epoch 39/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9514 - sparse_categorical_accuracy: 0.6240\n",
      "Epoch 40/500\n",
      "4/4 [==============================] - 0s 58ms/step - loss: 0.9475 - sparse_categorical_accuracy: 0.8712 - val_loss: 1.1449 - val_sparse_categorical_accuracy: 0.3667\n",
      "Epoch 41/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.1000 - sparse_categorical_accuracy: 0.5540\n",
      "Epoch 42/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8879 - sparse_categorical_accuracy: 0.8081\n",
      "Epoch 43/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8780 - sparse_categorical_accuracy: 0.7185\n",
      "Epoch 44/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9395 - sparse_categorical_accuracy: 0.7856\n",
      "Epoch 45/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8953 - sparse_categorical_accuracy: 0.7944\n",
      "Epoch 46/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.0197 - sparse_categorical_accuracy: 0.6587\n",
      "Epoch 47/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.9313 - sparse_categorical_accuracy: 0.7498\n",
      "Epoch 48/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8849 - sparse_categorical_accuracy: 0.8973\n",
      "Epoch 49/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0343 - sparse_categorical_accuracy: 0.8410\n",
      "Epoch 50/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9245 - sparse_categorical_accuracy: 0.8425\n",
      "Epoch 51/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.1649 - sparse_categorical_accuracy: 0.4779\n",
      "Epoch 52/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.1331 - sparse_categorical_accuracy: 0.7642\n",
      "Epoch 53/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9107 - sparse_categorical_accuracy: 0.8067\n",
      "Epoch 54/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.9343 - sparse_categorical_accuracy: 0.7865\n",
      "Epoch 55/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.8611 - sparse_categorical_accuracy: 0.8144\n",
      "Epoch 56/500\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.8554 - sparse_categorical_accuracy: 0.8525\n",
      "Epoch 57/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.8523 - sparse_categorical_accuracy: 0.9065\n",
      "Epoch 58/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9064 - sparse_categorical_accuracy: 0.7275\n",
      "Epoch 59/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.8546 - sparse_categorical_accuracy: 0.8102\n",
      "Epoch 60/500\n",
      "4/4 [==============================] - 0s 60ms/step - loss: 0.9855 - sparse_categorical_accuracy: 0.6902 - val_loss: 0.8584 - val_sparse_categorical_accuracy: 0.9000\n",
      "Epoch 61/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.8665 - sparse_categorical_accuracy: 0.8938\n",
      "Epoch 62/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.9940 - sparse_categorical_accuracy: 0.6985\n",
      "Epoch 63/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9756 - sparse_categorical_accuracy: 0.8535\n",
      "Epoch 64/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.9565 - sparse_categorical_accuracy: 0.7033\n",
      "Epoch 65/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8355 - sparse_categorical_accuracy: 0.9612\n",
      "Epoch 66/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8573 - sparse_categorical_accuracy: 0.8517\n",
      "Epoch 67/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.8756 - sparse_categorical_accuracy: 0.6996\n",
      "Epoch 68/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9662 - sparse_categorical_accuracy: 0.7581\n",
      "Epoch 69/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8615 - sparse_categorical_accuracy: 0.8994\n",
      "Epoch 70/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8342 - sparse_categorical_accuracy: 0.9229\n",
      "Epoch 71/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9803 - sparse_categorical_accuracy: 0.6015\n",
      "Epoch 72/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.0555 - sparse_categorical_accuracy: 0.7583\n",
      "Epoch 73/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8529 - sparse_categorical_accuracy: 0.8737\n",
      "Epoch 74/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.1012 - sparse_categorical_accuracy: 0.6963\n",
      "Epoch 75/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.0701 - sparse_categorical_accuracy: 0.6579\n",
      "Epoch 76/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8281 - sparse_categorical_accuracy: 0.9081\n",
      "Epoch 77/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.0649 - sparse_categorical_accuracy: 0.7302\n",
      "Epoch 78/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8665 - sparse_categorical_accuracy: 0.8904\n",
      "Epoch 79/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8319 - sparse_categorical_accuracy: 0.8269\n",
      "Epoch 80/500\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 0.8672 - sparse_categorical_accuracy: 0.7385 - val_loss: 0.9682 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 81/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8839 - sparse_categorical_accuracy: 0.8485\n",
      "Epoch 82/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7778 - sparse_categorical_accuracy: 0.9212\n",
      "Epoch 83/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.9131 - sparse_categorical_accuracy: 0.8479\n",
      "Epoch 84/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7857 - sparse_categorical_accuracy: 0.8656\n",
      "Epoch 85/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8828 - sparse_categorical_accuracy: 0.8119\n",
      "Epoch 86/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.1622 - sparse_categorical_accuracy: 0.6992\n",
      "Epoch 87/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.9376 - sparse_categorical_accuracy: 0.8554\n",
      "Epoch 88/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8427 - sparse_categorical_accuracy: 0.9094\n",
      "Epoch 89/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.1060 - sparse_categorical_accuracy: 0.6704\n",
      "Epoch 90/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9039 - sparse_categorical_accuracy: 0.8571\n",
      "Epoch 91/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8908 - sparse_categorical_accuracy: 0.8013\n",
      "Epoch 92/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8420 - sparse_categorical_accuracy: 0.9063\n",
      "Epoch 93/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7437 - sparse_categorical_accuracy: 0.9185\n",
      "Epoch 94/500\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.8718 - sparse_categorical_accuracy: 0.843 - 0s 7ms/step - loss: 0.8414 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 95/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8364 - sparse_categorical_accuracy: 0.8194\n",
      "Epoch 96/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7620 - sparse_categorical_accuracy: 0.9056\n",
      "Epoch 97/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.8552 - sparse_categorical_accuracy: 0.7592\n",
      "Epoch 98/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.8229 - sparse_categorical_accuracy: 0.8933\n",
      "Epoch 99/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8536 - sparse_categorical_accuracy: 0.9087\n",
      "Epoch 100/500\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 0.9258 - sparse_categorical_accuracy: 0.7731 - val_loss: 0.8577 - val_sparse_categorical_accuracy: 0.9000\n",
      "Epoch 101/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7758 - sparse_categorical_accuracy: 0.9381\n",
      "Epoch 102/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7331 - sparse_categorical_accuracy: 0.9435\n",
      "Epoch 103/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8748 - sparse_categorical_accuracy: 0.8810\n",
      "Epoch 104/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.0477 - sparse_categorical_accuracy: 0.8490\n",
      "Epoch 105/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7929 - sparse_categorical_accuracy: 0.9054\n",
      "Epoch 106/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8262 - sparse_categorical_accuracy: 0.8735\n",
      "Epoch 107/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7687 - sparse_categorical_accuracy: 0.9527\n",
      "Epoch 108/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8310 - sparse_categorical_accuracy: 0.8521\n",
      "Epoch 109/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.9428 - sparse_categorical_accuracy: 0.7927\n",
      "Epoch 110/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0410 - sparse_categorical_accuracy: 0.7273\n",
      "Epoch 111/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8227 - sparse_categorical_accuracy: 0.9233\n",
      "Epoch 112/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.1379 - sparse_categorical_accuracy: 0.7027\n",
      "Epoch 113/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8658 - sparse_categorical_accuracy: 0.9025\n",
      "Epoch 114/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7560 - sparse_categorical_accuracy: 0.9737\n",
      "Epoch 115/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7511 - sparse_categorical_accuracy: 0.9240\n",
      "Epoch 116/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7809 - sparse_categorical_accuracy: 0.9329\n",
      "Epoch 117/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7290 - sparse_categorical_accuracy: 0.8867\n",
      "Epoch 118/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8548 - sparse_categorical_accuracy: 0.8010\n",
      "Epoch 119/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8453 - sparse_categorical_accuracy: 0.8492\n",
      "Epoch 120/500\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.8652 - sparse_categorical_accuracy: 0.8087 - val_loss: 0.9924 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 121/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.9049 - sparse_categorical_accuracy: 0.8381\n",
      "Epoch 122/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7568 - sparse_categorical_accuracy: 0.9385\n",
      "Epoch 123/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7801 - sparse_categorical_accuracy: 0.8806\n",
      "Epoch 124/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7546 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 125/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8280 - sparse_categorical_accuracy: 0.8375\n",
      "Epoch 126/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.0800 - sparse_categorical_accuracy: 0.6419\n",
      "Epoch 127/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.1079 - sparse_categorical_accuracy: 0.7779\n",
      "Epoch 128/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8730 - sparse_categorical_accuracy: 0.8421\n",
      "Epoch 129/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7696 - sparse_categorical_accuracy: 0.9733\n",
      "Epoch 130/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7946 - sparse_categorical_accuracy: 0.9004\n",
      "Epoch 131/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6889 - sparse_categorical_accuracy: 0.9804\n",
      "Epoch 132/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8156 - sparse_categorical_accuracy: 0.8898\n",
      "Epoch 133/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7979 - sparse_categorical_accuracy: 0.8675\n",
      "Epoch 134/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.9787 - sparse_categorical_accuracy: 0.7846\n",
      "Epoch 135/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 6ms/step - loss: 0.8264 - sparse_categorical_accuracy: 0.8996\n",
      "Epoch 136/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.0886 - sparse_categorical_accuracy: 0.8202\n",
      "Epoch 137/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7782 - sparse_categorical_accuracy: 0.9535\n",
      "Epoch 138/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7658 - sparse_categorical_accuracy: 0.9633\n",
      "Epoch 139/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7688 - sparse_categorical_accuracy: 0.9075\n",
      "Epoch 140/500\n",
      "4/4 [==============================] - 0s 56ms/step - loss: 0.6913 - sparse_categorical_accuracy: 0.9196 - val_loss: 0.8083 - val_sparse_categorical_accuracy: 0.9333\n",
      "Epoch 141/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8830 - sparse_categorical_accuracy: 0.8123\n",
      "Epoch 142/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7201 - sparse_categorical_accuracy: 0.9502\n",
      "Epoch 143/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7211 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 144/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7891 - sparse_categorical_accuracy: 0.8944\n",
      "Epoch 145/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7944 - sparse_categorical_accuracy: 0.9427\n",
      "Epoch 146/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8554 - sparse_categorical_accuracy: 0.8227\n",
      "Epoch 147/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.0637 - sparse_categorical_accuracy: 0.7367\n",
      "Epoch 148/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8155 - sparse_categorical_accuracy: 0.9619\n",
      "Epoch 149/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7427 - sparse_categorical_accuracy: 0.9223\n",
      "Epoch 150/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7798 - sparse_categorical_accuracy: 0.9058\n",
      "Epoch 151/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7379 - sparse_categorical_accuracy: 0.9444\n",
      "Epoch 152/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8902 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 153/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.2864 - sparse_categorical_accuracy: 0.6631\n",
      "Epoch 154/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7103 - sparse_categorical_accuracy: 0.9762\n",
      "Epoch 155/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9155 - sparse_categorical_accuracy: 0.8460\n",
      "Epoch 156/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7652 - sparse_categorical_accuracy: 0.9762\n",
      "Epoch 157/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.2442 - sparse_categorical_accuracy: 0.6369\n",
      "Epoch 158/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7281 - sparse_categorical_accuracy: 0.9860\n",
      "Epoch 159/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7208 - sparse_categorical_accuracy: 0.9892\n",
      "Epoch 160/500\n",
      "4/4 [==============================] - 0s 72ms/step - loss: 0.6971 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.7630 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 161/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7569 - sparse_categorical_accuracy: 0.9448\n",
      "Epoch 162/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.9183 - sparse_categorical_accuracy: 0.8358\n",
      "Epoch 163/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7314 - sparse_categorical_accuracy: 0.9594\n",
      "Epoch 164/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7356 - sparse_categorical_accuracy: 0.8904\n",
      "Epoch 165/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8150 - sparse_categorical_accuracy: 0.9390\n",
      "Epoch 166/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7808 - sparse_categorical_accuracy: 0.9529\n",
      "Epoch 167/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7394 - sparse_categorical_accuracy: 0.9456\n",
      "Epoch 168/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7409 - sparse_categorical_accuracy: 0.9208\n",
      "Epoch 169/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6586 - sparse_categorical_accuracy: 0.9621\n",
      "Epoch 170/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8059 - sparse_categorical_accuracy: 0.8829\n",
      "Epoch 171/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7206 - sparse_categorical_accuracy: 0.9475\n",
      "Epoch 172/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.0008 - sparse_categorical_accuracy: 0.7962\n",
      "Epoch 173/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.4331 - sparse_categorical_accuracy: 0.6419\n",
      "Epoch 174/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7717 - sparse_categorical_accuracy: 0.9713\n",
      "Epoch 175/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7609 - sparse_categorical_accuracy: 0.9765\n",
      "Epoch 176/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7571 - sparse_categorical_accuracy: 0.9679\n",
      "Epoch 177/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7894 - sparse_categorical_accuracy: 0.9154\n",
      "Epoch 178/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9113 - sparse_categorical_accuracy: 0.8512\n",
      "Epoch 179/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.1747 - sparse_categorical_accuracy: 0.6746\n",
      "Epoch 180/500\n",
      "4/4 [==============================] - 0s 60ms/step - loss: 0.8264 - sparse_categorical_accuracy: 0.9469 - val_loss: 0.8820 - val_sparse_categorical_accuracy: 0.8333\n",
      "Epoch 181/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7623 - sparse_categorical_accuracy: 0.9415\n",
      "Epoch 182/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7684 - sparse_categorical_accuracy: 0.9658\n",
      "Epoch 183/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7271 - sparse_categorical_accuracy: 0.9623\n",
      "Epoch 184/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7799 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 185/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.6653 - sparse_categorical_accuracy: 0.9565\n",
      "Epoch 186/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7702 - sparse_categorical_accuracy: 0.9362\n",
      "Epoch 187/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8263 - sparse_categorical_accuracy: 0.8040\n",
      "Epoch 188/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7301 - sparse_categorical_accuracy: 0.9350\n",
      "Epoch 189/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8135 - sparse_categorical_accuracy: 0.9129\n",
      "Epoch 190/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.4031 - sparse_categorical_accuracy: 0.7362\n",
      "Epoch 191/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7922 - sparse_categorical_accuracy: 0.9298\n",
      "Epoch 192/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.9174 - sparse_categorical_accuracy: 0.8515\n",
      "Epoch 193/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7480 - sparse_categorical_accuracy: 0.9806\n",
      "Epoch 194/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7771 - sparse_categorical_accuracy: 0.9469\n",
      "Epoch 195/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9943 - sparse_categorical_accuracy: 0.8285\n",
      "Epoch 196/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7191 - sparse_categorical_accuracy: 0.9742\n",
      "Epoch 197/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7479 - sparse_categorical_accuracy: 0.9360\n",
      "Epoch 198/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.0957 - sparse_categorical_accuracy: 0.6962\n",
      "Epoch 199/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8484 - sparse_categorical_accuracy: 0.8277\n",
      "Epoch 200/500\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 0.7689 - sparse_categorical_accuracy: 0.9881 - val_loss: 0.7749 - val_sparse_categorical_accuracy: 0.9333\n",
      "Epoch 201/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7754 - sparse_categorical_accuracy: 0.8965\n",
      "Epoch 202/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 6ms/step - loss: 0.8224 - sparse_categorical_accuracy: 0.9217\n",
      "Epoch 203/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.1346 - sparse_categorical_accuracy: 0.7183\n",
      "Epoch 204/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.8069 - sparse_categorical_accuracy: 0.9767\n",
      "Epoch 205/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7373 - sparse_categorical_accuracy: 0.9704\n",
      "Epoch 206/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7133 - sparse_categorical_accuracy: 0.9540\n",
      "Epoch 207/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.0050 - sparse_categorical_accuracy: 0.7760\n",
      "Epoch 208/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7559 - sparse_categorical_accuracy: 0.9485\n",
      "Epoch 209/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7712 - sparse_categorical_accuracy: 0.9335\n",
      "Epoch 210/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9160 - sparse_categorical_accuracy: 0.8435\n",
      "Epoch 211/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7651 - sparse_categorical_accuracy: 0.9765\n",
      "Epoch 212/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7635 - sparse_categorical_accuracy: 0.9269\n",
      "Epoch 213/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7630 - sparse_categorical_accuracy: 0.9392\n",
      "Epoch 214/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7095 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 215/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7183 - sparse_categorical_accuracy: 0.9452\n",
      "Epoch 216/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8468 - sparse_categorical_accuracy: 0.8560\n",
      "Epoch 217/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8815 - sparse_categorical_accuracy: 0.9079\n",
      "Epoch 218/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7304 - sparse_categorical_accuracy: 0.8929\n",
      "Epoch 219/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.9302 - sparse_categorical_accuracy: 0.8648\n",
      "Epoch 220/500\n",
      "4/4 [==============================] - 0s 55ms/step - loss: 0.7990 - sparse_categorical_accuracy: 0.9479 - val_loss: 0.7880 - val_sparse_categorical_accuracy: 0.9000\n",
      "Epoch 221/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7275 - sparse_categorical_accuracy: 0.9758\n",
      "Epoch 222/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.9868 - sparse_categorical_accuracy: 0.7806\n",
      "Epoch 223/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8408 - sparse_categorical_accuracy: 0.8888\n",
      "Epoch 224/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7139 - sparse_categorical_accuracy: 0.9496\n",
      "Epoch 225/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8106 - sparse_categorical_accuracy: 0.9196\n",
      "Epoch 226/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7597 - sparse_categorical_accuracy: 0.9744\n",
      "Epoch 227/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6484 - sparse_categorical_accuracy: 0.9571\n",
      "Epoch 228/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.8209 - sparse_categorical_accuracy: 0.8715\n",
      "Epoch 229/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8794 - sparse_categorical_accuracy: 0.8871\n",
      "Epoch 230/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7781 - sparse_categorical_accuracy: 0.9454\n",
      "Epoch 231/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.9973 - sparse_categorical_accuracy: 0.8058\n",
      "Epoch 232/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7741 - sparse_categorical_accuracy: 0.8813\n",
      "Epoch 233/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7529 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 234/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7121 - sparse_categorical_accuracy: 0.9731\n",
      "Epoch 235/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7305 - sparse_categorical_accuracy: 0.9775\n",
      "Epoch 236/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7523 - sparse_categorical_accuracy: 0.9473\n",
      "Epoch 237/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8000 - sparse_categorical_accuracy: 0.8744\n",
      "Epoch 238/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7258 - sparse_categorical_accuracy: 0.9792\n",
      "Epoch 239/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8064 - sparse_categorical_accuracy: 0.9390\n",
      "Epoch 240/500\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 0.7116 - sparse_categorical_accuracy: 0.9192 - val_loss: 0.7999 - val_sparse_categorical_accuracy: 0.9333\n",
      "Epoch 241/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7464 - sparse_categorical_accuracy: 0.9617\n",
      "Epoch 242/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6907 - sparse_categorical_accuracy: 0.9596\n",
      "Epoch 243/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.9136 - sparse_categorical_accuracy: 0.8608\n",
      "Epoch 244/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.2243 - sparse_categorical_accuracy: 0.7658\n",
      "Epoch 245/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7397 - sparse_categorical_accuracy: 0.9627\n",
      "Epoch 246/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7760 - sparse_categorical_accuracy: 0.9675\n",
      "Epoch 247/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.4619 - sparse_categorical_accuracy: 0.6906\n",
      "Epoch 248/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8469 - sparse_categorical_accuracy: 0.9519\n",
      "Epoch 249/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7817 - sparse_categorical_accuracy: 0.9325\n",
      "Epoch 250/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.9626 - sparse_categorical_accuracy: 0.8629\n",
      "Epoch 251/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8125 - sparse_categorical_accuracy: 0.8704\n",
      "Epoch 252/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7265 - sparse_categorical_accuracy: 0.9708\n",
      "Epoch 253/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8196 - sparse_categorical_accuracy: 0.9356\n",
      "Epoch 254/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7072 - sparse_categorical_accuracy: 0.9671\n",
      "Epoch 255/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.9289 - sparse_categorical_accuracy: 0.8123\n",
      "Epoch 256/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7720 - sparse_categorical_accuracy: 0.9623\n",
      "Epoch 257/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9759 - sparse_categorical_accuracy: 0.8058\n",
      "Epoch 258/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7199 - sparse_categorical_accuracy: 0.9681\n",
      "Epoch 259/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7088 - sparse_categorical_accuracy: 0.9740\n",
      "Epoch 260/500\n",
      "4/4 [==============================] - 0s 78ms/step - loss: 0.7341 - sparse_categorical_accuracy: 0.9321 - val_loss: 0.8993 - val_sparse_categorical_accuracy: 0.8333\n",
      "Epoch 261/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0878 - sparse_categorical_accuracy: 0.7398\n",
      "Epoch 262/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7444 - sparse_categorical_accuracy: 0.9819\n",
      "Epoch 263/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7191 - sparse_categorical_accuracy: 0.9604\n",
      "Epoch 264/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7128 - sparse_categorical_accuracy: 0.9698\n",
      "Epoch 265/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7902 - sparse_categorical_accuracy: 0.9542\n",
      "Epoch 266/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7327 - sparse_categorical_accuracy: 0.9498\n",
      "Epoch 267/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7743 - sparse_categorical_accuracy: 0.8752\n",
      "Epoch 268/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7259 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 269/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6543 - sparse_categorical_accuracy: 0.9633\n",
      "Epoch 270/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6930 - sparse_categorical_accuracy: 0.9617\n",
      "Epoch 271/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6345 - sparse_categorical_accuracy: 0.9188\n",
      "Epoch 272/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.6764 - sparse_categorical_accuracy: 0.9612\n",
      "Epoch 273/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9204 - sparse_categorical_accuracy: 0.8260\n",
      "Epoch 274/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.1833 - sparse_categorical_accuracy: 0.7558\n",
      "Epoch 275/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8702 - sparse_categorical_accuracy: 0.9044\n",
      "Epoch 276/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7468 - sparse_categorical_accuracy: 0.9394\n",
      "Epoch 277/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7814 - sparse_categorical_accuracy: 0.8858\n",
      "Epoch 278/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6986 - sparse_categorical_accuracy: 0.9637\n",
      "Epoch 279/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.8000 - sparse_categorical_accuracy: 0.9210\n",
      "Epoch 280/500\n",
      "4/4 [==============================] - 0s 59ms/step - loss: 0.9262 - sparse_categorical_accuracy: 0.8237 - val_loss: 0.8252 - val_sparse_categorical_accuracy: 0.9000\n",
      "Epoch 281/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8324 - sparse_categorical_accuracy: 0.8902\n",
      "Epoch 282/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7212 - sparse_categorical_accuracy: 0.9648\n",
      "Epoch 283/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7588 - sparse_categorical_accuracy: 0.9138\n",
      "Epoch 284/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7293 - sparse_categorical_accuracy: 0.9494\n",
      "Epoch 285/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.2319 - sparse_categorical_accuracy: 0.7135\n",
      "Epoch 286/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7655 - sparse_categorical_accuracy: 0.9035\n",
      "Epoch 287/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.7157 - sparse_categorical_accuracy: 0.9858\n",
      "Epoch 288/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.7218 - sparse_categorical_accuracy: 0.9731\n",
      "Epoch 289/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.8269 - sparse_categorical_accuracy: 0.9267\n",
      "Epoch 290/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.9113 - sparse_categorical_accuracy: 0.8840\n",
      "Epoch 291/500\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.9700 - sparse_categorical_accuracy: 0.968 - 0s 8ms/step - loss: 0.7750 - sparse_categorical_accuracy: 0.9648\n",
      "Epoch 292/500\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.7818 - sparse_categorical_accuracy: 0.9317\n",
      "Epoch 293/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.7971 - sparse_categorical_accuracy: 0.9619\n",
      "Epoch 294/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.1935 - sparse_categorical_accuracy: 0.6098\n",
      "Epoch 295/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.8358 - sparse_categorical_accuracy: 0.9690\n",
      "Epoch 296/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7851 - sparse_categorical_accuracy: 0.9721\n",
      "Epoch 297/500\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.8166 - sparse_categorical_accuracy: 0.8971\n",
      "Epoch 298/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.2564 - sparse_categorical_accuracy: 0.6902\n",
      "Epoch 299/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8640 - sparse_categorical_accuracy: 0.9277\n",
      "Epoch 300/500\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 0.8031 - sparse_categorical_accuracy: 0.9573 - val_loss: 0.8936 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 301/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8496 - sparse_categorical_accuracy: 0.8631\n",
      "Epoch 302/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8719 - sparse_categorical_accuracy: 0.8887\n",
      "Epoch 303/500\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.8804 - sparse_categorical_accuracy: 0.9150\n",
      "Epoch 304/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.8317 - sparse_categorical_accuracy: 0.9517\n",
      "Epoch 305/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.8327 - sparse_categorical_accuracy: 0.9679\n",
      "Epoch 306/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8358 - sparse_categorical_accuracy: 0.9552\n",
      "Epoch 307/500\n",
      "4/4 [==============================] - ETA: 0s - loss: 1.0148 - sparse_categorical_accuracy: 0.968 - 0s 8ms/step - loss: 0.8328 - sparse_categorical_accuracy: 0.9679\n",
      "Epoch 308/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.7617 - sparse_categorical_accuracy: 0.9552\n",
      "Epoch 309/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.8189 - sparse_categorical_accuracy: 0.8719\n",
      "Epoch 310/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.7752 - sparse_categorical_accuracy: 0.9812\n",
      "Epoch 311/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9617 - sparse_categorical_accuracy: 0.8106\n",
      "Epoch 312/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8157 - sparse_categorical_accuracy: 0.8754\n",
      "Epoch 313/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7615 - sparse_categorical_accuracy: 0.9454\n",
      "Epoch 314/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.0535 - sparse_categorical_accuracy: 0.7971\n",
      "Epoch 315/500\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 1.0967 - sparse_categorical_accuracy: 0.6573\n",
      "Epoch 316/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.9075 - sparse_categorical_accuracy: 0.8725\n",
      "Epoch 317/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8709 - sparse_categorical_accuracy: 0.9569\n",
      "Epoch 318/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.8189 - sparse_categorical_accuracy: 0.9565\n",
      "Epoch 319/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.0461 - sparse_categorical_accuracy: 0.6673\n",
      "Epoch 320/500\n",
      "4/4 [==============================] - 0s 99ms/step - loss: 0.8233 - sparse_categorical_accuracy: 0.9721 - val_loss: 0.8334 - val_sparse_categorical_accuracy: 0.9667\n",
      "Epoch 321/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8175 - sparse_categorical_accuracy: 0.9773\n",
      "Epoch 322/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7647 - sparse_categorical_accuracy: 0.9827\n",
      "Epoch 323/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7783 - sparse_categorical_accuracy: 0.9710\n",
      "Epoch 324/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8123 - sparse_categorical_accuracy: 0.9650\n",
      "Epoch 325/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.8812 - sparse_categorical_accuracy: 0.8721\n",
      "Epoch 326/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9208 - sparse_categorical_accuracy: 0.8112\n",
      "Epoch 327/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7498 - sparse_categorical_accuracy: 0.9775\n",
      "Epoch 328/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8448 - sparse_categorical_accuracy: 0.8981\n",
      "Epoch 329/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7964 - sparse_categorical_accuracy: 0.9794\n",
      "Epoch 330/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7463 - sparse_categorical_accuracy: 0.9806\n",
      "Epoch 331/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8921 - sparse_categorical_accuracy: 0.8735\n",
      "Epoch 332/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.9232 - sparse_categorical_accuracy: 0.8100\n",
      "Epoch 333/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.0688 - sparse_categorical_accuracy: 0.7260\n",
      "Epoch 334/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8207 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 335/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7195 - sparse_categorical_accuracy: 0.9485\n",
      "Epoch 336/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7317 - sparse_categorical_accuracy: 0.9650\n",
      "Epoch 337/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7649 - sparse_categorical_accuracy: 0.9612\n",
      "Epoch 338/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.4910 - sparse_categorical_accuracy: 0.6179\n",
      "Epoch 339/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7925 - sparse_categorical_accuracy: 0.8410\n",
      "Epoch 340/500\n",
      "4/4 [==============================] - 0s 58ms/step - loss: 0.8502 - sparse_categorical_accuracy: 0.9283 - val_loss: 0.8189 - val_sparse_categorical_accuracy: 0.9667\n",
      "Epoch 341/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.8852 - sparse_categorical_accuracy: 0.8733\n",
      "Epoch 342/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7839 - sparse_categorical_accuracy: 0.9654\n",
      "Epoch 343/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.8250 - sparse_categorical_accuracy: 0.9637\n",
      "Epoch 344/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8167 - sparse_categorical_accuracy: 0.9612\n",
      "Epoch 345/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7925 - sparse_categorical_accuracy: 0.9173\n",
      "Epoch 346/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7387 - sparse_categorical_accuracy: 0.9752\n",
      "Epoch 347/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8042 - sparse_categorical_accuracy: 0.9721\n",
      "Epoch 348/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8365 - sparse_categorical_accuracy: 0.9204\n",
      "Epoch 349/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7073 - sparse_categorical_accuracy: 0.9752\n",
      "Epoch 350/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8296 - sparse_categorical_accuracy: 0.9185\n",
      "Epoch 351/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.1759 - sparse_categorical_accuracy: 0.6925\n",
      "Epoch 352/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7958 - sparse_categorical_accuracy: 0.9258\n",
      "Epoch 353/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9619 - sparse_categorical_accuracy: 0.7929\n",
      "Epoch 354/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7585 - sparse_categorical_accuracy: 0.9848\n",
      "Epoch 355/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8248 - sparse_categorical_accuracy: 0.9223\n",
      "Epoch 356/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.8220 - sparse_categorical_accuracy: 0.9531\n",
      "Epoch 357/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8558 - sparse_categorical_accuracy: 0.9679\n",
      "Epoch 358/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.7561 - sparse_categorical_accuracy: 0.9644\n",
      "Epoch 359/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.9712 - sparse_categorical_accuracy: 0.8108\n",
      "Epoch 360/500\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 0.7495 - sparse_categorical_accuracy: 0.9892 - val_loss: 0.7730 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 361/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7407 - sparse_categorical_accuracy: 0.9744\n",
      "Epoch 362/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7642 - sparse_categorical_accuracy: 0.9798\n",
      "Epoch 363/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7480 - sparse_categorical_accuracy: 0.9467\n",
      "Epoch 364/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.9915 - sparse_categorical_accuracy: 0.8237\n",
      "Epoch 365/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.8198 - sparse_categorical_accuracy: 0.9215\n",
      "Epoch 366/500\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.6962 - sparse_categorical_accuracy: 0.968 - 0s 10ms/step - loss: 0.7203 - sparse_categorical_accuracy: 0.9798\n",
      "Epoch 367/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7084 - sparse_categorical_accuracy: 0.9573\n",
      "Epoch 368/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7737 - sparse_categorical_accuracy: 0.9033\n",
      "Epoch 369/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.9981 - sparse_categorical_accuracy: 0.8056\n",
      "Epoch 370/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8427 - sparse_categorical_accuracy: 0.9035\n",
      "Epoch 371/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7816 - sparse_categorical_accuracy: 0.9594\n",
      "Epoch 372/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8323 - sparse_categorical_accuracy: 0.9423\n",
      "Epoch 373/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7542 - sparse_categorical_accuracy: 0.9540\n",
      "Epoch 374/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7995 - sparse_categorical_accuracy: 0.9456\n",
      "Epoch 375/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.8222 - sparse_categorical_accuracy: 0.4867\n",
      "Epoch 376/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.8166 - sparse_categorical_accuracy: 0.9477\n",
      "Epoch 377/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7351 - sparse_categorical_accuracy: 0.9646\n",
      "Epoch 378/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8146 - sparse_categorical_accuracy: 0.8902\n",
      "Epoch 379/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7372 - sparse_categorical_accuracy: 0.9679\n",
      "Epoch 380/500\n",
      "4/4 [==============================] - 0s 57ms/step - loss: 1.0379 - sparse_categorical_accuracy: 0.7492 - val_loss: 0.9454 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 381/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.8466 - sparse_categorical_accuracy: 0.8888\n",
      "Epoch 382/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6930 - sparse_categorical_accuracy: 0.9794\n",
      "Epoch 383/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8160 - sparse_categorical_accuracy: 0.9150\n",
      "Epoch 384/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7478 - sparse_categorical_accuracy: 0.9531\n",
      "Epoch 385/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.0015 - sparse_categorical_accuracy: 0.7148\n",
      "Epoch 386/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7530 - sparse_categorical_accuracy: 0.9337\n",
      "Epoch 387/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7844 - sparse_categorical_accuracy: 0.9142\n",
      "Epoch 388/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8165 - sparse_categorical_accuracy: 0.9377\n",
      "Epoch 389/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8210 - sparse_categorical_accuracy: 0.9306\n",
      "Epoch 390/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7890 - sparse_categorical_accuracy: 0.9771\n",
      "Epoch 391/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7759 - sparse_categorical_accuracy: 0.9658\n",
      "Epoch 392/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7363 - sparse_categorical_accuracy: 0.9650\n",
      "Epoch 393/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7261 - sparse_categorical_accuracy: 0.9713\n",
      "Epoch 394/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7530 - sparse_categorical_accuracy: 0.9471\n",
      "Epoch 395/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9474 - sparse_categorical_accuracy: 0.8171\n",
      "Epoch 396/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.9251 - sparse_categorical_accuracy: 0.8060\n",
      "Epoch 397/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7789 - sparse_categorical_accuracy: 0.9713\n",
      "Epoch 398/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7098 - sparse_categorical_accuracy: 0.9827\n",
      "Epoch 399/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7638 - sparse_categorical_accuracy: 0.9546\n",
      "Epoch 400/500\n",
      "4/4 [==============================] - 0s 83ms/step - loss: 1.3117 - sparse_categorical_accuracy: 0.7077 - val_loss: 0.7794 - val_sparse_categorical_accuracy: 0.9333\n",
      "Epoch 401/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7452 - sparse_categorical_accuracy: 0.9469\n",
      "Epoch 402/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7556 - sparse_categorical_accuracy: 0.9773\n",
      "Epoch 403/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6999 - sparse_categorical_accuracy: 0.9713\n",
      "Epoch 404/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6900 - sparse_categorical_accuracy: 0.9531\n",
      "Epoch 405/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7297 - sparse_categorical_accuracy: 0.9785\n",
      "Epoch 406/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.1531 - sparse_categorical_accuracy: 0.6833\n",
      "Epoch 407/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6930 - sparse_categorical_accuracy: 0.9765\n",
      "Epoch 408/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.1076 - sparse_categorical_accuracy: 0.7304\n",
      "Epoch 409/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7656 - sparse_categorical_accuracy: 0.9042\n",
      "Epoch 410/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7096 - sparse_categorical_accuracy: 0.9383\n",
      "Epoch 411/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7818 - sparse_categorical_accuracy: 0.9698\n",
      "Epoch 412/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8272 - sparse_categorical_accuracy: 0.9112\n",
      "Epoch 413/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7945 - sparse_categorical_accuracy: 0.9825\n",
      "Epoch 414/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7669 - sparse_categorical_accuracy: 0.9671\n",
      "Epoch 415/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7260 - sparse_categorical_accuracy: 0.9967\n",
      "Epoch 416/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7702 - sparse_categorical_accuracy: 0.9400\n",
      "Epoch 417/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.6981 - sparse_categorical_accuracy: 0.9733\n",
      "Epoch 418/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7209 - sparse_categorical_accuracy: 0.9881\n",
      "Epoch 419/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7194 - sparse_categorical_accuracy: 0.9517\n",
      "Epoch 420/500\n",
      "4/4 [==============================] - 0s 59ms/step - loss: 0.7437 - sparse_categorical_accuracy: 0.9015 - val_loss: 1.6375 - val_sparse_categorical_accuracy: 0.5667\n",
      "Epoch 421/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.3580 - sparse_categorical_accuracy: 0.5779\n",
      "Epoch 422/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.3958 - sparse_categorical_accuracy: 0.7640\n",
      "Epoch 423/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7822 - sparse_categorical_accuracy: 0.9229\n",
      "Epoch 424/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7592 - sparse_categorical_accuracy: 0.9848\n",
      "Epoch 425/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9172 - sparse_categorical_accuracy: 0.8119\n",
      "Epoch 426/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8634 - sparse_categorical_accuracy: 0.9654\n",
      "Epoch 427/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7431 - sparse_categorical_accuracy: 0.8940\n",
      "Epoch 428/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8009 - sparse_categorical_accuracy: 0.9710\n",
      "Epoch 429/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7140 - sparse_categorical_accuracy: 0.9879\n",
      "Epoch 430/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8005 - sparse_categorical_accuracy: 0.9340\n",
      "Epoch 431/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7644 - sparse_categorical_accuracy: 0.9565\n",
      "Epoch 432/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7145 - sparse_categorical_accuracy: 0.9946\n",
      "Epoch 433/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7200 - sparse_categorical_accuracy: 0.9829\n",
      "Epoch 434/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7556 - sparse_categorical_accuracy: 0.9767\n",
      "Epoch 435/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7567 - sparse_categorical_accuracy: 0.9565\n",
      "Epoch 436/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6602 - sparse_categorical_accuracy: 0.9798\n",
      "Epoch 437/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7406 - sparse_categorical_accuracy: 0.9392\n",
      "Epoch 438/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.3806 - sparse_categorical_accuracy: 0.6671\n",
      "Epoch 439/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8021 - sparse_categorical_accuracy: 0.8135\n",
      "Epoch 440/500\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 0.8064 - sparse_categorical_accuracy: 0.8960 - val_loss: 1.1273 - val_sparse_categorical_accuracy: 0.7000\n",
      "Epoch 441/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.8500 - sparse_categorical_accuracy: 0.9154\n",
      "Epoch 442/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.1820 - sparse_categorical_accuracy: 0.7092\n",
      "Epoch 443/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8353 - sparse_categorical_accuracy: 0.8971\n",
      "Epoch 444/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7968 - sparse_categorical_accuracy: 0.9308\n",
      "Epoch 445/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7705 - sparse_categorical_accuracy: 0.9860\n",
      "Epoch 446/500\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7762 - sparse_categorical_accuracy: 1.000 - 0s 8ms/step - loss: 0.7678 - sparse_categorical_accuracy: 0.9587\n",
      "Epoch 447/500\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.8019 - sparse_categorical_accuracy: 0.875 - 0s 8ms/step - loss: 0.7412 - sparse_categorical_accuracy: 0.9375\n",
      "Epoch 448/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8087 - sparse_categorical_accuracy: 0.8946\n",
      "Epoch 449/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8558 - sparse_categorical_accuracy: 0.8481\n",
      "Epoch 450/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6864 - sparse_categorical_accuracy: 0.9671\n",
      "Epoch 451/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7751 - sparse_categorical_accuracy: 0.8998\n",
      "Epoch 452/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7119 - sparse_categorical_accuracy: 0.9829\n",
      "Epoch 453/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7039 - sparse_categorical_accuracy: 0.9446\n",
      "Epoch 454/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7092 - sparse_categorical_accuracy: 0.9967\n",
      "Epoch 455/500\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.8474 - sparse_categorical_accuracy: 0.8700\n",
      "Epoch 456/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.6884 - sparse_categorical_accuracy: 0.9913\n",
      "Epoch 457/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.7410 - sparse_categorical_accuracy: 0.9425\n",
      "Epoch 458/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.6905 - sparse_categorical_accuracy: 0.9531\n",
      "Epoch 459/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7275 - sparse_categorical_accuracy: 0.9121\n",
      "Epoch 460/500\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 0.7245 - sparse_categorical_accuracy: 0.9765 - val_loss: 0.8138 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 461/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8465 - sparse_categorical_accuracy: 0.8733\n",
      "Epoch 462/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.3106 - sparse_categorical_accuracy: 0.6154\n",
      "Epoch 463/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.4941 - sparse_categorical_accuracy: 0.5235\n",
      "Epoch 464/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.8145 - sparse_categorical_accuracy: 0.8912\n",
      "Epoch 465/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7201 - sparse_categorical_accuracy: 0.9679\n",
      "Epoch 466/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.8359 - sparse_categorical_accuracy: 0.9433\n",
      "Epoch 467/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 11ms/step - loss: 0.7746 - sparse_categorical_accuracy: 0.9246\n",
      "Epoch 468/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7283 - sparse_categorical_accuracy: 0.9671\n",
      "Epoch 469/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.7218 - sparse_categorical_accuracy: 0.9625\n",
      "Epoch 470/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7853 - sparse_categorical_accuracy: 0.9627\n",
      "Epoch 471/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.7371 - sparse_categorical_accuracy: 0.9775\n",
      "Epoch 472/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6721 - sparse_categorical_accuracy: 0.9742\n",
      "Epoch 473/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8711 - sparse_categorical_accuracy: 0.8504\n",
      "Epoch 474/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7503 - sparse_categorical_accuracy: 0.9200\n",
      "Epoch 475/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7535 - sparse_categorical_accuracy: 0.9085\n",
      "Epoch 476/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7224 - sparse_categorical_accuracy: 0.9860\n",
      "Epoch 477/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6621 - sparse_categorical_accuracy: 0.9713\n",
      "Epoch 478/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7412 - sparse_categorical_accuracy: 0.9721\n",
      "Epoch 479/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7250 - sparse_categorical_accuracy: 0.9819\n",
      "Epoch 480/500\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.7922 - sparse_categorical_accuracy: 0.9225 - val_loss: 1.3622 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 481/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8739 - sparse_categorical_accuracy: 0.8537\n",
      "Epoch 482/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7292 - sparse_categorical_accuracy: 0.9704\n",
      "Epoch 483/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7171 - sparse_categorical_accuracy: 0.9594\n",
      "Epoch 484/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6900 - sparse_categorical_accuracy: 0.9652\n",
      "Epoch 485/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.8874 - sparse_categorical_accuracy: 0.8556\n",
      "Epoch 486/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8576 - sparse_categorical_accuracy: 0.9150\n",
      "Epoch 487/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7945 - sparse_categorical_accuracy: 0.9775\n",
      "Epoch 488/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7210 - sparse_categorical_accuracy: 0.9700\n",
      "Epoch 489/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8409 - sparse_categorical_accuracy: 0.8723\n",
      "Epoch 490/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7242 - sparse_categorical_accuracy: 0.8794\n",
      "Epoch 491/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6526 - sparse_categorical_accuracy: 0.9765\n",
      "Epoch 492/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8028 - sparse_categorical_accuracy: 0.9292\n",
      "Epoch 493/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7010 - sparse_categorical_accuracy: 0.9913\n",
      "Epoch 494/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7270 - sparse_categorical_accuracy: 0.9700\n",
      "Epoch 495/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7683 - sparse_categorical_accuracy: 0.9212\n",
      "Epoch 496/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6985 - sparse_categorical_accuracy: 0.9615\n",
      "Epoch 497/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6870 - sparse_categorical_accuracy: 0.9767\n",
      "Epoch 498/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7420 - sparse_categorical_accuracy: 0.9479\n",
      "Epoch 499/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7321 - sparse_categorical_accuracy: 0.9860\n",
      "Epoch 500/500\n",
      "4/4 [==============================] - 0s 61ms/step - loss: 0.7710 - sparse_categorical_accuracy: 0.9525 - val_loss: 0.7156 - val_sparse_categorical_accuracy: 1.0000\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "softmax_13 (Softmax)         (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 105\n",
      "Trainable params: 105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#第一步，import\n",
    "import tensorflow as tf #导入模块\n",
    "from sklearn import datasets #从sklearn中导入数据集\n",
    "import numpy as np #导入科学计算模块\n",
    "import tensorflow.keras as keras\n",
    " \n",
    "#第二步，train, test\n",
    "x_train = datasets.load_iris().data #导入iris数据集的输入\n",
    " \n",
    "y_train = datasets.load_iris().target #导入iris数据集的标签\n",
    " \n",
    "np.random.seed(120) #设置随机种子，让每次结果都一样，方便对照\n",
    " \n",
    "np.random.shuffle(x_train) #使用shuffle()方法，让输入x_train乱序\n",
    " \n",
    "np.random.seed(120) #设置随机种子，让每次结果都一样，方便对照\n",
    " \n",
    "np.random.shuffle(y_train) #使用shuffle()方法，让输入y_train乱序\n",
    " \n",
    "tf.random.set_seed(120) #让tensorflow中的种子数设置为120\n",
    " \n",
    "#第三步，models.Sequential()\n",
    "model = tf.keras.models.Sequential([ #使用models.Sequential()来搭建神经网络\n",
    "    tf.keras.layers.Dense(10, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.l2()), #全连接层，三个神经元，激活函数为softmax,使用l2正则化\n",
    "    tf.keras.layers.Dense(5, activation = \"relu\"),\n",
    "    tf.keras.layers.Softmax()\n",
    "])\n",
    " \n",
    "#第四步，model.compile()\n",
    "model.compile(  #使用model.compile()方法来配置训练方法\n",
    "    optimizer = tf.keras.optimizers.SGD(lr = 0.1), #使用SGD优化器，学习率为0.1\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False), #配置损失函数\n",
    "    metrics = ['sparse_categorical_accuracy'] #标注网络评价指标\n",
    ")\n",
    " \n",
    "#第五步，model.fit()\n",
    "history = model.fit(  #使用model.fit()方法来执行训练过程，\n",
    "    x_train, y_train, #告知训练集的输入以及标签，\n",
    "    batch_size = 32, #每一批batch的大小为32，\n",
    "    epochs = 500, #迭代次数epochs为500\n",
    "    validation_split = 0.2, #从测试集中划分80%给训练集\n",
    "    validation_freq = 20 #测试的间隔次数为20\n",
    ")\n",
    " \n",
    "#第六步，model.summary()\n",
    "model.summary() #打印神经网络结构，统计参数数目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [1.9167262315750122,\n",
       "  1.3963083028793335,\n",
       "  1.3247488737106323,\n",
       "  1.254538655281067,\n",
       "  1.2144882678985596,\n",
       "  1.1674425601959229,\n",
       "  1.138580322265625,\n",
       "  1.1192318201065063,\n",
       "  1.1386946439743042,\n",
       "  1.0937007665634155,\n",
       "  1.0813243389129639,\n",
       "  1.0743645429611206,\n",
       "  1.071738600730896,\n",
       "  1.0646189451217651,\n",
       "  1.045695424079895,\n",
       "  1.0825247764587402,\n",
       "  1.0412251949310303,\n",
       "  1.0286390781402588,\n",
       "  1.0232199430465698,\n",
       "  1.0087693929672241,\n",
       "  1.0260872840881348,\n",
       "  1.0072910785675049,\n",
       "  0.9937226176261902,\n",
       "  1.0099222660064697,\n",
       "  1.0188924074172974,\n",
       "  0.9783613085746765,\n",
       "  0.9716012477874756,\n",
       "  0.9616180658340454,\n",
       "  0.9842042922973633,\n",
       "  1.0101852416992188,\n",
       "  0.9669749140739441,\n",
       "  0.9287338256835938,\n",
       "  0.9125385284423828,\n",
       "  0.9223558902740479,\n",
       "  1.0944104194641113,\n",
       "  1.0218292474746704,\n",
       "  1.0071922540664673,\n",
       "  0.9566805362701416,\n",
       "  0.9523071050643921,\n",
       "  0.9777871370315552,\n",
       "  1.0200749635696411,\n",
       "  0.891160786151886,\n",
       "  0.8960562348365784,\n",
       "  0.9519903063774109,\n",
       "  0.8993299603462219,\n",
       "  0.9746337532997131,\n",
       "  0.945414125919342,\n",
       "  0.9091475605964661,\n",
       "  0.9668383002281189,\n",
       "  0.8995905518531799,\n",
       "  1.1993255615234375,\n",
       "  1.0410629510879517,\n",
       "  0.9083185791969299,\n",
       "  0.9355239868164062,\n",
       "  0.8613654971122742,\n",
       "  0.8624131679534912,\n",
       "  0.8396298885345459,\n",
       "  0.9077569246292114,\n",
       "  0.8508284687995911,\n",
       "  1.0703967809677124,\n",
       "  0.8697879910469055,\n",
       "  0.9855488538742065,\n",
       "  0.8815578818321228,\n",
       "  1.073493480682373,\n",
       "  0.819291353225708,\n",
       "  0.8122951984405518,\n",
       "  0.9670385122299194,\n",
       "  0.9898272156715393,\n",
       "  0.8417215943336487,\n",
       "  0.8074266910552979,\n",
       "  1.002624750137329,\n",
       "  0.9772555828094482,\n",
       "  0.8866099119186401,\n",
       "  1.0569050312042236,\n",
       "  1.053566575050354,\n",
       "  0.8835172653198242,\n",
       "  1.01225745677948,\n",
       "  0.8342710137367249,\n",
       "  0.8354703187942505,\n",
       "  0.8813562393188477,\n",
       "  0.8367434144020081,\n",
       "  0.7920709848403931,\n",
       "  0.8434743285179138,\n",
       "  0.8006612658500671,\n",
       "  0.9190303683280945,\n",
       "  1.1446492671966553,\n",
       "  0.9763263463973999,\n",
       "  0.8748144507408142,\n",
       "  1.1194270849227905,\n",
       "  0.864963948726654,\n",
       "  0.94316166639328,\n",
       "  0.8546397089958191,\n",
       "  0.7726735472679138,\n",
       "  0.7946417331695557,\n",
       "  0.7872316241264343,\n",
       "  0.7861020565032959,\n",
       "  0.8697093725204468,\n",
       "  0.845837414264679,\n",
       "  0.8016628623008728,\n",
       "  1.0433446168899536,\n",
       "  0.7793692946434021,\n",
       "  0.7633602023124695,\n",
       "  0.9122872948646545,\n",
       "  0.940963089466095,\n",
       "  0.786067008972168,\n",
       "  0.869916558265686,\n",
       "  0.7735052108764648,\n",
       "  0.8611311316490173,\n",
       "  0.9797954559326172,\n",
       "  1.0282081365585327,\n",
       "  0.7943291068077087,\n",
       "  1.2125602960586548,\n",
       "  0.8182134032249451,\n",
       "  0.7685317993164062,\n",
       "  0.7559488415718079,\n",
       "  0.7603374719619751,\n",
       "  0.7545053958892822,\n",
       "  0.8922151923179626,\n",
       "  0.8209009170532227,\n",
       "  0.9414352178573608,\n",
       "  0.9504513144493103,\n",
       "  0.7579221129417419,\n",
       "  0.7682174444198608,\n",
       "  0.7507461905479431,\n",
       "  0.798969566822052,\n",
       "  1.0656551122665405,\n",
       "  1.0480132102966309,\n",
       "  0.8854389190673828,\n",
       "  0.7570528388023376,\n",
       "  0.8153733611106873,\n",
       "  0.7546510100364685,\n",
       "  0.8151161074638367,\n",
       "  0.8922103643417358,\n",
       "  0.9841299653053284,\n",
       "  0.8594847321510315,\n",
       "  0.9895449280738831,\n",
       "  0.7607338428497314,\n",
       "  0.7508628964424133,\n",
       "  0.745219886302948,\n",
       "  0.7384434342384338,\n",
       "  0.9624806046485901,\n",
       "  0.7448689341545105,\n",
       "  0.7540910243988037,\n",
       "  0.7794239521026611,\n",
       "  0.737747311592102,\n",
       "  0.9229175448417664,\n",
       "  1.1755090951919556,\n",
       "  0.7533129453659058,\n",
       "  0.7593054175376892,\n",
       "  0.7625192999839783,\n",
       "  0.7344694137573242,\n",
       "  0.9524060487747192,\n",
       "  1.1531672477722168,\n",
       "  0.7660916447639465,\n",
       "  0.8757266402244568,\n",
       "  0.7570523023605347,\n",
       "  1.2211551666259766,\n",
       "  0.7541665434837341,\n",
       "  0.7436870336532593,\n",
       "  0.7423075437545776,\n",
       "  0.7795423865318298,\n",
       "  0.8342410922050476,\n",
       "  0.7383304834365845,\n",
       "  0.7322345972061157,\n",
       "  0.7292374968528748,\n",
       "  0.7449200749397278,\n",
       "  0.7253914475440979,\n",
       "  0.7294734716415405,\n",
       "  0.7370917201042175,\n",
       "  0.8262090086936951,\n",
       "  0.743484377861023,\n",
       "  0.9892339110374451,\n",
       "  1.2350108623504639,\n",
       "  0.7613701224327087,\n",
       "  0.7454547882080078,\n",
       "  0.7441951632499695,\n",
       "  0.7688688039779663,\n",
       "  0.8962429165840149,\n",
       "  1.1651304960250854,\n",
       "  0.8102959990501404,\n",
       "  0.7846431136131287,\n",
       "  0.7356485724449158,\n",
       "  0.7249976396560669,\n",
       "  0.7796759009361267,\n",
       "  0.727590799331665,\n",
       "  0.7362491488456726,\n",
       "  0.8957300782203674,\n",
       "  0.7478960752487183,\n",
       "  0.829799234867096,\n",
       "  1.1514966487884521,\n",
       "  0.8543770909309387,\n",
       "  0.8654510974884033,\n",
       "  0.7316395044326782,\n",
       "  0.7913132905960083,\n",
       "  0.8626867532730103,\n",
       "  0.7217417359352112,\n",
       "  0.7619897127151489,\n",
       "  1.0159096717834473,\n",
       "  0.8974739909172058,\n",
       "  0.7270238399505615,\n",
       "  0.8167949318885803,\n",
       "  0.8039559125900269,\n",
       "  1.1679213047027588,\n",
       "  0.7747205495834351,\n",
       "  0.7262346744537354,\n",
       "  0.7400657534599304,\n",
       "  0.9967993497848511,\n",
       "  0.750493049621582,\n",
       "  0.8158008456230164,\n",
       "  0.8635953664779663,\n",
       "  0.7319044470787048,\n",
       "  0.7438132166862488,\n",
       "  0.7360455393791199,\n",
       "  0.7165853381156921,\n",
       "  0.7236681580543518,\n",
       "  0.8882548213005066,\n",
       "  0.8086690306663513,\n",
       "  0.8174907565116882,\n",
       "  0.9866393208503723,\n",
       "  0.7676568627357483,\n",
       "  0.7761222124099731,\n",
       "  0.9806724190711975,\n",
       "  0.80464106798172,\n",
       "  0.7488259077072144,\n",
       "  0.794445276260376,\n",
       "  0.7155368328094482,\n",
       "  0.7219391465187073,\n",
       "  0.9633733034133911,\n",
       "  0.8469611406326294,\n",
       "  0.7877117991447449,\n",
       "  1.0891053676605225,\n",
       "  0.7896906137466431,\n",
       "  0.7492389678955078,\n",
       "  0.7242055535316467,\n",
       "  0.7159700989723206,\n",
       "  0.7434876561164856,\n",
       "  0.7904282212257385,\n",
       "  0.7177448272705078,\n",
       "  0.7312710285186768,\n",
       "  0.7655863165855408,\n",
       "  0.7247569561004639,\n",
       "  0.7674148678779602,\n",
       "  1.0356745719909668,\n",
       "  1.0345810651779175,\n",
       "  0.7237758040428162,\n",
       "  0.7897142767906189,\n",
       "  1.2882148027420044,\n",
       "  0.8376907110214233,\n",
       "  0.8575946688652039,\n",
       "  0.871267557144165,\n",
       "  0.9218651056289673,\n",
       "  0.7484151124954224,\n",
       "  0.8191656470298767,\n",
       "  0.7812946438789368,\n",
       "  0.8514206409454346,\n",
       "  0.7355390191078186,\n",
       "  0.9863698482513428,\n",
       "  0.7371941208839417,\n",
       "  0.7321481108665466,\n",
       "  0.7360270023345947,\n",
       "  1.0526361465454102,\n",
       "  0.7201737761497498,\n",
       "  0.7226083278656006,\n",
       "  0.7197170257568359,\n",
       "  0.7136824131011963,\n",
       "  0.7114802598953247,\n",
       "  0.8190205097198486,\n",
       "  0.7077642679214478,\n",
       "  0.7446144819259644,\n",
       "  0.7048090100288391,\n",
       "  0.701655924320221,\n",
       "  0.7371797561645508,\n",
       "  0.8374070525169373,\n",
       "  1.0624953508377075,\n",
       "  0.7887773513793945,\n",
       "  0.7437435984611511,\n",
       "  0.800226092338562,\n",
       "  0.705986738204956,\n",
       "  0.8017120957374573,\n",
       "  0.9120355248451233,\n",
       "  0.8114921450614929,\n",
       "  0.7207163572311401,\n",
       "  0.7544227242469788,\n",
       "  0.7332500219345093,\n",
       "  1.378996729850769,\n",
       "  0.8033521771430969,\n",
       "  0.7115675806999207,\n",
       "  0.714988648891449,\n",
       "  0.7636658549308777,\n",
       "  0.9846606850624084,\n",
       "  0.7321693897247314,\n",
       "  0.7776767015457153,\n",
       "  0.7959315776824951,\n",
       "  1.140795350074768,\n",
       "  0.8112525343894958,\n",
       "  0.7273504734039307,\n",
       "  0.8531894683837891,\n",
       "  1.3125239610671997,\n",
       "  0.8662528991699219,\n",
       "  0.8458980321884155,\n",
       "  0.8612936735153198,\n",
       "  0.905144453048706,\n",
       "  0.8390308618545532,\n",
       "  0.8401156067848206,\n",
       "  0.8136051893234253,\n",
       "  0.8109661936759949,\n",
       "  0.7853396534919739,\n",
       "  0.7448652386665344,\n",
       "  0.8122779726982117,\n",
       "  0.7597985863685608,\n",
       "  0.8478125333786011,\n",
       "  0.8259055018424988,\n",
       "  0.7610225677490234,\n",
       "  1.3403897285461426,\n",
       "  1.004622220993042,\n",
       "  0.8506532311439514,\n",
       "  0.8255407810211182,\n",
       "  0.8363168239593506,\n",
       "  1.028027892112732,\n",
       "  0.8057685494422913,\n",
       "  0.7959267497062683,\n",
       "  0.7952497005462646,\n",
       "  0.7947381138801575,\n",
       "  0.7886024713516235,\n",
       "  0.8879131078720093,\n",
       "  0.9394780993461609,\n",
       "  0.7818208336830139,\n",
       "  0.8150038123130798,\n",
       "  0.7775915861129761,\n",
       "  0.7750803232192993,\n",
       "  0.8808284997940063,\n",
       "  0.9254369735717773,\n",
       "  1.0494797229766846,\n",
       "  0.8221355676651001,\n",
       "  0.7588587403297424,\n",
       "  0.7168585658073425,\n",
       "  0.7515001893043518,\n",
       "  1.5703376531600952,\n",
       "  0.8247692584991455,\n",
       "  0.803918719291687,\n",
       "  0.8300867676734924,\n",
       "  0.7904049754142761,\n",
       "  0.7821060419082642,\n",
       "  0.7877238988876343,\n",
       "  0.7864562273025513,\n",
       "  0.7650974988937378,\n",
       "  0.7942191958427429,\n",
       "  0.778520941734314,\n",
       "  0.7586643099784851,\n",
       "  0.8353818655014038,\n",
       "  1.1492079496383667,\n",
       "  0.8214154839515686,\n",
       "  0.9252379536628723,\n",
       "  0.7588199377059937,\n",
       "  0.7945773005485535,\n",
       "  0.7617692351341248,\n",
       "  0.7498219609260559,\n",
       "  0.770616888999939,\n",
       "  0.9340508580207825,\n",
       "  0.746588945388794,\n",
       "  0.7429801821708679,\n",
       "  0.7470056414604187,\n",
       "  0.7569941282272339,\n",
       "  0.9470025300979614,\n",
       "  0.7687981128692627,\n",
       "  0.7005547881126404,\n",
       "  0.7281367182731628,\n",
       "  0.8550803661346436,\n",
       "  0.9989637732505798,\n",
       "  0.7814514636993408,\n",
       "  0.7674737572669983,\n",
       "  0.7661072611808777,\n",
       "  0.743170976638794,\n",
       "  0.7957865595817566,\n",
       "  1.6034588813781738,\n",
       "  0.761004626750946,\n",
       "  0.7248044610023499,\n",
       "  0.8355363607406616,\n",
       "  0.7313533425331116,\n",
       "  1.0194008350372314,\n",
       "  0.7973353266716003,\n",
       "  0.7229297161102295,\n",
       "  0.7682536244392395,\n",
       "  0.7338842153549194,\n",
       "  1.1364789009094238,\n",
       "  0.7746292352676392,\n",
       "  0.7701115012168884,\n",
       "  0.8143227696418762,\n",
       "  0.8149834275245667,\n",
       "  0.7622525095939636,\n",
       "  0.7285789847373962,\n",
       "  0.7065462470054626,\n",
       "  0.7036964297294617,\n",
       "  0.768004834651947,\n",
       "  0.9494410157203674,\n",
       "  0.8092080950737,\n",
       "  0.7319299578666687,\n",
       "  0.7214980721473694,\n",
       "  0.7849542498588562,\n",
       "  1.2069497108459473,\n",
       "  0.7299010157585144,\n",
       "  0.7347549200057983,\n",
       "  0.7185370326042175,\n",
       "  0.7178875207901001,\n",
       "  0.711627185344696,\n",
       "  1.0949819087982178,\n",
       "  0.7209033966064453,\n",
       "  1.0698074102401733,\n",
       "  0.7611433267593384,\n",
       "  0.7431067228317261,\n",
       "  0.7574341297149658,\n",
       "  0.7596797347068787,\n",
       "  0.7527945041656494,\n",
       "  0.7708314061164856,\n",
       "  0.7072423696517944,\n",
       "  0.7506174445152283,\n",
       "  0.7137950658798218,\n",
       "  0.6975802779197693,\n",
       "  0.7623226642608643,\n",
       "  0.9165173768997192,\n",
       "  1.2882258892059326,\n",
       "  1.1255651712417603,\n",
       "  0.804172158241272,\n",
       "  0.7710143327713013,\n",
       "  0.897377610206604,\n",
       "  0.7817657589912415,\n",
       "  0.7699156403541565,\n",
       "  0.7390977144241333,\n",
       "  0.7546359300613403,\n",
       "  0.7613877654075623,\n",
       "  0.7149785757064819,\n",
       "  0.698997437953949,\n",
       "  0.7009485363960266,\n",
       "  0.72659832239151,\n",
       "  0.7146039605140686,\n",
       "  0.6955075860023499,\n",
       "  0.7329741716384888,\n",
       "  1.3826059103012085,\n",
       "  0.8207674026489258,\n",
       "  0.8147382736206055,\n",
       "  0.8099937438964844,\n",
       "  1.0858192443847656,\n",
       "  0.7838358283042908,\n",
       "  0.7863666415214539,\n",
       "  0.7071449756622314,\n",
       "  0.7678811550140381,\n",
       "  0.73009192943573,\n",
       "  0.7589986324310303,\n",
       "  0.8302032947540283,\n",
       "  0.7071636915206909,\n",
       "  0.8657935857772827,\n",
       "  0.702810525894165,\n",
       "  0.7397384643554688,\n",
       "  0.69597327709198,\n",
       "  0.8506492972373962,\n",
       "  0.7009502053260803,\n",
       "  0.7308382391929626,\n",
       "  0.7366409301757812,\n",
       "  0.7264627814292908,\n",
       "  0.7003024220466614,\n",
       "  0.875708281993866,\n",
       "  1.2369446754455566,\n",
       "  1.1876907348632812,\n",
       "  0.7860902547836304,\n",
       "  0.7361411452293396,\n",
       "  0.794578492641449,\n",
       "  0.7774755954742432,\n",
       "  0.7297484874725342,\n",
       "  0.7612062096595764,\n",
       "  0.7522791624069214,\n",
       "  0.716566264629364,\n",
       "  0.7050073146820068,\n",
       "  0.9116512537002563,\n",
       "  0.7793988585472107,\n",
       "  0.7564373016357422,\n",
       "  0.7003495693206787,\n",
       "  0.7011353969573975,\n",
       "  0.7211374044418335,\n",
       "  0.7054907083511353,\n",
       "  0.7659383416175842,\n",
       "  0.8217295408248901,\n",
       "  0.6945571303367615,\n",
       "  0.7413700819015503,\n",
       "  0.7135381698608398,\n",
       "  1.0913207530975342,\n",
       "  0.7565057873725891,\n",
       "  0.7325248122215271,\n",
       "  0.7148755192756653,\n",
       "  0.809766411781311,\n",
       "  0.7871745824813843,\n",
       "  0.7058551907539368,\n",
       "  0.7777158617973328,\n",
       "  0.6993567943572998,\n",
       "  0.7140195369720459,\n",
       "  0.7596668601036072,\n",
       "  0.7120149731636047,\n",
       "  0.6872388124465942,\n",
       "  0.7465684413909912,\n",
       "  0.6890390515327454,\n",
       "  0.7689045667648315],\n",
       " 'sparse_categorical_accuracy': [0.3333333432674408,\n",
       "  0.375,\n",
       "  0.34166666865348816,\n",
       "  0.3499999940395355,\n",
       "  0.3583333194255829,\n",
       "  0.3333333432674408,\n",
       "  0.3583333194255829,\n",
       "  0.36666667461395264,\n",
       "  0.4000000059604645,\n",
       "  0.4416666626930237,\n",
       "  0.40833333134651184,\n",
       "  0.42500001192092896,\n",
       "  0.42500001192092896,\n",
       "  0.3916666805744171,\n",
       "  0.46666666865348816,\n",
       "  0.4000000059604645,\n",
       "  0.6083333492279053,\n",
       "  0.5666666626930237,\n",
       "  0.6499999761581421,\n",
       "  0.5916666388511658,\n",
       "  0.5083333253860474,\n",
       "  0.574999988079071,\n",
       "  0.675000011920929,\n",
       "  0.6166666746139526,\n",
       "  0.6583333611488342,\n",
       "  0.6666666865348816,\n",
       "  0.6833333373069763,\n",
       "  0.6499999761581421,\n",
       "  0.6000000238418579,\n",
       "  0.550000011920929,\n",
       "  0.675000011920929,\n",
       "  0.7916666865348816,\n",
       "  0.7916666865348816,\n",
       "  0.699999988079071,\n",
       "  0.5166666507720947,\n",
       "  0.7333333492279053,\n",
       "  0.7083333134651184,\n",
       "  0.7583333253860474,\n",
       "  0.625,\n",
       "  0.8083333373069763,\n",
       "  0.699999988079071,\n",
       "  0.7833333611488342,\n",
       "  0.7416666746139526,\n",
       "  0.7166666388511658,\n",
       "  0.8166666626930237,\n",
       "  0.6833333373069763,\n",
       "  0.699999988079071,\n",
       "  0.8083333373069763,\n",
       "  0.8916666507720947,\n",
       "  0.7833333611488342,\n",
       "  0.5333333611488342,\n",
       "  0.8166666626930237,\n",
       "  0.7666666507720947,\n",
       "  0.7916666865348816,\n",
       "  0.824999988079071,\n",
       "  0.8083333373069763,\n",
       "  0.8833333253860474,\n",
       "  0.8083333373069763,\n",
       "  0.824999988079071,\n",
       "  0.6916666626930237,\n",
       "  0.875,\n",
       "  0.6916666626930237,\n",
       "  0.8916666507720947,\n",
       "  0.7166666388511658,\n",
       "  0.949999988079071,\n",
       "  0.8583333492279053,\n",
       "  0.6499999761581421,\n",
       "  0.7416666746139526,\n",
       "  0.8916666507720947,\n",
       "  0.9166666865348816,\n",
       "  0.6416666507720947,\n",
       "  0.7916666865348816,\n",
       "  0.824999988079071,\n",
       "  0.7250000238418579,\n",
       "  0.6916666626930237,\n",
       "  0.8666666746139526,\n",
       "  0.75,\n",
       "  0.9083333611488342,\n",
       "  0.8666666746139526,\n",
       "  0.75,\n",
       "  0.8583333492279053,\n",
       "  0.8916666507720947,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.7250000238418579,\n",
       "  0.7583333253860474,\n",
       "  0.8416666388511658,\n",
       "  0.875,\n",
       "  0.6916666626930237,\n",
       "  0.8666666746139526,\n",
       "  0.800000011920929,\n",
       "  0.9166666865348816,\n",
       "  0.9083333611488342,\n",
       "  0.9166666865348816,\n",
       "  0.8583333492279053,\n",
       "  0.8916666507720947,\n",
       "  0.7833333611488342,\n",
       "  0.8166666626930237,\n",
       "  0.9333333373069763,\n",
       "  0.7166666388511658,\n",
       "  0.9416666626930237,\n",
       "  0.9083333611488342,\n",
       "  0.824999988079071,\n",
       "  0.875,\n",
       "  0.8833333253860474,\n",
       "  0.8583333492279053,\n",
       "  0.9416666626930237,\n",
       "  0.8333333134651184,\n",
       "  0.7916666865348816,\n",
       "  0.7583333253860474,\n",
       "  0.9333333373069763,\n",
       "  0.6916666626930237,\n",
       "  0.9333333373069763,\n",
       "  0.949999988079071,\n",
       "  0.9166666865348816,\n",
       "  0.9416666626930237,\n",
       "  0.8833333253860474,\n",
       "  0.7916666865348816,\n",
       "  0.8833333253860474,\n",
       "  0.7666666507720947,\n",
       "  0.8166666626930237,\n",
       "  0.9583333134651184,\n",
       "  0.8916666507720947,\n",
       "  0.9166666865348816,\n",
       "  0.875,\n",
       "  0.675000011920929,\n",
       "  0.7833333611488342,\n",
       "  0.8500000238418579,\n",
       "  0.9750000238418579,\n",
       "  0.8916666507720947,\n",
       "  0.9666666388511658,\n",
       "  0.8833333253860474,\n",
       "  0.824999988079071,\n",
       "  0.8416666388511658,\n",
       "  0.8583333492279053,\n",
       "  0.8916666507720947,\n",
       "  0.9333333373069763,\n",
       "  0.949999988079071,\n",
       "  0.925000011920929,\n",
       "  0.9083333611488342,\n",
       "  0.7833333611488342,\n",
       "  0.9666666388511658,\n",
       "  0.9166666865348816,\n",
       "  0.8999999761581421,\n",
       "  0.9166666865348816,\n",
       "  0.7833333611488342,\n",
       "  0.7583333253860474,\n",
       "  0.9750000238418579,\n",
       "  0.9333333373069763,\n",
       "  0.8999999761581421,\n",
       "  0.9416666626930237,\n",
       "  0.824999988079071,\n",
       "  0.7333333492279053,\n",
       "  0.9666666388511658,\n",
       "  0.8833333253860474,\n",
       "  0.9666666388511658,\n",
       "  0.7250000238418579,\n",
       "  0.9833333492279053,\n",
       "  0.9833333492279053,\n",
       "  0.9666666388511658,\n",
       "  0.9166666865348816,\n",
       "  0.8916666507720947,\n",
       "  0.9583333134651184,\n",
       "  0.9083333611488342,\n",
       "  0.9333333373069763,\n",
       "  0.949999988079071,\n",
       "  0.949999988079071,\n",
       "  0.9166666865348816,\n",
       "  0.9416666626930237,\n",
       "  0.8583333492279053,\n",
       "  0.9416666626930237,\n",
       "  0.7666666507720947,\n",
       "  0.7166666388511658,\n",
       "  0.9750000238418579,\n",
       "  0.9750000238418579,\n",
       "  0.9666666388511658,\n",
       "  0.9083333611488342,\n",
       "  0.8416666388511658,\n",
       "  0.6916666626930237,\n",
       "  0.9166666865348816,\n",
       "  0.949999988079071,\n",
       "  0.9666666388511658,\n",
       "  0.949999988079071,\n",
       "  0.9166666865348816,\n",
       "  0.925000011920929,\n",
       "  0.949999988079071,\n",
       "  0.7833333611488342,\n",
       "  0.9416666626930237,\n",
       "  0.8916666507720947,\n",
       "  0.824999988079071,\n",
       "  0.8999999761581421,\n",
       "  0.8916666507720947,\n",
       "  0.9750000238418579,\n",
       "  0.9166666865348816,\n",
       "  0.8916666507720947,\n",
       "  0.9666666388511658,\n",
       "  0.9416666626930237,\n",
       "  0.7666666507720947,\n",
       "  0.8166666626930237,\n",
       "  0.9833333492279053,\n",
       "  0.8999999761581421,\n",
       "  0.9083333611488342,\n",
       "  0.6916666626930237,\n",
       "  0.9833333492279053,\n",
       "  0.9833333492279053,\n",
       "  0.949999988079071,\n",
       "  0.7916666865348816,\n",
       "  0.9416666626930237,\n",
       "  0.925000011920929,\n",
       "  0.8666666746139526,\n",
       "  0.9750000238418579,\n",
       "  0.949999988079071,\n",
       "  0.9416666626930237,\n",
       "  0.9583333134651184,\n",
       "  0.9333333373069763,\n",
       "  0.824999988079071,\n",
       "  0.9416666626930237,\n",
       "  0.8833333253860474,\n",
       "  0.800000011920929,\n",
       "  0.9583333134651184,\n",
       "  0.949999988079071,\n",
       "  0.8083333373069763,\n",
       "  0.925000011920929,\n",
       "  0.9416666626930237,\n",
       "  0.9083333611488342,\n",
       "  0.9750000238418579,\n",
       "  0.949999988079071,\n",
       "  0.8166666626930237,\n",
       "  0.8999999761581421,\n",
       "  0.9416666626930237,\n",
       "  0.7749999761581421,\n",
       "  0.9166666865348816,\n",
       "  0.9583333134651184,\n",
       "  0.9666666388511658,\n",
       "  0.9750000238418579,\n",
       "  0.9333333373069763,\n",
       "  0.8916666507720947,\n",
       "  0.9583333134651184,\n",
       "  0.9333333373069763,\n",
       "  0.8916666507720947,\n",
       "  0.9666666388511658,\n",
       "  0.925000011920929,\n",
       "  0.8083333373069763,\n",
       "  0.8416666388511658,\n",
       "  0.9666666388511658,\n",
       "  0.949999988079071,\n",
       "  0.75,\n",
       "  0.949999988079071,\n",
       "  0.8833333253860474,\n",
       "  0.8916666507720947,\n",
       "  0.8583333492279053,\n",
       "  0.9583333134651184,\n",
       "  0.925000011920929,\n",
       "  0.9333333373069763,\n",
       "  0.8666666746139526,\n",
       "  0.949999988079071,\n",
       "  0.8166666626930237,\n",
       "  0.9750000238418579,\n",
       "  0.9583333134651184,\n",
       "  0.949999988079071,\n",
       "  0.7583333253860474,\n",
       "  0.9833333492279053,\n",
       "  0.9583333134651184,\n",
       "  0.9583333134651184,\n",
       "  0.9583333134651184,\n",
       "  0.949999988079071,\n",
       "  0.8416666388511658,\n",
       "  0.9583333134651184,\n",
       "  0.949999988079071,\n",
       "  0.9666666388511658,\n",
       "  0.9166666865348816,\n",
       "  0.949999988079071,\n",
       "  0.875,\n",
       "  0.7749999761581421,\n",
       "  0.925000011920929,\n",
       "  0.949999988079071,\n",
       "  0.8916666507720947,\n",
       "  0.9666666388511658,\n",
       "  0.8833333253860474,\n",
       "  0.824999988079071,\n",
       "  0.8999999761581421,\n",
       "  0.9666666388511658,\n",
       "  0.925000011920929,\n",
       "  0.9333333373069763,\n",
       "  0.6666666865348816,\n",
       "  0.9333333373069763,\n",
       "  0.9750000238418579,\n",
       "  0.9666666388511658,\n",
       "  0.9416666626930237,\n",
       "  0.8166666626930237,\n",
       "  0.9666666388511658,\n",
       "  0.9333333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.6416666507720947,\n",
       "  0.9666666388511658,\n",
       "  0.9666666388511658,\n",
       "  0.8833333253860474,\n",
       "  0.6916666626930237,\n",
       "  0.8999999761581421,\n",
       "  0.9583333134651184,\n",
       "  0.8999999761581421,\n",
       "  0.8416666388511658,\n",
       "  0.9333333373069763,\n",
       "  0.9416666626930237,\n",
       "  0.9666666388511658,\n",
       "  0.9583333134651184,\n",
       "  0.9666666388511658,\n",
       "  0.9583333134651184,\n",
       "  0.875,\n",
       "  0.9583333134651184,\n",
       "  0.8833333253860474,\n",
       "  0.8916666507720947,\n",
       "  0.9416666626930237,\n",
       "  0.675000011920929,\n",
       "  0.7083333134651184,\n",
       "  0.8999999761581421,\n",
       "  0.9416666626930237,\n",
       "  0.925000011920929,\n",
       "  0.7333333492279053,\n",
       "  0.9666666388511658,\n",
       "  0.9666666388511658,\n",
       "  0.9750000238418579,\n",
       "  0.9666666388511658,\n",
       "  0.9750000238418579,\n",
       "  0.8416666388511658,\n",
       "  0.824999988079071,\n",
       "  0.9750000238418579,\n",
       "  0.925000011920929,\n",
       "  0.9666666388511658,\n",
       "  0.9750000238418579,\n",
       "  0.8583333492279053,\n",
       "  0.8166666626930237,\n",
       "  0.75,\n",
       "  0.9166666865348816,\n",
       "  0.9416666626930237,\n",
       "  0.9750000238418579,\n",
       "  0.949999988079071,\n",
       "  0.550000011920929,\n",
       "  0.8500000238418579,\n",
       "  0.925000011920929,\n",
       "  0.8916666507720947,\n",
       "  0.949999988079071,\n",
       "  0.9666666388511658,\n",
       "  0.949999988079071,\n",
       "  0.9416666626930237,\n",
       "  0.9666666388511658,\n",
       "  0.9666666388511658,\n",
       "  0.9416666626930237,\n",
       "  0.9666666388511658,\n",
       "  0.9083333611488342,\n",
       "  0.699999988079071,\n",
       "  0.9083333611488342,\n",
       "  0.8416666388511658,\n",
       "  0.9750000238418579,\n",
       "  0.9333333373069763,\n",
       "  0.9583333134651184,\n",
       "  0.9666666388511658,\n",
       "  0.949999988079071,\n",
       "  0.8083333373069763,\n",
       "  0.9833333492279053,\n",
       "  0.9750000238418579,\n",
       "  0.9833333492279053,\n",
       "  0.949999988079071,\n",
       "  0.824999988079071,\n",
       "  0.9416666626930237,\n",
       "  0.9833333492279053,\n",
       "  0.9583333134651184,\n",
       "  0.8833333253860474,\n",
       "  0.8083333373069763,\n",
       "  0.9333333373069763,\n",
       "  0.9583333134651184,\n",
       "  0.9416666626930237,\n",
       "  0.949999988079071,\n",
       "  0.9083333611488342,\n",
       "  0.550000011920929,\n",
       "  0.949999988079071,\n",
       "  0.9583333134651184,\n",
       "  0.8999999761581421,\n",
       "  0.9666666388511658,\n",
       "  0.7583333253860474,\n",
       "  0.925000011920929,\n",
       "  0.9666666388511658,\n",
       "  0.9333333373069763,\n",
       "  0.9583333134651184,\n",
       "  0.675000011920929,\n",
       "  0.9333333373069763,\n",
       "  0.9416666626930237,\n",
       "  0.925000011920929,\n",
       "  0.9333333373069763,\n",
       "  0.9583333134651184,\n",
       "  0.9666666388511658,\n",
       "  0.9750000238418579,\n",
       "  0.9750000238418579,\n",
       "  0.925000011920929,\n",
       "  0.8083333373069763,\n",
       "  0.8666666746139526,\n",
       "  0.9750000238418579,\n",
       "  0.9750000238418579,\n",
       "  0.9333333373069763,\n",
       "  0.7250000238418579,\n",
       "  0.9583333134651184,\n",
       "  0.9666666388511658,\n",
       "  0.9750000238418579,\n",
       "  0.9583333134651184,\n",
       "  0.9750000238418579,\n",
       "  0.7083333134651184,\n",
       "  0.9750000238418579,\n",
       "  0.7166666388511658,\n",
       "  0.9166666865348816,\n",
       "  0.949999988079071,\n",
       "  0.9583333134651184,\n",
       "  0.949999988079071,\n",
       "  0.9666666388511658,\n",
       "  0.9750000238418579,\n",
       "  0.9916666746139526,\n",
       "  0.9333333373069763,\n",
       "  0.9750000238418579,\n",
       "  0.9833333492279053,\n",
       "  0.9416666626930237,\n",
       "  0.8500000238418579,\n",
       "  0.6166666746139526,\n",
       "  0.8500000238418579,\n",
       "  0.9166666865348816,\n",
       "  0.9750000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.949999988079071,\n",
       "  0.925000011920929,\n",
       "  0.9666666388511658,\n",
       "  0.9750000238418579,\n",
       "  0.9416666626930237,\n",
       "  0.9666666388511658,\n",
       "  0.9916666746139526,\n",
       "  0.9833333492279053,\n",
       "  0.9833333492279053,\n",
       "  0.9666666388511658,\n",
       "  0.9833333492279053,\n",
       "  0.9416666626930237,\n",
       "  0.6416666507720947,\n",
       "  0.8333333134651184,\n",
       "  0.8833333253860474,\n",
       "  0.9083333611488342,\n",
       "  0.7416666746139526,\n",
       "  0.925000011920929,\n",
       "  0.9416666626930237,\n",
       "  0.9833333492279053,\n",
       "  0.9333333373069763,\n",
       "  0.9583333134651184,\n",
       "  0.9083333611488342,\n",
       "  0.8833333253860474,\n",
       "  0.9750000238418579,\n",
       "  0.8666666746139526,\n",
       "  0.9833333492279053,\n",
       "  0.949999988079071,\n",
       "  0.9916666746139526,\n",
       "  0.8833333253860474,\n",
       "  0.9833333492279053,\n",
       "  0.949999988079071,\n",
       "  0.9583333134651184,\n",
       "  0.9416666626930237,\n",
       "  0.9750000238418579,\n",
       "  0.8500000238418579,\n",
       "  0.6583333611488342,\n",
       "  0.6499999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.9666666388511658,\n",
       "  0.9416666626930237,\n",
       "  0.9416666626930237,\n",
       "  0.9750000238418579,\n",
       "  0.9583333134651184,\n",
       "  0.9666666388511658,\n",
       "  0.9750000238418579,\n",
       "  0.9666666388511658,\n",
       "  0.8500000238418579,\n",
       "  0.925000011920929,\n",
       "  0.925000011920929,\n",
       "  0.9833333492279053,\n",
       "  0.9750000238418579,\n",
       "  0.9666666388511658,\n",
       "  0.9833333492279053,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.9833333492279053,\n",
       "  0.9583333134651184,\n",
       "  0.9416666626930237,\n",
       "  0.7666666507720947,\n",
       "  0.9333333373069763,\n",
       "  0.9750000238418579,\n",
       "  0.9666666388511658,\n",
       "  0.8916666507720947,\n",
       "  0.8833333253860474,\n",
       "  0.9750000238418579,\n",
       "  0.9166666865348816,\n",
       "  0.9833333492279053,\n",
       "  0.9666666388511658,\n",
       "  0.9333333373069763,\n",
       "  0.9583333134651184,\n",
       "  0.9833333492279053,\n",
       "  0.9166666865348816,\n",
       "  0.9833333492279053,\n",
       "  0.9333333373069763],\n",
       " 'val_loss': [1.0531922578811646,\n",
       "  1.1448719501495361,\n",
       "  0.8584021329879761,\n",
       "  0.9681841731071472,\n",
       "  0.857746958732605,\n",
       "  0.9924024343490601,\n",
       "  0.8083369731903076,\n",
       "  0.7630128860473633,\n",
       "  0.8819665312767029,\n",
       "  0.7749290466308594,\n",
       "  0.7879782915115356,\n",
       "  0.7998955249786377,\n",
       "  0.899329423904419,\n",
       "  0.8251737356185913,\n",
       "  0.8935530185699463,\n",
       "  0.8333543539047241,\n",
       "  0.8188889026641846,\n",
       "  0.7730339765548706,\n",
       "  0.9453592300415039,\n",
       "  0.7793882489204407,\n",
       "  1.6375070810317993,\n",
       "  1.127280354499817,\n",
       "  0.8138256072998047,\n",
       "  1.36222505569458,\n",
       "  0.7156364321708679],\n",
       " 'val_sparse_categorical_accuracy': [0.5666666626930237,\n",
       "  0.36666667461395264,\n",
       "  0.8999999761581421,\n",
       "  0.800000011920929,\n",
       "  0.8999999761581421,\n",
       "  0.800000011920929,\n",
       "  0.9333333373069763,\n",
       "  1.0,\n",
       "  0.8333333134651184,\n",
       "  0.9333333373069763,\n",
       "  0.8999999761581421,\n",
       "  0.9333333373069763,\n",
       "  0.8333333134651184,\n",
       "  0.8999999761581421,\n",
       "  0.8666666746139526,\n",
       "  0.9666666388511658,\n",
       "  0.9666666388511658,\n",
       "  1.0,\n",
       "  0.8666666746139526,\n",
       "  0.9333333373069763,\n",
       "  0.5666666626930237,\n",
       "  0.699999988079071,\n",
       "  0.8666666746139526,\n",
       "  0.7333333492279053,\n",
       "  1.0]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABcU0lEQVR4nO2deZwUxfn/P88cey/3ciPggeLBJYLxRI0HXoiighqDR4gYTYz5JTExBkkwMSbxq1HjrTGKwRPFC0+84gkRkUMUFWQBYbmW3WWvmanfHz3VW11T3dMzO7Ozu/O8X6997Uwf1VU93fXUc9RTJIQAwzAMk78Ecl0BhmEYJrewIGAYhslzWBAwDMPkOSwIGIZh8hwWBAzDMHkOCwKGYZg8hwUB06kgopeI6IftoB7XE9EjWSj3X0Q0J9PlMvlNKNcVYBgiqlW+lgBoBBCNf/+xEGKu37KEEBMzWTeGyQdYEDA5RwhRJj8T0VoAlwohXtOPI6KQECLSlnVjmHyATUNMu4WIJhBRJRH9moi+A/AgEXUnoueJqIqIdsQ/D1TOeZOILo1/nk5E7xLR3+LHfkNErhoDEV1DRF8RUQ0RrSSiyco+z7KIaCgRvRU/91UAvTyus4qITlW+h4hoKxGNiX9/goi+I6JqInqbiA7web/2IqI3iGhbvLy5RNRN2T+IiJ6O37ttRHS7su9H8XrJto/xc02mc8CCgGnv9AXQA8BgADNgPbMPxr/vAaAewO2uZwPjAayG1THfBOB+IiKXY78CcCSArgBmA3iEiPr5LOtRAEvi+/4IwMtP8R8A05TvJwLYKoT4X/z7SwD2AdAbwP8A+DWNEYA/A+gPYDiAQQCuBwAiCgJ4HsA6AEMADAAwL77v7PhxFwLoAuB0ANt8XpPpDAgh+I//2s0fgLUAvh//PAFAE4Aij+NHAdihfH8TlmkJAKYDWKPsKwEgAPT1WZelACYlKwuWQIoAKFX2PwrgEZdy9wZQA6Ak/n0ugN+7HNstfp2u8e//AjDHZ/3PAPBJ/PP3AFQBCBmOexnAz3L92/Nf7v5YI2DaO1VCiAb5hYhKiOhuIlpHRLsAvA2gW3zEa+I7+UEIsTv+scx0IBFdSERLiWgnEe0EcCCcJh63svrDEkZ1yrHr3BokhFgDYBWA04ioBNYI/NF4HYJEdGPcRLULlmAEPExNSv17E9E8ItoQP/cR5bxBANYJs49lECxtiMlTWBAw7R09Pe4vAOwLYLwQoguAo+Lb3cw9viCiwQDuBXAFgJ5CiG4AlvssdxOA7kRUqmzbI8k50jw0CcDKuHAAgPPi274Py0Q1RFbRRz3+DOt+jYjfmwuU89YD2IOITAEi6wHs5aN8ppPCgoDpaJTD8gvsJKIeAGZlqNxSWJ1oFQAQ0UWwNIKkCCHWAVgMYDYRFRDREQBOS3LaPAAnAJiJuDYQpxxW+Ow2WOanP6XQhnIAtbDuzQAAv1T2fQRLYN1IRKVEVEREh8f33Qfg/xHRwWSxd1wwMnkCCwKmo3ELgGIAWwF8AGBhJgoVQqwE8HcA7wPYDOAgAP9NoYjzYDmTt8MSTv9Ocr1N8WsdBuAxZde/YZmVNgBYCauNfpkNYAyAagAvAHhauV4UlnDaG8C3ACoBnBvf9wSAG2AJpBoAz8By0DN5AgnBC9MwDMPkM6wRMAzD5DksCBiGYfIcFgQMwzB5DgsChmGYPKfDJZ3r1auXGDJkSK6rwTAM06FYsmTJViFEhWlfhxMEQ4YMweLFi3NdDYZhmA4FEbnOdmfTEMMwTJ7DgoBhGCbPYUHAMAyT52RNEBDRA0S0hYiWu+wnIvoHEa0homW8EAbDMExuyKZG8C8AJ3nsnwhr8Y19YC04cmcW68IwDMO4kDVBIIR4G1YCLjcmAfi3sPgAVk75fh7HMwzDMFkglz6CAbDyoEsq49sSIKIZRLSYiBZXVVW1SeUYhmHyhVwKAtNCG8ZUqEKIe4QQY4UQYysqjPMhGIZhHLz77btYtnlZ0uO21G3BY8sfS3qczvxV87G5dnM6VWt35FIQVMJaIk8yEMDGHNWFYZhOxpEPHomRd41MetzlL1yOqU9NxaqqVb7LrmmswZmPn4lT/3Nqa6rYbsilIFgA4MJ49NChAKrji3UwnZS2XvuC19rIX2Ii5vvY3c3W8tPvrX/P9zm1TbUAgPXV65Mc2THIZvjof2CtwLQvEVUS0SVEdBkRXRY/5EUAXwNYA2ut2MuzVRcm99zx0R0I/CGAuxffndXrFM4pxHVvXIeHP30YgT8E8Pf3/p7V6/lh6+6toNmEBasX5LoqbcKba98EzSbQbMKba9/EOU+cg/H3jW/TOqgmm8I5hbjzY/egxJ4lPQEAlz53KWg24faPbnfsv/HdGxGY3dJVLt+yHP1v7m9dp24zznr8LN/1mrVoFgbePBCHP3A4zn7ibHS7sRt+98bvEo77fOvnoNmExRsX45JnLwHNJvT4S/YWjctm1NA0IUQ/IURYCDFQCHG/EOIuIcRd8f1CCPETIcReQoiDhBCcQKgTs2qrpXa/u/7drF6nKdqEOe/MwedbPwcA/O+7/2X1en74ctuXAIAb3rkhxzVpG55eZa+QiUc/exRPrHwCH234qE3rsK66Ja1OU7QJs9+a7Xrs1t1bHd/1jvk3r/8GAgKRWAQA8PwXzzv2q+1Nxh/e/gM21GzAe+vfw5Mrn0R1Y7XxuXjjmzcAAPcuuReL1i4CAOxo2OH7OqnCM4sZTyKxCB785EFEY1Ffx6/YsgLvfpvY2UtVfd1O17xXAKwX4M/v/Bl1TXVJrxUTMfxr6b/QHG12vd7Ohp0AgGgsigc/edB+md2Yt3yefY7ki21f4NWvXk1aH52vd3yNl9e8jK5FXQEA2+u9oqndee3r17B66+q0znVj7c61eGz5Y3jgkwewqWYTFqxegOVbluOttW/Zxzz7+bPYWOPPbffM58/Yo/CDeh9kbx/cdXCr67pwzUL89b9/RUOkAW+ufRM3vnsjaptqIYTAQ0sfwtOrnsaznz+L99e/72ifyuh+owEADZEG/PvTf0MIgaZoE+7/3/3YXLsZxw491j62R7F55L27eTe21G3Bq18nPgu1TbV4ZNkjjm2bajbhmc+fwWebP8Oib6zO3Ot+vPLVK/hi2xeoqqvCf9dby2VvrtuMqt0tkZLZMnd2uOyjTNtyx0d34KqXr0JjtBGXjb0s6fEH3nkgAEDMcj6wsmPWX1CdGc/NwFc7vkKfsj64ePTFnsc+/OnDuOjZi/Bd7Xe45ohrjNerbqgGADz06UO4ZMEl2Fa/Df/vsP9nLO+r7V9h2lPTcMo+p+D581pGffvevq+xTck45N5DsL1+O5bPtCbXpysIjn/4+LSu78XR/zoa31Z/a9wnZlmd5BmPnYFhPYdh9RXeQqgx0ojJj03G/hX7Y8XlKxz7pC1dHlcYKkypnjERw8S5EwEAo/qOwvRnp2NjzUYM6zkM/cv7Y/qz0xPqDlgDEpXuRd0BAC9++SJ++MwPMbb/WDy09CHc9N5NAIAfjPiBfaw0FenUNdXhtP+chiWbliTsm/nCTDyy7BEM7zUcB/c/GAAwad4kfLzxY/uY6O+j2LfXvg5tReXER04EABw68FB8UPkBAEuzUe9hQ6QBxeFi4/mtgTUCxhOpNm+p29KqcmTHvKFmg3EELykKFQEA3l73dtIypaq8qcaKMVBHSyIeiSxH9w2RBgBWZ5+sjsu3LE/Ylg6y45f3Ll1BkIm66LgJAYkUoF9s+yJpWVFhaYsrq1Y6vgNwaFe6CcYPskzAEio1jTV2/VTtReftb53PT01TjV0GYD0P6si+oqQlLN1LI/hy+5fGfdIUqbZd7/BXVa3yfPYlX+/42v6sPouyDtmABQHjSYCsR2TWm7OMjt5F3ywCzSZjxxITMdtpeO//7rW3eZkbZAeumpcG3jwQs99MtPHKuj39+dOg2eRQoW2NoNHq0LoVdQMA7Gzc6Xrt5pj1kspOA3C+iNI81vXGrrjujetw24e3gWaTq7req6QXAOCbnd8k7Ptk0yeg2eQIWXz282dBswlbd29F9790x+/e+J2j8//Vq78CzSbUN9dj6K1D0eMvPRJMaCPvGgmaTTjl0VNc2zn9memu+yS6eQwApjw+BRPnTsQRDxxh/65zl811mNtoNuHDDR8CALoUdrHvPwDH76Ny5INH4tRHT0WPv/TAdW9c59indvZ1zXW2RlHbVJvQ2Uuao834sPJDxzYpAOqb6wFYJk/1t60obREEbr+nVycsyy8OFePdb98FzaaEe/jut++iPlJvPF+9h+qgSzdlsiBgckIwELQ//25RYnTD/Z/cDwAJozMhhKs9Xu0cdGTHt61+GwDrxd1QswHXv3V9Yt3IqlvlrkoAwJKNLSq77iMoCZcAAHbUuzvcpNYgR52AUzOpa7Y63V2NuzDnnTn2/XDTlgZ2GQjAOcKT3Pe/+wDAMSr9+/tWhNOKLSuws2EnbnjnBkdHL/evq16HtTvXYkfDDvs+AdY9lxOoXvzyRddOQ45qp4+abtwPmH+jp1Y9hYVrFtr2awC48b83JvzO0lbes7inw8HpphG8++27eOHLF7CjYQfmvDPHse/tb9+2Bf7u5t22xrizYafRFwUA63etR2O0Eb1Le9vb5G8qf+NILGILfsDSCFZfsRoBCjgGAip1zXWuQkI+AzERw78//TcAy0mtsrNhJxoiDTh68NEAgNJwqb3Pr7Yon8FMw4KAcVC5qxIvffmS/V2+hABAhsng8sWUL5ikIdLgasr4ctuXePHLF4375DmyPC8ThiqkAGdUhSyntqkWkVjEHs1/s/MbzFs+zz5u8cbF+HjDx45rqh3EW+taBNwb37yBTzZ9Yn8fUG5lRJF+j2gsin8t/ZfdMQ7qYs2XVAXBLR/cgg27NtgaktQagBZtSEW1D0t2Ne6yPzdGGrFwzUKsr17v2A4AN/33JttkoRITMRy/5/EOu7iOPpp16wDLCsoSBIEU0D2KezjCOKvqqrCyaqVj0GAKqZURRkIIvLX2LUzc2/IR7G7ebT+P/13/34T2ApY28PtFvwcA7NdrP3u7ahICEkfaFaUVGNZzGCbtO8l4zwHgnx//02H6UZEdeSQWQXlBuWPfsJ7DrLrFmlHfXI8+ZX1w0aiLHJ36NzsStUYTrBEwbcL4+8bj5EdPtr+rgsBEYdBS1RujjY7tNU01roJgyhNTcMqjpxg7F1UQCCE8ncuyw5GonZda9q7GXXbn/sW2LzDtqWn2i3fIvYdg3H3j7GvqqOaDyY9Nxph7WrKlD+hiCQJpC/73p//GRc9ehP97//8AAOWFVoegpjn4+cs/x57/2NOuj+keqJ2NaXSqaixN0SZMnDsRo+8enTDinv3WbAy/Y3jC+TERQ4ACCAXcY0Wkj0DiNmI1CQL5LPQs6Wlra4AlqA/45wGY8NAEAJZgmDRvUkKZcs5B1e4qbK7bjGOGHAPActbKa0kn7CWjL3Gce+fiOzH3s7kAgOG9Wtou76M0zegdqvQRlBeWO+6vysPLHk7aEUdiEft3l1w44kJ7X32kHsWhYvu9kbj5HnT8RNOlAwsCxoG038sRtKoFECVqBNJm2xhxCoLaptqkzk1deABOh2hjtNHhcJP2XYkupFSzj1rOzoadCU46fcS7dffWhDYAVqerjtpVpEYgQ2LlSFLVEABgRZUVwfLE2U8gSEGHyUDtWKRQUOuRTCOQwmtb/TbbBv/YFO+8OTERQzAQ9BQE6v2JxCKukS4mQSDpWdzT4RdQ29UcbcY7377jWc+qOuvcQV0HIRQIYXfzbvt3lILqt0f+FkO7DbXPUQX3Pj32sT/rGoH++0sfQVm4zFUj8ENzrBllBWX29x+M+AGuPepaBCloCYJmSxAUBAsc5+lRTm6wRtCJeX/9+6DZ5BnRkg0eW/4YaDYZR0Cyk3YzDZ34yIk454lzXE1DNY3uGoFk5gszQbPJYYpSz2mINDhMQ1e/fDUCswN2h6l3QKqtXi2nuqHaYe4BEh2X761/z9EG1bTUr8ycHV3aeH/12q8w8q6Rth9id2Q3Dr3vUDy2wtkhdynsYocWSmY8PwP3LrkXv3j5F7btXa2HqVNSOzFVKMiOUwooN7w0AtlBqdfYtnub6/wPL0GgR9+ogr9gToE9acoN+Rv1KumFknAJ6prr7N9Rak0BCjieUemoBmDP3wBgzzuQ93baU9Mc15LCvryw3KGFpRq3r5ohgZYghVAghOZos6URhA2CoMqfIGAfQSfm4WUPAwBe/urlNr3uNa9bsfcbajYk7JMvjGqHVzWCV756BU+sfMJVEJg0AnmsZP6q+QCcphP1nPrmemzb3eIMvWvJXRAQdiegO+M21bakqlLLaYo2JWgE0owiX9TqhmpHG5qjzRBCoKapBv3KzYJAFS7LNi9DaYElGHY373Z0SJIABYxCZdabs3DzBzfb39WOyCSkVTONKSrHrb4SL0EgOzG13KZok9EeD1htchMEqjMUSNQav9rhPfCRgq2ipAKl4VKHRqBeXxUEm2o24Yg9jsDrF77ueN5iIob6SL3R/AcAXQstoVFWUIamaJP9bKU6Ao/EIo5ryHLDwbCnRqCHibrBGkEnRtq6k816TZX65nrMXTbXHtWs2LLCMftSjvruWnyX/ZLKUb98mE0+AnXEo/sIZOeycM3ChIdbd6LJzmbBFwuwYPUCvLPuHbuTknUwhXs++MmDiIlYgiD4rvY7+7MqCOYtn5egAXy04SO8/vXrdnvrmutw95KW8NgVVSvw2tevISZirhqB/nuFA2GrLBc7boACCAetY1TzgS4g1U5XagRqHVSnuDpylx1n79Ledl0AyyG7pW4Lvtr+FV7/+nVEY1F3QSCiEEI4/S0QrmGP9c31jvugRurok8f0TszNFg9Ys6llEriK0gpbIzA5ptUBys6GnTh80OE4duixCQORmsYa13bIMuQzKu+7l5lImp5+ffiv7W2RWMSh+agaQX2kHlERRXG4OOHemEKMTWRLEPDM4naAHHX7TePgl+sWXYe/v/939CjugYn7TEyY9SvtrLd+eCu6FXXD9ROut1TYWLNREMhOUx15ywdaHl8aLkV1YzXmvDMnIRSwrKDMGEv+3vr3bKdh//L+KAmXoLap1hIEDTtRGi51qMQznp+BAAUSfAyOUEolAueWD29JuOatH96KWz+81Z5xes1r1zhGwQff02LC8SsIpIbg9rIGKWh30GpHrc8UVQWB1A4GdR1k33fVF6I6dddsX4NuRd1QGi5FQbDArs+keZMwbsA4rKxaidqmWuxfsT+C5O4jiIqoQ9gIIWz/zDWHX4P7P7kfJ+x1AuZ+Nhf1Eacg6FvWF1vqtoBAjjYCzudGb6eOnE0NWL6G0oK4RhDz1giaY832KFz/feqa6xI0gr2672Wb9ICW32J38270KO7heKZ05Hvbt6yvvU3XCA7qY6XbCAVCdnuLQ8WOdhQGC43+MhPsLO7EyBfSLTQtXaQJwWT6AZydpexcZF2khqDaSOWoSY3kkcfJ/+pLpSNNJ17ERMwuoz5Sj+qGagzpNiThuI01GxM0AvW7m3+i5jc1jlGrbJPX3AY/piEAttnCzY6ragRqJ6xrBOpIWX5W7e1qJ63W+51v38ERexwBIkoIrf22+lt7dLu9frtn1FBztNnWLoAWswoAzJowC1t+uQWPnPkIDht0GOqb6x0DGFlPkzNaRhD9+OAfA/AWBJLuRd0RDoZREi4x+p10QQC0jMJlvYpDxXa7dEHw5DlPYtnMFtOkLEs+9165sWT7woEwPvmxFVYsBUGvkl6I/j6KE/Y6wT5GCnXdR+D1zuiwaagTkw3T0HVvXIcHlz4IIFEFX7FlBWi2MwJImipkR2WKqZeoDtzNdVaceEPUOt7roQ5SMOlDHxMx27YsNYLB3RITdTVGG9MSBOFA2JFOwI8zUB3xqeijM3l9t0ybAQogRFbn4SUI1A7y6leuBtDSuQHAf5b/x/6sagSrt63GkXscaby2eo3var9DgAIJ4beSSCziCEUVsDQCAjnCHotDxQkagXS6mjQOOSCRUT5uE7dUZHkl4RKjsPYSBNJZLAcSTdGmhMgz1UQnywOUJIku0VLqsQXBArutUtgUhYoc9dI1AlUQpJI7iJ3FnRj5EGVSEKhmGf2Fm//5/ITj5Qsh6+I28QZwmiakXV5qBF6j/gAFfAkCeUxDpAHVjdXoV9YvwczQGElTEATDjhdPL8NEt6Ju9qhSRQ9BTFZWMBC0Ba38DyQ62nc1JY6UTdc31cEtYqg4VOzo9Dw1glgzqnZX2aZAISwfQVGoyGGPLw4XO3wEPxz5QxxQcYDdVr18WVepkfnRCPav2B9A3OTYkCgIgoFggiCQAmDK/lNw/+n34w/H/MFul36vdb+VrRGgRSMIB8J46fyXoCPvTzgYdrzDUhCohINhe0CmawTytzX545465ynccGxLmuoJQyYkHJMJWBC0A7LlI5DoL5xpFCzr4CUI5IOvOtBsjUBGGbmMMgHrQdcjSboUdnF8d5iGmuuxs2EnuhV1S+hUGqONCVEoakdsmqVLIMs8owgVN+ehSllBWcIkISAxP3yy0Zp6bbU9ukNS1+C8Om19lKxHo0iKQkXoWdySVdNrHkEkFkFVXZXdYa/duRbPf/F8wshV1wh+MOIHjjLVz6XhUluD6l7c3b6O3s5zDjjHsW2v7nsBSE8jCFAAF4++2H7mmqJNCYIgmUawtnotBnUdhJP2Pslx/1TCgbD9u0pnsS4IQoGQnbVUn1Am72soEEr4/c4cfqbto5o+arptaso0LAjaAbLzzLSPQKJ3LKZOUqrMnoIgPhpUNQxp95XHe7XBpBHoI92YiNlaRU1TDXY370bXwq4JHXY6GoFstzoa95PRs7ygPKHDABJn33pFwQBOH4EqjHQTk67BeQkCXcirbVPRI1VMZUpBX91QjfpIvS0Ivv/w97F62+qE36ooVOTQCEKBkF2mEMJRF+nsBVpSQuuUhEsSOvVLxlxi11+2Ve1EvQSBRHauTdGmhOdIfx7lPZDPxZa6LbZp0E2b9aURKL/3Xj32MmoEoUAoYcZxW8GCoB2QDdOQih9brHxBbGdxPIrBTSMoChWBQHaqBnm8V8caoECC6Uj3QagagcxTo7/Y8npNMacgUDUEr3upm5mSUVZQZhQECZPo4vdZ1XpUDUmNGlI7Yd0BKDu8vXvsDSBzGoGqeehlju0/FvefbiUQlNE9qlNdrzOQqBGEAiFHe3WNQApidaKXXr7s1ItCRRCzhG0aCgfC9v3WBVqCaajQWb68J7qz+P1L3k+YLa87i6OxqN0O/dm1BxaBREGgd+hy/7QDp+HA3gcafQRBChrXa5DXyeYa3CwI2gF21FAsis82fwaaTY487H44ee7JoNmEQ+87NGGfH9OQrhFMfmwyvq3+1qwRNNagZ3FP9CvvZ3fkb659E13+3CWpINBHVXt239PxXRUE0v9g6jhMzmJVqJjSRUjcRs1ulBWUJdiSgUSzkj1iVV5m9YVXO1+1Dm6CQLUduwqCBn+CoDhU7NBYAggkmHHkd7m+g5qaGUj0gRSHi9EQaTBrBBCO8lVBWhgsNPo8gtRi79c7dzfnulEQaM+LvNe6ach0r3TTUFREbeGmP7vyPVKdxV4+AqDlNzVFDQUDQaNGIDUot6CFTMCCoB0g7fORWMSO9HnhixdSKuOlNZYzyzSjVdcIVNOQtMnqGgFgpWA2LaRR21yL8sLyhGX3appqPBfeCFAgwSzw3LTnHC+yGjWkmgIW/2gxZoyZYR/XGEn0Eai4zSAFEke2ySgMFfrSCOSIW9U49NGr7BCCFLRX89JjyGW7ZQeh+zVUdGex23GFoUJPjUCN8pHHJcwM1upZHCr2NA05NAJlNB0KhIw+F9Xxq3fujnuqdJaq8DDtBzTTkBI1ZLpXCYIgFrXfT/1+2OVopqHGiNlHALQ8D+pzIYWDqhEM7DLQXtnujP3OwP2n34/ZE9zXXW4tLAjaAaqPQMb+q8vl1TXV4bHl3onEvEjwESgawZF7HIkh3YbYnZr68haHio2moZrGGpQVlBnj+70iZwIUcIRuAtYo57yDzrO/qxqBdL4GKICD+x+Mo4ccbR/XEGnwvNZ/1//XtcNP1TTk1nHp5qeaphqEA2FHx6ReKxgIOr7vX7G/0fmnRpcAmTENCSEcAwDdWRwKhBJChxMc9JrgLQ5bE6PkmhTBQNAxf0E3DanbTYLVr0aQzDSkz6GwTUNa1JBJM5Rab0zE8MpXr2Bd9TpXjcAuRzENyWu4+QikkDKahhSN4Pg9j8cBvQ+w63Tx6ItTXuYzFbIqCIjoJCJaTURriOgaw/7uRDSfiJYR0UdEdGA269NeUU1DciajGqFw9ctXY+pTUx3pIbzQTT96VIragQUoYNt61boAluagHis/1zTVoLyg3Djj1muGZIACDnPDz8b/DABQEGh5KVRBIE0m8sVW7c9qPhgTuxp3uXb4qZqGghQ0dlw6NY01CAedgkA3DenXNpkCpAA0mYb0Tk/XStwEge7ET9AIFMHgJgj0+y3r/uzqZ+3j3UxDukZg6lSDgaA92EjQCIJmjcAkCPR1M+RzoJuGkmkEJz5yIjbWbLSfP9NgQNZN1s/NNKRrBOrvVBS0jlU1ArffMVtkTRAQURDAHQAmAtgfwDQi2l877LcAlgohRgC4EMCt2apPe0aOQiKxiJ1kTZX+39VZtnKvdYP7l/e3P+sOWL0TUF/oAAXseHDA+XLIRV0kspOvbapFWUGZsUNNphFI09DVh16NW066BYDzoY+JGAqDhQhS0KERAM6RnslHoOPW4bsJiBuPuxELz1+YsD0UCBl9BDpSI1AdkAmmofi15ejca5QnR4oEsjsSr/BcwL3NyZK1hQIhu27yd9bvk/5c6SNvL9OQ6hPQhaVdXgZ8BAEKJDiA5fO1u3m3410w3StZlhoaLO95r2JnOvJ0nMUmjcAe6CgaQVsLgmzmGhoHYI0Q4msAIKJ5ACYBUL2g+wP4MwAIIT4noiFE1EcIsTmhtE6MmlZZzuhUna7yJfKaXq6+6PrsSd2EkSAIFI1Afblrm2oREZGE86RpyNShJhMEcmStailqZygTohWFiuywSlsQKJ2g2zKFKqlqBGpH5lZvL2oaa1AQLHCMSN00Avmbe4ULyn3q6N20JoSKWweiPzuywwwFQojEIg4fgTQBJdOcdKGkRw25+UpCgZBxtTs1asivaYiIEgSBjrwnetCEl7NYHXTJbbrz3MtZrAt4+T7L7apglPdM1QhSNV+2lmyahgYAWK98r4xvU/kUwJkAQETjAAwGMFAviIhmENFiIlpcVWVeALsjI0cpUdFiGlLNO3Jk6OUAjcQi9sOoR7NEYhGHYFFHdrpGoB5X01jjECKyk69tqkV5Qbl9vfKCchzS/xAAVidy2rDTjB1ngAK2eq06sNUXUkDYgsA2DVHLiCkVXDt8cvEdKE4/tc5E5F8jCHr4CAxpF9wEQWGw0G6vQxAYOlAVN0GgPzvynspy/fgIdEydtZtpSG2n2uEP7jrYDvdMx1msH2vSmGS7EuZcGDpbeX9VQSB/B7cFimR7CGStRyCiCWXbgiBe9+EVw/G34/+Gx6Y81i40gmwKAtMTq8ct3gigOxEtBXAlgE8AJASACyHuEUKMFUKMraio0He3axasXuC5MPWmmk12xI/qLDZpBPWRerzxzRvGRFjNsWZ7lq5cPFwSiUUcGoP6OUhBh0agzm7WTUO2RtBkaQTqi37pmEvtevQt64vzDzo/oY4BCtgdqvpS6i+NFE4JpqEkZhGJdEhnSiMAEmegmthStyXBWZygEWh1cjMNqblqVE0iXY3AlLUTaOnsVR+Bm2lIJxXTkC4IZDu6FXXD9FHTrfLScBbrx5oGC24agZdpSF1r2TYNuQgC2RapXamp1CVysCfrHqAAfnHYL3DOAec4hLKsa2cSBJUABinfBwLYqB4ghNglhLhICDEKlo+gAoC/xNwdgC11WzBp3iRMeXyK6zGHPXAYXvnqFQBOE44qCFTn6XH/Pg4H3XlQQjnqotm/fPWXjn3RWNRhslEnYukagVoHPRw0JmJojDRid/NulBe2aAQxEXO8qG5RLgEKYPxAaz3aGQe3hILqL6+racjwkl928GUJ2+RLlKqPQLX1SuQIcVTfURjabSguH3u58VxJfaTe0Qm4hY/a+900glCho1P0qxG4tc2UzgFwagbyXDeN4NojrzWWIQkFQq5RQ7ppSJ6rmnY8NQLlvumOWPVY0/1xFQQezmKZOkXWC0icqKYTClirkJkEga4RqKjBEHIwl2pAQ2vJpiD4GMA+RDSUiAoATAWwQD2AiLrF9wHApQDeFkIkz0TVQZCd65rta1yPUVM6uwkCqRFIu7pppnBztDkhb4+9L9bsiObx8hFEYhFM2X8KBnYZaGsEI/qMwI3H3QigZRUsNf9PTMRcZ9CqBCiAvmV9IWYJnL7v6fZ2/eXVTUNuGkFJuAR3nnonxCzhmGOQbFTlpRG4mZ+OHHwkvv7Z17jjlDvw5ZXuC43XNdU5Ru1u4aMmZ/GXV36JMf3GWNuDhQjAIAjS1QgMzmJA0QgoUSNQO/ILRlyAOcfOcZRh8hH4NQ3J35xADhu5W9SQW1n6sab7I++5H41Anu8wDcXrp09U01O1eGoEMadGoKKaPsf2HwvAO/11NsiaIBBCRABcAeBlAKsAPC6EWEFElxGRHMYNB7CCiD6HFV30s2zVJxfIB0V9OCt3VYJmE95Zl7hwt2qWUQWBHAGp6qrjOsJavtEtvC0Si2DYbcPs77ogKAoV4dvqb0GzCd/VfmfHeUtBEA6E7QdYviCqINBfei+NwA9SOEnTkJ4QT+KYWKR04KZ8PipeJiM/k828jtndvNsh2NTOJplGEAq05JrRTUOt9REkMw0l8xGYRrKePgLdNKR0gLoJyJdG4OJ4Nh2rI+uhD6BMpkaTs1g+W3rKjUFdLIOHfD/lcpSt0QiOGnwUAH/JEDNJVucRCCFeFEIME0LsJYS4Ib7tLiHEXfHP7wsh9hFC7CeEOFMIscO7xI6FHOGrD8Vba98CYK2/q6OGtqmCQAoSfYUn/Tpuduzdzbsd4XC6IFAfzh0NO+xwyZqmGtsJLY+RwqhrYVfXlz4TgsCoEWijdb1zkZjy+ah4bU8wDXmMME1IZ7epXqb7ordBnXlqFARJNAI9KumGY29A79LeiUs8asLVNI/ALXZfL0Otv+N3MJwv/QOyHeoiOr59BF4agUFQEhEKggUJ6ThM91KWpU7Uk9uG9RyGuWfOtbc/etajeGTyI3ZOqFR8BCqqRnBwv4Px8OSHcfOJNyccl014ZnEWkeq4WweoLtoOuJuGpKagC4JoLIpHP3vU7tjdTENu9ZJ100eRcgKV1AhCgZAduSTr0K2oW0uno2kEat59ADio90H2tUzoKrYUBOp3WS8VN40gqY/Aw3fg5iNQMQkSadJR66t/NpnM9DZIM2Bh0CwITLiFbALA5YdcjsJgYVLTUIhCCYLAy0Grt00e72oaCrUIAvVcmRpctj8dH4H6G7kJynAg7Gv9A3ldNbxZvb/qLPgexT1w/oiWoAgpCGQItIpfjYCIcMGICxK0j2zDgiCLyA7aTZ0feddIx3c305AcTchkYJKb378Z5z99Ph769CEAiYtsuKGHj+qCQM783N28u0UQxDsoWYeuRf41ArdZsW5IB7bELXxU7RxMI9FUTUNeUUP6cTrqUpJqZ6R+Ns0jUO+9KnDdTEOme2ivLqdMZpOLsZSGSxGggK+oIXtCWSQxashkcvLyEahlA06NAIDDF6A6rFurEbhRECzwlYXXtOaG32i1UCCEiLA0Av1Zle+21330+35kAxYEWUQ63ZKp8xL1ZVUFgdQU5FJ/8oGRTmip8vrVCFTTkD56B1qcptFYFM2xZkcHJTOC6s5ih0agjXyTCQI3Z7H6XZarH6e2QyJfNrdOPZXwUaNpyHC+QxDA2fmrn3UhpAszWyPQTEPyPNOgQqZvUMu66tCrIGYJe15DKs5iv6Yh/ff0yjWkawSqacjkL/DyEaQaNQTAaBoyYdQIfM5fkVFDunkQSJxQppLq/JhswIIgi8gO16+kN62wde3r1+KGd24AkKheyiUNf/vGbwH41wh0H4HJNBSkIKIimqgRKKYhv+GjskNJ1UegfgcSXxh10p3JPOJlJjBuNziL/ZqG1Kyq6hrLjmgqRejK3zdBI0hiGjLdQ5nQzc1RHKBAoo/AY0KZKWrITwfmpgnK9qjbVNOQWo78zdKdR+BGOJiaaUidgJeKRuD2zts+ApNpyGf52YQFQRYxPRRei0yYVtj607t/SjhOvvB6Mrl0NAI301AwEERMxKyoIWWdX6kRdCns4moP1rUM2fGm4iNwTME3JJ3TzzNFDanXe+iMh/DZzM/s9pnwaxoyCRJVcD0y+RF8b+D3EupgsvXr4aXJTEMm4SZTHHgJgpQmlEUMgsCHRqCfY+q8dc1G1QhUf4GXINAT1iULHwWse2RamU/H1CZd4L32g9fsZ0klHAi7CgI/GoFfy0E2SP7UM2lj8hF4LdyiT95yQz5Menppt/BRr+uYzBXSVhuNJWoE39V+Z88qdnvpM+EjaI1GIDtE9XrH73k8+pX3M5YjMTmLTZiOUa/fvbg7Lhx5Id6vfD8l05BDI0hhQpkUnG6ajmoakpqeMXxUm1DmFbKpt9m0zeRj0J8F1Uegb1dxLHuprQvg1zTkB1NnrLfzuD2PM57rpRHI99mPryUXsEaQReTISn24vJKyJVtzVyJfal0jcFs4w+s6RtNQwDINSY1A9RFU7qq07eH6aFYtMxUfwRn7neH47iYI9A7YTSMwCQL1XLfOIhQIJbyUpo7BLexwQPkATDtwWsJ2u46KHV4KMT0XkbzPBcEC3xqB1CS8NAJpmlDzF8k2y2urWTrVfUAGNALdWSx9BIoWoPsL3MrVl4z0ZRrymcTNVJbfAYynaSjuLDYKT/YRdG5MD4XXqlpu4aM6crKJHgURDobx6WWfGs/pU9oHn/z4EwD+ooZk56FrBI3RRntlMreXXk+ulsxHMKbfGMw7a56jTm7ZGVWSaQRqh2+yQ+v4nVBmIhgIovLqSjx61qOOa+sCUr+2w1mshI8KIcyCQJ2sppjcikPegkD/rM8jkJFiBLJj6FP1Eejn+AofJUq4R3o71bbq5ert8zIN+cGPacgNPxqBsXzWCDo3JtOQ18ItfjWChkgDhBAJpiFVvddR9+lJ5zyjhqLOqCGgxRnqZRrSTR6Atw1U7wx8mYbcfAQGn4Rfk48fZ7EJ/QVXR7ymY0zOYrXdauSJej/Ve6iO5ovDxa7RUKb7YPIREBHKCspsjSDVqCFZjn4t9XxT+KjJR6A/K/rzlaweOvI+e6X9disrFWexfL/1cgZ2sZIq6xFPQPvQCNhHkEVMo4NMmIYASxgkaAQeNm51tOs3akiahvTFxpNqBIZlEAHvTlW3E/sJH3U73yQI/LzMxgllPh14CSYlQ84cP7OUpcBVZ6e6aQRqp14cKnatq7pdj1lXM2cC1pwE+Vx5jcLVslRS1giUXEN6+1Tc5oyoZXmdLzvbcDCMdy9+1zWlu+l8vx11MBBMmA0veeqcp/Dq169iQBc9E3/70AhYEGQRWyNQXkQv01AqgqA+Up/gI1BDAHXUkX8qpiG5ELeqEci1ilN1FntFbegagXo9X87iJD4CPy+z1IRSIRwIoznW7HpesvL030sKXFdBYNIIAkEUB4pdnxnTfdA7KtkZlReW2yHCDr+KRzoGlZR9BGT2Eegdsl+NwE0YqvdSJnbzOk7Fb0cdpKD9bunlVJRWOGYlO85jjaBzY08oM5iG9HA+IEVBoK1CBvg3DekagSlqSGoEcv1VmWc/JmJJNQK9TD+OOl0jMKWP8AwfNcwsTtVZbFpCMZlpqDBUiOamZl+mIaBFm7r60KutaxruPeD0EciVxPQ2qe0c1WeU8ZnSz9E1Ar2DVvNVeXW+gNkMpP4OxvBR7bcJUKAlfBJkvA7gHpggz0uGmxPa7TiVVDQCU36xpOexRtC58XIWmzQDx4QywzwDFVN2QnURba996rXdooYCZIWPyvVXicjOCCp9BCY/AGBwFvsRBB5O1XQ1ApNJRN+uYjJ/JDMNdSvqhtqmWlfTkH5+16KuELNa6q3/XvL4mIg5hImXaShIQdx28m2udTQJRFlfuda1vUB7fFKi3ikni6bZ/qvtjvLVtgDuPgI/19HLdRO6ark6bgEHOn41HxMBCiTNL2asWzvQCNhZnEWMpqG4RmByGqeiEVzx4hUJ2/xqBGqWUzfTUDAQRFO0CVERte310lyzR9c97OPUc9Qy3YSEG15ZOt18BK4agcFH4MfWn07EULeibgnXUkk1bbQsR3UWqxqBit/5GV6moQHlls1aBh5IjUA6j01l2GVps6bVOul4pZiQ5ejrD6t4+Qj80CqNIAOmoWTnAf4DE7IBC4IsYowa8tAI3HINqQztNhSAcwUliZezWIYH6uidtjw2SC2OL1sQhIrRp7SP/T2jzmJNI9AXutGPAdw1Ajc7eDLSWTBcXW9XRXZ0yWazmpbpBJymIcAceSXPTTai9DINyWgWmcdKTkpUU0ID7hPZ9M9udfGbYiIdjUDFj4/Ai9aahtSJe35pDxoBm4ayiOzspd2wprEGj614DADw1rq3PM/d2bATz61+LmF7z5KeGNV3FL7cnrhKlpezOBQI2WkLVCETDAQRhPNBlJ2AFEyqRiBNCbJM02fdR5ApjcBzQpmP2anJMJqGkozSpEbgZhpKZuJLMA2hxTRk6oi9QkHd8NIIpCCo3FUJoMU0FAwEHW1P1kHquYt0EjQCl/BRP2a7dDrOttAI1HeGfQSMjdQI5MPxs4U/85UKFwD+8PYfjNvDAcvWbwpDDQfDrqPan43/GYgI5QXlCYtu6C+vHj0jBcHovqNts5A8TuLVkftZfzVBIzCMEvUX5jdH/MZ4frIX6+jBRxu3p7NObDLTUDL032vcgHEAgItHX4zlW5YnHO/mI/DCpBHI+yXXkL5o1EUAFNMQJTcNmTQCkyC4fOzlrikmVNOQvl1FvU/p3Gs74CCJEGlV+CgFO6yPgAVBFrEFQfzhWL9rfavLlJ29XNhdxS38UXVOlhWUJQgCt1xDEikI5k2Zl3Cc6bOarkDd5xk+qtmbTSNAtW1qm/Tzk72E+/TcB2KWwJi7x+CT7z4xtkGSzLcgE/2l+zLrwmdQ10F221ZWrfSsh9+srl4aQa+SXo57KTWCAAWSawSGe57gx1Ed44GWgUqyFBM6boMOHTcNrk18BIHW+QhyCfsIssBdi+9Clz93sR3Cq7etBs2mjCxILc0/+hwCwJ+NW09M5xY1pD6cptmQ+vV0U45aZjpRQ346Gbfz/XbK+stqqmcy05Cbo08udN6zuKfn+V6pD9T6yeNMprlUfATJzElSI2iKNiV1tpv2e5kBC0OFCdf3HT4abJ1GkI4gSNXEGKQ0w0fbgUbAgiALzHxhJmqaahJCPE12/VQhEEIUsh25Kn5s8fq6xiZBUBwqdjzIpjwz+vX0EXw2ooa8Xi6T4Ehmn3fLWbPw/IV4ZPIjSevsxeT9JuPOU+7EnGPneB7nJSTV9vYv74+HJz+M+efOt7el4yNIdo4MKGiKNiWPGjJ0YF51kemyAZc01H41Ao+ggUw6i92CFLzOTcc0lK5ZMZPkvgadhI01G+2F6SWmzrq1EBHCwbBx8pAfG7e+eI0paqgkXGL0Eeh4OYtNpiEv9JGYyebvZaYxjeSS4Rbpc+LeJ+KgPgclvaYXRITLxl7mmCFtwus309txwYgL0Kesj/29NT4Ct3skhX5URJOahkzbvO5XYTBRI9BTTCQTBKpz2USyeQTJng3j/JUUTEPS/MmmIQUiOomIVhPRGiK6xrC/KxE9R0SfEtEKIroom/XJJmPvGYsJD01wbMuGIADcR5F+HihdI9Dt+UBcEPgwDbmN0vQy03EW+w0XdLs+kDx0Ux536MBDXfe5MWHIBBzY+8Ck9UqGX43A69x0zB1u7TPleHK7htc9uubwhNcdo/uNxv4V+wNo6XATooaSrCfwx2P+2Kp5BCkJTZ8OZvv4FPxUjvM688I0RBQEcAeA4wFUAviYiBYIIVYqh/0EwEohxGlEVAFgNRHNFUK4Z2Zrp8j8LCqmNBDH73k8XvnBK9j7H3vjqx1fpXwdArl2rH5WYPLjIygtKDU6i3XcOutMhI+m+lKp4Yt+TUPyuD8d+yccM/QYx75kdV70w0UAgCtfvDJp3bzwuk5SQRBMfR5BMhOJ+lsnm7HrVobuyJe8cN4LCeX5TUMdoIBd7pKNS4zl63U21TUt01AKE8pM5fg9r7NOKBsHYI0Q4ut4xz4PwCTtGAGgnKxfrwzAdgARdGDUzseUBkKGG6Yzi1XSKo0gnOgj0MtLxzSkl6kKF3uE7tExe4WP+hmRyWMKg4X252SCUWaCNK4aFWibl9OvuctEqj4CNSTUlyBIYR5Bqpi0AC8fQWuvm5YgSFEjSMc8mUr52SSb4aMDAKjxkpUAxmvH3A5gAYCNAMoBnCtE4pRaIpoBYAYA7LHHHvrudoWavsGU6tZtJmoqmDrgnxzyE3spRi/8OItLw6lrBCrBgNM05Ksj9+Es9nO+utZvMu485U4cPfhoHDboMNfycqmu+xUEfs0dfjqqVExDrXFyqikmVIHtp8y0fAQ+Z5u7pfn2g58Z0iY6u7PY9IvoQ7QTASwF0B/AKAC3E1HCCuxCiHuEEGOFEGMrKioyXc+U2V6/Hbd+cCtWbFmRsE9d9MVkGmrtBCTAbHOfPmq6r3NNpiG9o9Z9BG6Lebh18LoD2o+m4id81M/56lq/yUxDXQq7YMbBM1yXf8w1GdcIfGhZ6m/tMA35WMs3FUzCySt81O91k5mG0knHkW3TUHsgm7WtBDBI+T4Q1shf5SIATwuLNQC+AbBfFuuUEeYtn4erXr4KV79ydcI+NZrHyzSU7oNCREbTkN+H1RQ1pFNaUOrLNORGkIIpzwTNpEagOotP2OsEO41CKrQLu61PZ3EyrcVhGjKkqlBJxTTUKo1AyT6qCuzWagSu56D9mobaA9ms7ccA9iGioURUAGAqLDOQyrcAjgMAIuoDYF8AX2exThlBmnxMk7qSaQQVpZZG05oOxqQR+H1Y5UQniT16Vhx8+jyCVAWBrmWkavdtzfmFwULH8S9f8DLW/zz1Gd3tIZLDr0bgtxw1o2iqzuKs+QgU0xCRe64hv9dt7czi1oaP+qljeyRrPgIhRISIrgDwMoAggAeEECuI6LL4/rsA/BHAv4joM1impF8LIbZmq06ZQs4eNJkeev21l/3ZpBFUlMQFQSs6mNZoBFIjsc8zPLD6zGK3CWUSU4SH13cTXhqBH+T5hSHFWZzENORFexjRZVoQtCsfgaKZyN/Ja81ilXRMUn59BEbTUJbDR1vznGaKrOYaEkK8COBFbdtdyueNAE7IZh2ygRQEftYV1smERmDqANSH9a3pb2HDrg047+nEpfF0QeD2wKrbvdIgzD93Pg6oOMC1Ll7XcDtH9xGovHDeC/YqX6ZrpOIs9kLtnHKFX9NQMmQbTKm9ddxMQ8ZkbBnwETiu4TNqyNNZnMnw0RRH9ek6i9sDnHQuDaT5J5kgMO3vVWJpDGnPWHWZR6C+lEcNPso1r5GMWpK4PbB+H+oz9jvDUYeoiCYc7yc3v1+N4OR9TjZulwn+VNOQn3kV7ZmsmIaS+AhU7S+V7KOp0pqoodaYhtKZhe2X1voIOuWEss6M1AjUUFG/2Kah1vgITKYhn6NwvxpBOqO9HsU9ULW7yjU3vxdePgI/SO2rMFToa95CMuR9+vXhv067DL8MKB9gz7hVSXbf0jENqR2wiZRmFmfAR5BO1FBazmK/PgLlnqesEXTgqCEWBGkgBYFpTYBkdC/uDqCVUUNJNALA/SF2cxbrpFM/KQjSObe1PgKZ6TVTpqHCUKHrDNlMU3l1pXF7UtNQ/DnwO4Paj3DvCFFD6YSP+s01pO5P2U/VgU1DHau27QQZIpqOIPDjDEtGazSCdExDfulR3ANAem3z6yNww07TXNbf98zi9k42TUNuqM9WNucRmDQTIvdcQyrZ1AhMpiG/mmW6GoFMTNi7tLfvczINawRpIDUCNVSUQMaOR93+l+//JSPXNzqLdY3A5SXVI4DUB/adi94xbvdLz5KeAKxlNlNFnciUjkYwce+JuH3i7Zg+ajpeWvNSytdvj2QzasgNN79ApjUC1Vls8hH4Xd9aJ6mPII28TH5J99yD+x2Mu0+9G+cccE5K18skLAjSQDcNLd+y3HX0WRgqtO3Xp+97ur092/MI/D6IqsA4Yo8jjNv9Ihdh2V6/PeVzW+sjICL8ZNxP7POB9hGW1xoyrhEouYb83Jts5hpKFj7qRTajhtTz/aysp5KuaYiIMOPgGb6PzwZsGkoDqQlIQXDQnQe5HuvmfGtV1JCPeQReL6lcXlGvk9/z3ZBpLkxpnVMhHY1AxW8a6rZiRJ8RaaWs9hs+mqydptGwn3uTzaghdeQ/tv9YANbzk46PwM+71Cbho+wszi9s05BhcRgd3eQhSVcjcHUWp6ARVF9TDZrtHUaYzoM8YcgETwer3xF6Oj4C/fz2xKeXfZrWeVnRCFJ47rI5j0D1EQzuNth+bl768iXHfhOeGkErk87lwjTUHuhYtW0nRIT/qCHVJp+JB6VXSS9fGoHf8jMZPupGqtpPOqYh/XygbUxDMgpMz+qaCZL9hqnmwMl0x5YJjUAvI515BH5+Z7+RU47w0RTfgY4cNcQaQRropiEv1Fm56ZqGhvcajufPex4L1yzEBSMuwGebP0s4Rn85/D7EmdQIMkWAvJcjTIafCWyZ4tojr0Wvkl64cOSFGS87ldGrn+McpiGPzvOVC15Bj+IeSU1DrYl8U8NHUy2zNT4Cvwn6ADYNMUmQpqFILJJ0NOJI65tE1XZj2oHTsGf3PXH5IZcDcHEWZ1ojyGHSrNa+RPLetoVGUBgqxE/H/zQrZfsVBKnMI/AjJI/f63gAQFVdle+6pIpbx+wrashrHkGSqKFkqPVJOXzUsFxqR6Fjia12ghQEQHI/gZtpKJXRlN4p+5lH4Lcjb+0EnGzQakGQw6n6mSTTGkGAAikJyUwEN7jhlgW1tVFDbqTTMV8y+hIA1lrLfmAfQZ6hCoK6pjrPYzPhLNYfYqkRqBFJ6WoEbqQyGzXTdLSXKFtk0zTkh2SmodbgZhrKdvhoKkzZfwrELIE9uvpbFbEjm4Y6Vm3bCaoW0Ptv3rMBs6kRlIZLXctrbdbMbOTj92uzz5hpqJ2Ej6ZLxgUBBe3st3I2qxfJ5hG0hmSmIc96pfFM+jE56XD2UcYTVSNQP5twdRa3QiOQD1xpQSm21W8zntPaDjzdSBOVZZctw7rqddhRv8PX8XKWaaZeos4+ocz2EaQwj+DuU+/GhMET8L2B30vp+hnXCCh9jcCz3CQ+glQGB6kOplgjyDOSdf4qrqahVmgEMlqpJFziu4xUSXW9VhMH9TkIpw471ffxcpSaKR9BZ9cI/HZUqlDvVtQNMw+Z6ev5c+QByvC6DG4DjdZex9XnlUbwQzphz6bPHYGOVdt2gppjKBmupqFWaAT9yvoBAK4+9GrfZaSK3wk4fpAzjacdOM3zOOnzaHVnkMPFZDJJNkxDqZBN05DtI9BNmknSZKtcNf4q39dri46ZTUN5RroaQbJsjm7oo5nuxd3tmZgzns9OjpJ0HYwm9um5j6+UzlIQpLPOQ2ekMzuL3Wz2foW42/OUzDSUzUECm4byjIyYhnw8kNIZnIvIHXnNthxdD+k2BAAQjbVOEKSSWK09k6l5BMlWJfNz/az5CDIcluqWyjkdH0GqsGkoz2iONXv+0LeceIv92W/U0K0n3ZpQjlwlK91R+eNTHsfqK1andW4mnMWpMv/c+bjn1HswuFvimsSp0FmihpJ1kn470UyYhjLdYWfjuRo/YDweP/tx4762GEx1ZNNQx6ptOyESi3g6aq8Yd4X92U0jUD8fM+QYHNQ7MYOpLQjSfIjPPuBsDOs5LK1z5UPdljOMe5f2xo8O/lHGyuvoGoHErR0d2TSUDU3zR2N+lFQjyCZsGnKBiE4iotVEtIaIrjHs/yURLY3/LSeiKBH1yGadMkEyQaA+BH6dxaYHRwqCXIxsczmzuLV0lpnFyUg1n1Sqqb2zaepwKy+dZ/3cA88FABy+x+EpXy+TdGSNIGvOYiIKArgDwPEAKgF8TEQLhBAr5TFCiL8C+Gv8+NMA/FwIkfqqJm1Mc7TZIQhKw6Woa26ZYax2RH7DR02jNSkIks1ezga5MA0xTtwia+z9KZqGygvK07q+WkamSGr2SkFjOGGvE5IGI7TFc9yahHW5Jpt3ZxyANUKIr4UQTQDmAZjkcfw0AP/JYn0yhq4R6Ms/qqgTytySzrlNopKCoKappjXVTYtMho+2NZ3FR5Ap5G+oLkjkh2zOI2jrEN+26Jg7smkom+GjAwCsV75XAhhvOpCISgCcBOAKl/0zAMwAgD328Jf3I5tEYhFHnh/Z2Xcv6o73LnnPcWzfsr72Z6+Hw1MQNLa9IMhlrqHW0lmihvySTODJ/alqBH5MQ89Pex79yvulVK5aXlv9Rmwa8iabgsAk8t1+9dMA/NfNLCSEuAfAPQAwduzYnL/dboLgrOFnYb9e+zmOlSGRQBLTkKHDlYKgtqm2tVVOmY7sI5B0do3A76haPj8pawQ+TEOnDDslpTLtstvYj8POYm+yWdtKAIOU7wMBbHQ5dio6iFkIsMJHVdu//GzqeFQTkvpwTB853f58yehLjA/OxL0nAgBO2/c017ocv+fxDqHkl9P3Pd1zf3vwEezZfU8cUHFAyudJYSzTCHdWxg+0FOxkM7Z3Ne4CAJQXpugjaIMJZW1FW/sIOpogyKZG8DGAfYhoKIANsDr78/SDiKgrgKMBXJDFumSUSCzisP3LzyY1100LOGv/sxwOrqXfLbWOiSdeA6xFz5M5wV75wSupNwDAs1Of9dwfEzEAuX2gv/rpV2md17esr6+ZzB2dvXvs7audUhCkqhGoZHw9AhdtJlumIp5H4E3WaiuEiMCy+b8MYBWAx4UQK4joMiK6TDl0MoBXhBBtHxqTAvOWz8PwO4YjGou6moZMGkGqsd5qiF+q4X6ZpD0IAiYzyGCD1giCtqYjTGDT6cimIV89DRFNBvCGEKI6/r0bgAlCiGe8zhNCvAjgRW3bXdr3fwH4l98K54ppT1nq986GnWiONjsihUyCYP658zG021Df6SjsbJ+BoL3eQS5D0KQg6GhhcEwitmkoRWdxZ4JNQ974re0sKQQAQAixE8CsrNSonfHxho+xsabFtVHdWI2oiDp8BCbT0Bn7nYGRfUe2KkMkawRMJpBRZ+1RI2grhz5HDXnjt6cxtSovMpeOu28cygrK7O/bdlsLwTicxSF3Z7HfB0I+RGrnn8uHaWCXgQCAmWNn5qwOTGa4ZPQlWLJpCfbusXeuq5KUfXruAyDzjn42DXnjtzNfTEQ3w5opLABcCWBJ1mrVzlDDN7fu3grAOYlMLh2ZzFnsRSbTPmcCNdU107GZechMzDykYwj03qW9O+xz15E1Ar+1vRJAE4DHADwOoB7AT7JVqfaMLQgUjUCO4k0agd+OPRMrgjGdCznY6FncM8c1YfzQkX0EvjSCeERPQtK4fMSkEdiCoBNpBEzuGdlnJG6beBumHjg111Xp8LSFL0IdxHW0VfJ89VJE9Go8Ukh+705EL2etVu0EU8cuBYEaPioFgXSwqvgVBDJcjjUCRkJEuGLcFehV0ivXVWF8oA7iOloGXL/6S694pBAAQAixA4A58XcnQoZxArBnuJpMQ/IBaI2zmMM1mXyirTvKthihd+TwXL+CIEZEdrY3IhoC97xBnQZ1kfqK0gqUFZRha32iaUiO4ltjGpLLM+YyZJRh2prOlBiwf3n/XFchbfwKgmsBvEtEDxPRwwDeAvCb7FUrt7y//n30uqkXNtdttreVFZSha2HXlJ3FqfoI+pT2SbveDMOYaQsfQUczB6n4dRYvJKKxsFJBLwXwLKzIoU7JDe/cgG312/Dm2jftbWUFZSgvLMfOhp0AMq8RDO0+FP88+Z+YPHwy+v099bS+DMPknlU/WYWPNnyU62qkjN8UE5cC+BmsDKJLARwK4H0Ax2atZjlE2ukbIg32tvKCcpSES7C93sqUnWmNAECHifVmmNbS1lE1bXW9/Xrtl5CKviPgt5f6GYBDAKwTQhwDYDSAqqzVKsfIEb4qCMoKylAaLkV1g5VpI9NRQyon7X1Syucw+cNpw9zTkjOtp6KkAgf1PijX1WhT/HomG4QQDUQEIioUQnxORPtmtWY5RHbeJo1AJvBS01CnuvJYMl46/6WUz2HyhwXTFuS6Ch2OVHwEW365JYs1aZ/4FQSV8XkEzwB4lYh2wH2RmQ6P7LwbI432tuJwMUrCJYgKK7onHAzb+zIxoYxhGCZX+HUWT45/vJ6IFgHoCmBh1mqVY6SPoD7S4g8PBUIoLSh1fNePb62PgGHyjbbKPtrRZvq2NSkHrQsh3spGRdoT0kegJpsLBUIoCbUsOykTzcl9AGsEDMN0TLiXMiBH+NIfAHhrBAf3OxgAcN5BCStxcsoIhmHaPTyN1YAcxcsl/oC4RqAsRK8Kgj277+maOpc1AoZJpCNPvuqMcC9lQI7iVY1ARg1J/C4gw4KAYXKPDPfuWtg1xzVpn3AvZUDa+uWcgTP2OwNTD5yK0nCLaUiNGvJKFMeCgGFyzwl7nYC/Hf833DbxtlxXpV3CpiGNzbWb8X7l+wBaNILrjroOwUCQNQKGyTBtlXSOiPCLw37RJtfqiGS1lyKik4hoNRGtISLjwjZENIGIlhLRCiLKeUTSmHvGYNXWVQBaBIGMEHINH/VwCLMgYBimvZM1jYCIgrDWOD4eQCWAj4logRBipXJMNwD/BHCSEOJbIsr5Ggcba1rmydmCIG4GUjUCNXyUNQKGYToy2eylxgFYI4T4WgjRBGAegEnaMecBeFoI8S0ACCHa1dzuxqg1s1h2+mwaYhimM5LNXmoAgPXK98r4NpVhALoT0ZtEtISILjQVREQziGgxES2uqmr7XHdSI1CdxaaZxSY4TI5hmPZONgWBqQfUPUMhAAcDOAXAiQCuI6JhCScJcY8QYqwQYmxFRUXma6pgGsHLBHMO01DQn2mIYRimvZPNHqwSwCDl+0AkJqqrBLBQCFEnhNgK4G0AI7NYp6Sonb2kNc5ihmES4dw/7YtsCoKPAexDREOJqADAVAB6/txnARxJRCEiKgEwHsCqLNYpKUZBYHAW+/URMAzjTlslnWO8yVrUkBAiQkRXAHgZQBDAA0KIFUR0WXz/XUKIVUS0EMAyADEA9wkhlmerTn7w0gjSiRpiGIZp72R1QpkQ4kUAL2rb7tK+/xXAX7NZj1Tw0gjScRYzDMO0d3goq1EcKnZ8J5DtAygIFtijf1ULYI2AYVKDo+naF9yDaegaQTgYth9aIkJJuAThQNjxILOzmGGYjgznGtLQBYG6SD1gmYeisahjG2sEDMN0ZLgH01DnBwCJgqAkXOLwDwAsCBiG6dhwD6YREzEAwMn7nAzAoBEUlCYIAnYWM0x6tFX2UcYbFgQaMRHDuAHjMLKPNa/NpBHoWgNrBAzDdGS4B9OIxqIIUACFwUIALeklJCbTEDuLGYbpyLAg0IiJGIIUtAWAPtovDSeahlgjYJjU4BQT7QvuwTSiwtIIXAVBQaljVrHpGIZhvCkOW/N11PxdTO7g8FGNmIghGGjRCPSRy9WHXo3KXZWObTw5humsvHT+S7aZNJNMHzUdm2s34+ff+3nGy2ZShwWBRjQWRSgUctUIxg8cj/EYn4uqMUybc9LeJ2Wl3FAghGuPujYrZTOpwzYNjZiIWc7ikDUK4tE+wzCdHRYEGlER9XQWMwzDdDa4l9OQGoGbj4BhGKazwT4CjWgs6nAWZ0IjePqcp9GnrE+ry2EYhskGLAg0bB9BMHM+gsnDJ7e6DIZhmGzBpiEN6SOQaSTYR8AwTGeHezkNqRFIAcA+AoZhOjssCDTkhDKZFZE1AoZhOjvsI9CQSecEkguCJTOWYFXVqraqGsMwTFZgQaAhk87JdQm8nMVj+o3BmH5j2qpqDMMwWSGrdg8iOomIVhPRGiK6xrB/AhFVE9HS+N/vs1kfP8ikc8N6DgMAnHfgeTmuEcMwTHbJmkZAREEAdwA4HkAlgI+JaIEQYqV26DtCiFOzVY9UkT6CPbrugabfNSUsQsMwDNPZyKZGMA7AGiHE10KIJgDzAEzK4vUyQjQWRSB+W1gIMAyTD2RTEAwAsF75XhnfpvM9IvqUiF4iogNMBRHRDCJaTESLq6qqslFXG6kRMAzD5AvZFAQmL6u+UvX/AAwWQowEcBuAZ0wFCSHuEUKMFUKMraioyGwtNaSPgGEYJl/IZo9XCWCQ8n0ggI3qAUKIXUKI2vjnFwGEiahXFuuUFBk1xDAMky9kUxB8DGAfIhpKRAUApgJYoB5ARH0pHp9JROPi9dmWxTolRc4jYBiGyReyFjUkhIgQ0RUAXgYQBPCAEGIFEV0W338XgCkAZhJRBEA9gKlCTunNEewjYBgm38jqhLK4uedFbdtdyufbAdyezTqkCvsIGIbJN7jH02AfAcMw+QYLAg32ETAMk29wj6fBPgKGYfINFgQKP33pp2iONbNGwDBMXsE9nsJtH90GAOwjYBgmr2BBYIA1AoZh8gnu8Qywj4BhmHyCBYEB1ggYhsknuMczwD4ChmHyCRYEBlgjYBgmn8j7Hu+LbV9gU80mxzb2ETAMk0/k/eL1+96+LwAg+vuovY01AoZh8gnu8eI0RZvsz+wjYBgmn2BBEKcx0mh/ji+RwDAMkxfktSCIiZj9uTHaIggisUguqsMwDJMT8loQ1DfX258fWvqQ/bk52pyL6jAMw+SEvBYENU019udfvfYr+3NzjAUBwzD5Q14LgtqmWuN21XHMMAzT2clrQVDTWGPczqYhhmHyibwWBG4aAZuGGIbJJ/JaEKg+AhXWCBiGySeyKgiI6CQiWk1Ea4joGo/jDiGiKBFNyWZ9dNhHwDAMk0VBQERBAHcAmAhgfwDTiGh/l+P+AuDlbNXFDekjWHn5SpQXlNvb2TTEMEw+kU2NYByANUKIr4UQTQDmAZhkOO5KAE8B2JLFuhiRGkGfsj4oDhfb20OBvE/BxDBMHpFNQTAAwHrle2V8mw0RDQAwGcBdXgUR0QwiWkxEi6uqqjJWwerGagBAl8IuKAoVAQDO3v9s/Om4P2XsGgzDMO2dbAoCU8IeoX2/BcCvhRBRw7EtJwlxjxBirBBibEVFRabqh+qGapQVlCEUCKE4ZGkE10+4Ht2KumXsGgzDMO2dbAqCSgCDlO8DAWzUjhkLYB4RrQUwBcA/ieiMLNbJwc6Gneha2BUAcPHoiwEAFSWZEzQMwzAdgWwKgo8B7ENEQ4moAMBUAAvUA4QQQ4UQQ4QQQwA8CeByIcQzWayTg52NO+3R/y8P+yVqf1OLilIWBAzD5BdZ84oKISJEdAWsaKAggAeEECuI6LL4fk+/QFtQ3VCNrkWWRkBEKC0ozXGNGIZh2p6shscIIV4E8KK2zSgAhBDTs1kXEzsbdqJPWZ+2vizDMEy7Iq9mFm/YtQHrdq6zv6s+AoZhmHwlrwLmB/7fQACAmGUFL+1s2MkRQgyTAs3NzaisrERDQ0Ouq8K4UFRUhIEDByIcDvs+J68EgYoQAtWN1SwIGCYFKisrUV5ejiFDhvCSru0QIQS2bduGyspKDB061Pd5eWUaUqlrrkMkFmHTEMOkQENDA3r27MlCoJ1CROjZs2fKGlveCoKl3y0FAAzrOSy3FWGYDgYLgfZNOr9PXgqCZZuX4elVTwMAjhx8ZI5rwzAMk1vyUhAcet+h+L8P/g/Deg5Dr5Jeua4OwzBZpKysDACwceNGTJliznQ/YcIELF682LOcW265Bbt377a/n3zyydi5c2fG6plL8kYQqGsM1EfqAQADyge4Hc4wTCejf//+ePLJJ9M+XxcEL774Irp165aBmuWevIkaMi1Cw+kkGCZ9rlp4le1ryxSj+o7CLSfd4rr/17/+NQYPHozLL78cAHD99dejvLwcP/7xjzFp0iTs2LEDzc3NmDNnDiZNcma9X7t2LU499VQsX74c9fX1uOiii7By5UoMHz4c9fX19nEzZ87Exx9/jPr6ekyZMgWzZ8/GP/7xD2zcuBHHHHMMevXqhUWLFmHIkCFYvHgxevXqhZtvvhkPPPAAAODSSy/FVVddhbVr12LixIk44ogj8N5772HAgAF49tlnUVxc7KjXc889hzlz5qCpqQk9e/bE3Llz0adPH9TW1uLKK6/E4sWLQUSYNWsWzjrrLCxcuBC//e1vEY1G0atXL7z++uutvu95IwhMC9VzgjmG6VhMnToVV111lS0IHn/8cSxcuBBFRUWYP38+unTpgq1bt+LQQw/F6aef7uo4vfPOO1FSUoJly5Zh2bJlGDNmjL3vhhtuQI8ePRCNRnHcccdh2bJl+OlPf4qbb74ZixYtQq9eTnPykiVL8OCDD+LDDz+EEALjx4/H0Ucfje7du+PLL7/Ef/7zH9x7770455xz8NRTT+GCCy5wnH/EEUfggw8+ABHhvvvuw0033YS///3v+OMf/4iuXbvis88+AwDs2LEDVVVV+NGPfoS3334bQ4cOxfbt2zNyX/NGEBg1AhYEDJM2XiP3bDF69Ghs2bIFGzduRFVVFbp374499tgDzc3N+O1vf4u3334bgUAAGzZswObNm9G3b19jOW+//TZ++tOfAgBGjBiBESNG2Psef/xx3HPPPYhEIti0aRNWrlzp2K/z7rvvYvLkySgttXKVnXnmmXjnnXdw+umnY+jQoRg1ahQA4OCDD8batWsTzq+srMS5556LTZs2oampyY7/f+211zBv3jz7uO7du+O5557DUUcdZR/To0cP/zfPg7zxEagL1fcu7Q3AWpCGYZiOxZQpU/Dkk0/isccew9SpUwEAc+fORVVVFZYsWYKlS5eiT58+SWPpTdrCN998g7/97W94/fXXsWzZMpxyyilJyxFCX2alhcLCQvtzMBhEJBJJOObKK6/EFVdcgc8++wx33323fT0hREIdTdsyQd4IAqkRPDftOZy9/9kAOB6aYToiU6dOxbx58/Dkk0/aUUDV1dXo3bs3wuEwFi1ahHXr1nmWcdRRR2Hu3LkAgOXLl2PZsmUAgF27dqG0tBRdu3bF5s2b8dJLL9nnlJeXo6Ym0cR81FFH4ZlnnsHu3btRV1eH+fPn48gj/YelV1dXY8AAK3DloYcesrefcMIJuP322+3vO3bswPe+9z289dZb+OabbwAgY6ahvBEE0kcwsMtAlIYtFa4wWOh1CsMw7ZADDjgANTU1GDBgAPr16wcAOP/887F48WKMHTsWc+fOxX777edZxsyZM1FbW4sRI0bgpptuwrhx4wAAI0eOxOjRo3HAAQfg4osvxuGHH26fM2PGDEycOBHHHHOMo6wxY8Zg+vTpGDduHMaPH49LL70Uo0eP9t2e66+/HmeffTaOPPJIh//hd7/7HXbs2IEDDzwQI0eOxKJFi1BRUYF77rkHZ555JkaOHIlzzz3X93W8IC+1pj0yduxYkSze18R769/Dze/fjH9M/AdKw6WY8/Yc/PHYP9prFTMMk5xVq1Zh+PDhua4GkwTT70RES4QQY03H542z+LBBh+GwQYfZ3/96wl9zWBuGYZj2Q96YhhiGYRgzLAgYhkmJjmZOzjfS+X1YEDAM45uioiJs27aNhUE7Ra5HUFSUmu8zb3wEDMO0noEDB6KyshJVVVW5rgrjglyhLBVYEDAM45twOJzSyldMx4BNQwzDMHkOCwKGYZg8hwUBwzBMntPhZhYTURUA70Qi7vQCsDWD1ekIcJvzA25zftCaNg8WQhhTLnc4QdAaiGix2xTrzgq3OT/gNucH2Wozm4YYhmHyHBYEDMMweU6+CYJ7cl2BHMBtzg+4zflBVtqcVz4ChmEYJpF80wgYhmEYDRYEDMMweU5eCAIiOomIVhPRGiK6Jtf1yRRE9AARbSGi5cq2HkT0KhF9Gf/fXdn3m/g9WE1EJ+am1q2DiAYR0SIiWkVEK4joZ/HtnbbdRFRERB8R0afxNs+Ob++0bZYQUZCIPiGi5+PfO3WbiWgtEX1GREuJaHF8W/bbLITo1H8AggC+ArAngAIAnwLYP9f1ylDbjgIwBsByZdtNAK6Jf74GwF/in/ePt70QwND4PQnmug1ptLkfgDHxz+UAvoi3rdO2GwABKIt/DgP4EMChnbnNStuvBvAogOfj3zt1mwGsBdBL25b1NueDRjAOwBohxNdCiCYA8wBMynGdMoIQ4m0A27XNkwA8FP/8EIAzlO3zhBCNQohvAKyBdW86FEKITUKI/8U/1wBYBWAAOnG7hUVt/Gs4/ifQidsMAEQ0EMApAO5TNnfqNruQ9TbngyAYAGC98r0yvq2z0kcIsQmwOk0AvePbO919IKIhAEbDGiF36nbHTSRLAWwB8KoQotO3GcAtAH4FIKZs6+xtFgBeIaIlRDQjvi3rbc6H9QjIsC0fY2Y71X0gojIATwG4Sgixi8jUPOtQw7YO124hRBTAKCLqBmA+ER3ocXiHbzMRnQpgixBiCRFN8HOKYVuHanOcw4UQG4moN4BXiehzj2Mz1uZ80AgqAQxSvg8EsDFHdWkLNhNRPwCI/98S395p7gMRhWEJgblCiKfjmzt9uwFACLETwJsATkLnbvPhAE4norWwzLnHEtEj6NxthhBiY/z/FgDzYZl6st7mfBAEHwPYh4iGElEBgKkAFuS4TtlkAYAfxj//EMCzyvapRFRIREMB7APgoxzUr1WQNfS/H8AqIcTNyq5O224iqohrAiCiYgDfB/A5OnGbhRC/EUIMFEIMgfXOviGEuACduM1EVEpE5fIzgBMALEdbtDnXXvI28sSfDCu65CsA1+a6Phls138AbALQDGt0cAmAngBeB/Bl/H8P5fhr4/dgNYCJua5/mm0+Apb6uwzA0vjfyZ253QBGAPgk3ublAH4f395p26y1fwJaooY6bZthRTZ+Gv9bIfuqtmgzp5hgGIbJc/LBNMQwDMN4wIKAYRgmz2FBwDAMk+ewIGAYhslzWBAwDMPkOSwIGIZh8hwWBAzDMHkOCwKGSQEiuiC+NsBSIro7ngyuloj+TkT/I6LXiagifuwoIvqAiJYR0Xw1jzzDtCdYEDCMT4hoOIBzYSUGGwUgCuB8AKUA/ieEGAPgLQCz4qf8G8CvhRAjAHymbGeYdkU+ZB9lmExxHICDAXwcz3ZaDCsBWAzAY/FjHgHwNBF1BdBNCPFWfPtDAJ5o2+oyjD9YEDCMfwjAQ0KI3zg2El2nHcd5W5gOBZuGGMY/rwOYEs8VL9eSHQzrPZoSP+Y8AO8KIaoB7CCiI+PbfwDLbMQw7Q7WCBjGJ0KIlUT0O1grSAVgZX39CYA6AAcQ0RIA1bD8CICVMvguIioB8DWAi3JQbYZJCmcfZZhWQkS1QoiyXNeDYdKFTUMMwzB5DmsEDMMweQ5rBAzDMHkOCwKGYZg8hwUBwzBMnsOCgGEYJs9hQcAwDJPn/H8aqOkgerCVNgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['sparse_categorical_accuracy']\n",
    "val_acc = history.history['sparse_categorical_accuracy']\n",
    "\n",
    "epo = range(1, len(acc)+1)\n",
    "# plt.plot(epo, acc, 'b', label = \"Training acc\")\n",
    "plt.plot(epo, val_acc, 'g', label = \"validation acc\")\n",
    "plt.title(\"Train and val acc\")\n",
    "plt.xlabel(\"epo\")\n",
    "plt.ylabel(\"acc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6. , 2.2, 4. , 1. ],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5.9, 3. , 5.1, 1.8],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [5.1, 3.5, 1.4, 0.2]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 784)\n",
      "(None, 500)\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, 28, 28)]    0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "my_dense (MyDense)           (None, 100)               50100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 443,610\n",
      "Trainable params: 443,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import layers,Model,Input,utils\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "class MyDense(layers.Layer): \n",
    "    def __init__(self, units=32): #初始化, 可以初始化与输入无关的参数，比如隐含层神经元个数\n",
    "        super(MyDense, self).__init__()#初始化父类\n",
    "        self.units = units  #定义输出规模\n",
    "    def build(self, input_shape):   #定义训练参数\n",
    "        self.w = K.variable(K.random_normal(shape=[input_shape[-1],self.units]))  #训练参数\n",
    "        self.b = tf.Variable(K.random_normal(shape=[self.units]),trainable=True)  #训练参数\n",
    "#         self.a = tf.Variable(K.random_normal(shape=[self.units]),trainable=False) #非训练参数\n",
    "    def call(self, inputs): #功能实现\n",
    "        return K.dot(inputs, self.w) + self.b\n",
    "#定义模型\n",
    "input_feature = Input([None,28,28]) \n",
    "x = layers.Reshape(target_shape=[28*28])(input_feature)\n",
    "print(x.shape)\n",
    "x = layers.Dense(500,activation='relu')(x)  \n",
    "print(x.shape)\n",
    "x = MyDense(100)(x)\n",
    "x = layers.Dense(10,activation='softmax')(x) \n",
    "  \n",
    "model = Model(input_feature,x) \n",
    "model.summary() \n",
    "utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None, 28, 28 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 784)          0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 500)          392500      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 500)          392500      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "my_layer (MyLayer)              (None, 180)          90180       dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 10)           1810        my_layer[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 876,990\n",
      "Trainable params: 876,990\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import layers,Model,Input,utils\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "class MyLayer(layers.Layer): \n",
    "    def __init__(self, output_dims):\n",
    "        super(MyLayer, self).__init__()  \n",
    "        self.output_dims = output_dims\n",
    "    def build(self, input_shape):  \n",
    "        [dim1,dim2] = self.output_dims\n",
    "        self.w1 = tf.Variable(K.random_uniform(shape=[input_shape[0][-1],dim1]))\n",
    "        self.b1 = tf.Variable(K.random_uniform(shape=[dim1]))  \n",
    "        self.w2 = tf.Variable(K.random_uniform(shape=[input_shape[1][-1],dim2])) \n",
    "        self.b2 = tf.Variable(K.random_uniform(shape=[dim2])) \n",
    "    def call(self, x): \n",
    "        [x1, x2] = x\n",
    "        y1 = K.dot(x1, self.w1)+self.b1 \n",
    "        y2 = K.dot(x2, self.w2)+self.b2\n",
    "        return K.concatenate([y1,y2],axis = -1)\n",
    "\n",
    "#定义模型\n",
    "input_feature = Input([None,28,28])#输入\n",
    "x = layers.Reshape(target_shape=[28*28])(input_feature) \n",
    "x1 = layers.Dense(500,activation='relu')(x)  \n",
    "x2 = layers.Dense(500,activation='relu')(x)  \n",
    "x = MyLayer([100,80])([x1,x2])   \n",
    "x = layers.Dense(10,activation='softmax')(x) \n",
    "  \n",
    "model = Model(input_feature,x) \n",
    "model.summary() \n",
    "utils.plot_model(model,show_layer_names=False,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.9545455>, <tf.Tensor: shape=(), dtype=float32, numpy=-5.4545455>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.9334943>, <tf.Tensor: shape=(), dtype=float32, numpy=-5.4173865>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.9125757>, <tf.Tensor: shape=(), dtype=float32, numpy=-5.3804655>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.8917882>, <tf.Tensor: shape=(), dtype=float32, numpy=-5.3437824>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.871132>, <tf.Tensor: shape=(), dtype=float32, numpy=-5.3073335>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.850605>, <tf.Tensor: shape=(), dtype=float32, numpy=-5.271119>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.8302073>, <tf.Tensor: shape=(), dtype=float32, numpy=-5.235137>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.8099377>, <tf.Tensor: shape=(), dtype=float32, numpy=-5.1993856>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.7897959>, <tf.Tensor: shape=(), dtype=float32, numpy=-5.163864>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.7697804>, <tf.Tensor: shape=(), dtype=float32, numpy=-5.1285706>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.7498906>, <tf.Tensor: shape=(), dtype=float32, numpy=-5.093503>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.7301257>, <tf.Tensor: shape=(), dtype=float32, numpy=-5.0586605>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.710485>, <tf.Tensor: shape=(), dtype=float32, numpy=-5.024042>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.690968>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.9896455>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.671573>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.95547>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.6523001>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.9215136>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.6331482>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.8877754>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.6141167>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.854254>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.5952048>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.820947>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.5764112>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.787854>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.557736>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.7549744>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.5391777>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.7223043>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.520736>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.689845>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.50241>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.657594>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.484199>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.6255503>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.4661021>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.593712>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.4481192>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.562078>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.4302485>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.5306473>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.4124904>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.4994183>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.394843>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.4683905>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.377307>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.437561>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.3598804>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.40693>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.3425634>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.3764954>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.325355>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.3462567>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.3082542>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.316212>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.2912607>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.2863603>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.2743735>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.2567005>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.2575924>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.227231>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.2409165>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.197951>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.2243447>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.168859>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.2078772>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.1399536>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.1915123>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.111234>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.17525>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.0826993>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.1590898>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.054348>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.143031>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.0261784>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.127072>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.99819>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.1112132>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.9703813>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.0954537>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.9427512>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.079793>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.915299>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.0642302>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.888023>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.0487647>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.8609219>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.033396>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.8339956>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.0181234>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.8072422>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-3.0029464>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.7806606>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.987864>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.7542498>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.9728763>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.7280092>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.957982>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.701937>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.943181>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.6760323>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.9284725>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.650294>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.913856>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.6247213>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.8993306>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.5993133>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.8848963>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.5740683>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.8705518>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.5489857>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.856297>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.524064>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.8421311>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.4993033>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.828054>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.4747014>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.8140645>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.4502578>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.8001626>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.4259715>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.7863474>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.4018412>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.7726183>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.377866>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.758975>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.3540452>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.7454169>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.3303776>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.7319431>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.3068619>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.7185535>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.2834978>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.7052476>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.260284>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.6920247>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.2372193>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.678884>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.2143033>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.6658254>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.1915345>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.6528482>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.1689124>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.6399517>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.1464355>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.6271358>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.1241035>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.6143997>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.1019154>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.6017427>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.0798697>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.5891647>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.0579658>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.5766652>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.0362031>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.5642433>, <tf.Tensor: shape=(), dtype=float32, numpy=-3.0145807>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.551899>, <tf.Tensor: shape=(), dtype=float32, numpy=-2.9930968>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.5396314>, <tf.Tensor: shape=(), dtype=float32, numpy=-2.9717517>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.52744>, <tf.Tensor: shape=(), dtype=float32, numpy=-2.9505436>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.515325>, <tf.Tensor: shape=(), dtype=float32, numpy=-2.9294724>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.503285>, <tf.Tensor: shape=(), dtype=float32, numpy=-2.908537>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.49132>, <tf.Tensor: shape=(), dtype=float32, numpy=-2.8877358>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.4794295>, <tf.Tensor: shape=(), dtype=float32, numpy=-2.867069>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.4676127>, <tf.Tensor: shape=(), dtype=float32, numpy=-2.8465352>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.4558697>, <tf.Tensor: shape=(), dtype=float32, numpy=-2.8261333>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.4441996>, <tf.Tensor: shape=(), dtype=float32, numpy=-2.8058631>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.432602>, <tf.Tensor: shape=(), dtype=float32, numpy=-2.7857232>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.4210768>, <tf.Tensor: shape=(), dtype=float32, numpy=-2.7657132>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.409623>, <tf.Tensor: shape=(), dtype=float32, numpy=-2.745832>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=-2.39824>, <tf.Tensor: shape=(), dtype=float32, numpy=-2.7260785>]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_raw = np.array([2013, 2014, 2015, 2016, 2017], dtype=np.float32)\n",
    "y_raw = np.array([12000, 14000, 15000, 16500, 17500], dtype=np.float32)\n",
    "\n",
    "X = (X_raw - X_raw.min()) / (X_raw.max() - X_raw.min())\n",
    "y = (y_raw - y_raw.min()) / (y_raw.max() - y_raw.min())\n",
    "\n",
    "X = tf.constant(X)\n",
    "y = tf.constant(y)\n",
    "\n",
    "a = tf.Variable(initial_value=0.)\n",
    "b = tf.Variable(initial_value=0.)\n",
    "variables = [a, b]\n",
    "\n",
    "num_epoch = 100\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=5e-4)\n",
    "for e in range(num_epoch):\n",
    "    # 使用tf.GradientTape()记录损失函数的梯度信息\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = a * X + b\n",
    "        loss = tf.reduce_sum(tf.square(y_pred - y))\n",
    "    # TensorFlow自动计算损失函数关于自变量（模型参数）的梯度\n",
    "    grads = tape.gradient(loss, variables)\n",
    "    # TensorFlow自动根据梯度更新参数\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.15485655>,\n",
       " <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.19738102>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(125.0, shape=(), dtype=float32) tf.Tensor(\n",
      "[[ 70.]\n",
      " [100.]], shape=(2, 1), dtype=float32) tf.Tensor(30.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X = tf.constant([[1., 2.], [3., 4.]])\n",
    "y = tf.constant([[1.], [2.]])\n",
    "w = tf.Variable(initial_value=[[1.], [2.]])\n",
    "b = tf.Variable(initial_value=1.)\n",
    "with tf.GradientTape() as tape:\n",
    "    L = tf.reduce_sum(tf.square(tf.matmul(X, w) + b - y))\n",
    "w_grad, b_grad = tape.gradient(L, [w, b])        # 计算L(w, b)关于w, b的偏导数\n",
    "print(L, w_grad, b_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
