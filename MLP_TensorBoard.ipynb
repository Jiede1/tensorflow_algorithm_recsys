{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        # MNIST中的图像默认为uint8（0-255的数字）。以下代码将其归一化到0-1之间的浮点数，并在最后增加一维作为颜色通道\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        # 从数据集中随机取出batch_size个元素并返回\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()    # Flatten层将除第一维（batch_size）以外的维度展平\n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):         # [batch_size, 28, 28, 1]\n",
    "        x = self.flatten(inputs)    # [batch_size, 784]\n",
    "        x = self.dense1(x)          # [batch_size, 100]\n",
    "        x = self.dense2(x)          # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1297: start (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.start` instead.\n",
      "batch 0: loss 2.443352\n",
      "batch 1: loss 2.258847\n",
      "batch 2: loss 2.221576\n",
      "batch 3: loss 2.110501\n",
      "batch 4: loss 2.048766\n",
      "batch 5: loss 2.031292\n",
      "batch 6: loss 1.925211\n",
      "batch 7: loss 1.855865\n",
      "batch 8: loss 1.597718\n",
      "batch 9: loss 1.589581\n",
      "batch 10: loss 1.512605\n",
      "batch 11: loss 1.556588\n",
      "batch 12: loss 1.552692\n",
      "batch 13: loss 1.563621\n",
      "batch 14: loss 1.566183\n",
      "batch 15: loss 1.384006\n",
      "batch 16: loss 1.248298\n",
      "batch 17: loss 1.330122\n",
      "batch 18: loss 1.264917\n",
      "batch 19: loss 1.244726\n",
      "batch 20: loss 1.108884\n",
      "batch 21: loss 0.980656\n",
      "batch 22: loss 1.079119\n",
      "batch 23: loss 1.055150\n",
      "batch 24: loss 1.196806\n",
      "batch 25: loss 1.026810\n",
      "batch 26: loss 1.114170\n",
      "batch 27: loss 0.767136\n",
      "batch 28: loss 1.006876\n",
      "batch 29: loss 0.860522\n",
      "batch 30: loss 1.027892\n",
      "batch 31: loss 0.921140\n",
      "batch 32: loss 0.785437\n",
      "batch 33: loss 0.951793\n",
      "batch 34: loss 1.018467\n",
      "batch 35: loss 0.750574\n",
      "batch 36: loss 0.928034\n",
      "batch 37: loss 0.501290\n",
      "batch 38: loss 0.776034\n",
      "batch 39: loss 0.790735\n",
      "batch 40: loss 0.693625\n",
      "batch 41: loss 0.799249\n",
      "batch 42: loss 0.724739\n",
      "batch 43: loss 0.637070\n",
      "batch 44: loss 0.912467\n",
      "batch 45: loss 0.812868\n",
      "batch 46: loss 0.591784\n",
      "batch 47: loss 0.704889\n",
      "batch 48: loss 0.556670\n",
      "batch 49: loss 0.590149\n",
      "batch 50: loss 0.709498\n",
      "batch 51: loss 0.501700\n",
      "batch 52: loss 0.825782\n",
      "batch 53: loss 0.464766\n",
      "batch 54: loss 0.597364\n",
      "batch 55: loss 0.695229\n",
      "batch 56: loss 0.511711\n",
      "batch 57: loss 0.524655\n",
      "batch 58: loss 0.630714\n",
      "batch 59: loss 0.494084\n",
      "batch 60: loss 0.689477\n",
      "batch 61: loss 0.419977\n",
      "batch 62: loss 0.631801\n",
      "batch 63: loss 0.549659\n",
      "batch 64: loss 0.506936\n",
      "batch 65: loss 0.426891\n",
      "batch 66: loss 0.478072\n",
      "batch 67: loss 0.515893\n",
      "batch 68: loss 0.515202\n",
      "batch 69: loss 0.542694\n",
      "batch 70: loss 0.578640\n",
      "batch 71: loss 0.490466\n",
      "batch 72: loss 0.462143\n",
      "batch 73: loss 0.429665\n",
      "batch 74: loss 0.436489\n",
      "batch 75: loss 0.364831\n",
      "batch 76: loss 0.510946\n",
      "batch 77: loss 0.348848\n",
      "batch 78: loss 0.315522\n",
      "batch 79: loss 0.410942\n",
      "batch 80: loss 0.509607\n",
      "batch 81: loss 0.642951\n",
      "batch 82: loss 0.593954\n",
      "batch 83: loss 0.504923\n",
      "batch 84: loss 0.735617\n",
      "batch 85: loss 0.676519\n",
      "batch 86: loss 0.404384\n",
      "batch 87: loss 0.602902\n",
      "batch 88: loss 0.476550\n",
      "batch 89: loss 0.691090\n",
      "batch 90: loss 0.546378\n",
      "batch 91: loss 0.547479\n",
      "batch 92: loss 0.434375\n",
      "batch 93: loss 0.358302\n",
      "batch 94: loss 0.489931\n",
      "batch 95: loss 0.395835\n",
      "batch 96: loss 0.373560\n",
      "batch 97: loss 0.482667\n",
      "batch 98: loss 0.488984\n",
      "batch 99: loss 0.302283\n",
      "batch 100: loss 0.360682\n",
      "batch 101: loss 0.476230\n",
      "batch 102: loss 0.341461\n",
      "batch 103: loss 0.435809\n",
      "batch 104: loss 0.400293\n",
      "batch 105: loss 0.376317\n",
      "batch 106: loss 0.339766\n",
      "batch 107: loss 0.301953\n",
      "batch 108: loss 0.391266\n",
      "batch 109: loss 0.439476\n",
      "batch 110: loss 0.317081\n",
      "batch 111: loss 0.443961\n",
      "batch 112: loss 0.738992\n",
      "batch 113: loss 0.352521\n",
      "batch 114: loss 0.293077\n",
      "batch 115: loss 0.337294\n",
      "batch 116: loss 0.426096\n",
      "batch 117: loss 0.549608\n",
      "batch 118: loss 0.494754\n",
      "batch 119: loss 0.470445\n",
      "batch 120: loss 0.419211\n",
      "batch 121: loss 0.357591\n",
      "batch 122: loss 0.459146\n",
      "batch 123: loss 0.313246\n",
      "batch 124: loss 0.342078\n",
      "batch 125: loss 0.308839\n",
      "batch 126: loss 0.462893\n",
      "batch 127: loss 0.663669\n",
      "batch 128: loss 0.324938\n",
      "batch 129: loss 0.382448\n",
      "batch 130: loss 0.199590\n",
      "batch 131: loss 0.367122\n",
      "batch 132: loss 0.369275\n",
      "batch 133: loss 0.426130\n",
      "batch 134: loss 0.309782\n",
      "batch 135: loss 0.479294\n",
      "batch 136: loss 0.227495\n",
      "batch 137: loss 0.320970\n",
      "batch 138: loss 0.314377\n",
      "batch 139: loss 0.407273\n",
      "batch 140: loss 0.280098\n",
      "batch 141: loss 0.414695\n",
      "batch 142: loss 0.477533\n",
      "batch 143: loss 0.473489\n",
      "batch 144: loss 0.285068\n",
      "batch 145: loss 0.316199\n",
      "batch 146: loss 0.507612\n",
      "batch 147: loss 0.513108\n",
      "batch 148: loss 0.525782\n",
      "batch 149: loss 0.389956\n",
      "batch 150: loss 0.307269\n",
      "batch 151: loss 0.293347\n",
      "batch 152: loss 0.315376\n",
      "batch 153: loss 0.290459\n",
      "batch 154: loss 0.570867\n",
      "batch 155: loss 0.258576\n",
      "batch 156: loss 0.197384\n",
      "batch 157: loss 0.349289\n",
      "batch 158: loss 0.264193\n",
      "batch 159: loss 0.207849\n",
      "batch 160: loss 0.192334\n",
      "batch 161: loss 0.273064\n",
      "batch 162: loss 0.499617\n",
      "batch 163: loss 0.362057\n",
      "batch 164: loss 0.220319\n",
      "batch 165: loss 0.310340\n",
      "batch 166: loss 0.282174\n",
      "batch 167: loss 0.380148\n",
      "batch 168: loss 0.351703\n",
      "batch 169: loss 0.237865\n",
      "batch 170: loss 0.277730\n",
      "batch 171: loss 0.238317\n",
      "batch 172: loss 0.381438\n",
      "batch 173: loss 0.409697\n",
      "batch 174: loss 0.383187\n",
      "batch 175: loss 0.221738\n",
      "batch 176: loss 0.375352\n",
      "batch 177: loss 0.372139\n",
      "batch 178: loss 0.321395\n",
      "batch 179: loss 0.338542\n",
      "batch 180: loss 0.475429\n",
      "batch 181: loss 0.264207\n",
      "batch 182: loss 0.367153\n",
      "batch 183: loss 0.216006\n",
      "batch 184: loss 0.366953\n",
      "batch 185: loss 0.307830\n",
      "batch 186: loss 0.522176\n",
      "batch 187: loss 0.528814\n",
      "batch 188: loss 0.348197\n",
      "batch 189: loss 0.355657\n",
      "batch 190: loss 0.227260\n",
      "batch 191: loss 0.348639\n",
      "batch 192: loss 0.434519\n",
      "batch 193: loss 0.411314\n",
      "batch 194: loss 0.443630\n",
      "batch 195: loss 0.211495\n",
      "batch 196: loss 0.398246\n",
      "batch 197: loss 0.371720\n",
      "batch 198: loss 0.321656\n",
      "batch 199: loss 0.338853\n",
      "batch 200: loss 0.277483\n",
      "batch 201: loss 0.398658\n",
      "batch 202: loss 0.157346\n",
      "batch 203: loss 0.381130\n",
      "batch 204: loss 0.271970\n",
      "batch 205: loss 0.372937\n",
      "batch 206: loss 0.299087\n",
      "batch 207: loss 0.305880\n",
      "batch 208: loss 0.335349\n",
      "batch 209: loss 0.406337\n",
      "batch 210: loss 0.253295\n",
      "batch 211: loss 0.380246\n",
      "batch 212: loss 0.167668\n",
      "batch 213: loss 0.362714\n",
      "batch 214: loss 0.205488\n",
      "batch 215: loss 0.285053\n",
      "batch 216: loss 0.423637\n",
      "batch 217: loss 0.422890\n",
      "batch 218: loss 0.357128\n",
      "batch 219: loss 0.306382\n",
      "batch 220: loss 0.201462\n",
      "batch 221: loss 0.485504\n",
      "batch 222: loss 0.183122\n",
      "batch 223: loss 0.333653\n",
      "batch 224: loss 0.618152\n",
      "batch 225: loss 0.432865\n",
      "batch 226: loss 0.340313\n",
      "batch 227: loss 0.346153\n",
      "batch 228: loss 0.279879\n",
      "batch 229: loss 0.284455\n",
      "batch 230: loss 0.393261\n",
      "batch 231: loss 0.254155\n",
      "batch 232: loss 0.339053\n",
      "batch 233: loss 0.212332\n",
      "batch 234: loss 0.288203\n",
      "batch 235: loss 0.186925\n",
      "batch 236: loss 0.308961\n",
      "batch 237: loss 0.325868\n",
      "batch 238: loss 0.170705\n",
      "batch 239: loss 0.281594\n",
      "batch 240: loss 0.510695\n",
      "batch 241: loss 0.285764\n",
      "batch 242: loss 0.192225\n",
      "batch 243: loss 0.197289\n",
      "batch 244: loss 0.193385\n",
      "batch 245: loss 0.313111\n",
      "batch 246: loss 0.326506\n",
      "batch 247: loss 0.300567\n",
      "batch 248: loss 0.318655\n",
      "batch 249: loss 0.295111\n",
      "batch 250: loss 0.226248\n",
      "batch 251: loss 0.270003\n",
      "batch 252: loss 0.245790\n",
      "batch 253: loss 0.341751\n",
      "batch 254: loss 0.351034\n",
      "batch 255: loss 0.384263\n",
      "batch 256: loss 0.448982\n",
      "batch 257: loss 0.227937\n",
      "batch 258: loss 0.237831\n",
      "batch 259: loss 0.329301\n",
      "batch 260: loss 0.311583\n",
      "batch 261: loss 0.403996\n",
      "batch 262: loss 0.201318\n",
      "batch 263: loss 0.257830\n",
      "batch 264: loss 0.460823\n",
      "batch 265: loss 0.606680\n",
      "batch 266: loss 0.168910\n",
      "batch 267: loss 0.234746\n",
      "batch 268: loss 0.417658\n",
      "batch 269: loss 0.304958\n",
      "batch 270: loss 0.221482\n",
      "batch 271: loss 0.489858\n",
      "batch 272: loss 0.460971\n",
      "batch 273: loss 0.268481\n",
      "batch 274: loss 0.292702\n",
      "batch 275: loss 0.279147\n",
      "batch 276: loss 0.296999\n",
      "batch 277: loss 0.353491\n",
      "batch 278: loss 0.439240\n",
      "batch 279: loss 0.321690\n",
      "batch 280: loss 0.086257\n",
      "batch 281: loss 0.414939\n",
      "batch 282: loss 0.469090\n",
      "batch 283: loss 0.161452\n",
      "batch 284: loss 0.356054\n",
      "batch 285: loss 0.215697\n",
      "batch 286: loss 0.376727\n",
      "batch 287: loss 0.161425\n",
      "batch 288: loss 0.086963\n",
      "batch 289: loss 0.352064\n",
      "batch 290: loss 0.147550\n",
      "batch 291: loss 0.407625\n",
      "batch 292: loss 0.348159\n",
      "batch 293: loss 0.254914\n",
      "batch 294: loss 0.190061\n",
      "batch 295: loss 0.246349\n",
      "batch 296: loss 0.306611\n",
      "batch 297: loss 0.342488\n",
      "batch 298: loss 0.414456\n",
      "batch 299: loss 0.377468\n",
      "batch 300: loss 0.269054\n",
      "batch 301: loss 0.293342\n",
      "batch 302: loss 0.195708\n",
      "batch 303: loss 0.312267\n",
      "batch 304: loss 0.306476\n",
      "batch 305: loss 0.235267\n",
      "batch 306: loss 0.513331\n",
      "batch 307: loss 0.336905\n",
      "batch 308: loss 0.342030\n",
      "batch 309: loss 0.300325\n",
      "batch 310: loss 0.335391\n",
      "batch 311: loss 0.311767\n",
      "batch 312: loss 0.381904\n",
      "batch 313: loss 0.291886\n",
      "batch 314: loss 0.105859\n",
      "batch 315: loss 0.330698\n",
      "batch 316: loss 0.255042\n",
      "batch 317: loss 0.446337\n",
      "batch 318: loss 0.424329\n",
      "batch 319: loss 0.152237\n",
      "batch 320: loss 0.156638\n",
      "batch 321: loss 0.593436\n",
      "batch 322: loss 0.254387\n",
      "batch 323: loss 0.458197\n",
      "batch 324: loss 0.340844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 325: loss 0.308517\n",
      "batch 326: loss 0.236056\n",
      "batch 327: loss 0.324948\n",
      "batch 328: loss 0.330765\n",
      "batch 329: loss 0.317181\n",
      "batch 330: loss 0.322981\n",
      "batch 331: loss 0.343232\n",
      "batch 332: loss 0.292141\n",
      "batch 333: loss 0.341681\n",
      "batch 334: loss 0.171488\n",
      "batch 335: loss 0.221195\n",
      "batch 336: loss 0.216986\n",
      "batch 337: loss 0.294495\n",
      "batch 338: loss 0.266594\n",
      "batch 339: loss 0.318152\n",
      "batch 340: loss 0.222729\n",
      "batch 341: loss 0.208275\n",
      "batch 342: loss 0.274899\n",
      "batch 343: loss 0.291085\n",
      "batch 344: loss 0.137174\n",
      "batch 345: loss 0.322715\n",
      "batch 346: loss 0.138111\n",
      "batch 347: loss 0.295072\n",
      "batch 348: loss 0.125544\n",
      "batch 349: loss 0.514743\n",
      "batch 350: loss 0.394237\n",
      "batch 351: loss 0.634997\n",
      "batch 352: loss 0.291104\n",
      "batch 353: loss 0.405760\n",
      "batch 354: loss 0.212653\n",
      "batch 355: loss 0.226399\n",
      "batch 356: loss 0.329915\n",
      "batch 357: loss 0.382847\n",
      "batch 358: loss 0.206337\n",
      "batch 359: loss 0.192210\n",
      "batch 360: loss 0.207910\n",
      "batch 361: loss 0.446603\n",
      "batch 362: loss 0.312245\n",
      "batch 363: loss 0.159661\n",
      "batch 364: loss 0.610800\n",
      "batch 365: loss 0.257505\n",
      "batch 366: loss 0.212263\n",
      "batch 367: loss 0.210073\n",
      "batch 368: loss 0.112813\n",
      "batch 369: loss 0.374418\n",
      "batch 370: loss 0.257611\n",
      "batch 371: loss 0.597833\n",
      "batch 372: loss 0.156827\n",
      "batch 373: loss 0.354308\n",
      "batch 374: loss 0.307684\n",
      "batch 375: loss 0.140733\n",
      "batch 376: loss 0.239736\n",
      "batch 377: loss 0.186090\n",
      "batch 378: loss 0.333365\n",
      "batch 379: loss 0.317508\n",
      "batch 380: loss 0.361666\n",
      "batch 381: loss 0.145569\n",
      "batch 382: loss 0.169864\n",
      "batch 383: loss 0.322150\n",
      "batch 384: loss 0.195259\n",
      "batch 385: loss 0.367576\n",
      "batch 386: loss 0.146553\n",
      "batch 387: loss 0.099201\n",
      "batch 388: loss 0.314968\n",
      "batch 389: loss 0.197162\n",
      "batch 390: loss 0.296142\n",
      "batch 391: loss 0.274695\n",
      "batch 392: loss 0.224081\n",
      "batch 393: loss 0.258749\n",
      "batch 394: loss 0.205585\n",
      "batch 395: loss 0.285658\n",
      "batch 396: loss 0.156384\n",
      "batch 397: loss 0.270874\n",
      "batch 398: loss 0.247855\n",
      "batch 399: loss 0.406814\n",
      "batch 400: loss 0.302994\n",
      "batch 401: loss 0.413033\n",
      "batch 402: loss 0.337561\n",
      "batch 403: loss 0.164352\n",
      "batch 404: loss 0.169587\n",
      "batch 405: loss 0.217132\n",
      "batch 406: loss 0.345576\n",
      "batch 407: loss 0.435627\n",
      "batch 408: loss 0.195919\n",
      "batch 409: loss 0.245042\n",
      "batch 410: loss 0.299805\n",
      "batch 411: loss 0.109972\n",
      "batch 412: loss 0.282948\n",
      "batch 413: loss 0.362666\n",
      "batch 414: loss 0.178789\n",
      "batch 415: loss 0.218124\n",
      "batch 416: loss 0.155187\n",
      "batch 417: loss 0.203719\n",
      "batch 418: loss 0.261084\n",
      "batch 419: loss 0.293062\n",
      "batch 420: loss 0.208000\n",
      "batch 421: loss 0.193805\n",
      "batch 422: loss 0.365125\n",
      "batch 423: loss 0.184265\n",
      "batch 424: loss 0.181687\n",
      "batch 425: loss 0.167511\n",
      "batch 426: loss 0.261014\n",
      "batch 427: loss 0.227105\n",
      "batch 428: loss 0.311257\n",
      "batch 429: loss 0.226722\n",
      "batch 430: loss 0.448298\n",
      "batch 431: loss 0.159677\n",
      "batch 432: loss 0.126871\n",
      "batch 433: loss 0.295004\n",
      "batch 434: loss 0.262701\n",
      "batch 435: loss 0.284842\n",
      "batch 436: loss 0.311781\n",
      "batch 437: loss 0.195413\n",
      "batch 438: loss 0.282266\n",
      "batch 439: loss 0.103056\n",
      "batch 440: loss 0.399706\n",
      "batch 441: loss 0.256274\n",
      "batch 442: loss 0.179125\n",
      "batch 443: loss 0.314152\n",
      "batch 444: loss 0.266745\n",
      "batch 445: loss 0.208934\n",
      "batch 446: loss 0.279672\n",
      "batch 447: loss 0.348055\n",
      "batch 448: loss 0.150197\n",
      "batch 449: loss 0.098191\n",
      "batch 450: loss 0.353456\n",
      "batch 451: loss 0.243874\n",
      "batch 452: loss 0.224795\n",
      "batch 453: loss 0.331214\n",
      "batch 454: loss 0.225673\n",
      "batch 455: loss 0.187234\n",
      "batch 456: loss 0.363696\n",
      "batch 457: loss 0.202551\n",
      "batch 458: loss 0.283034\n",
      "batch 459: loss 0.139219\n",
      "batch 460: loss 0.205733\n",
      "batch 461: loss 0.337576\n",
      "batch 462: loss 0.252562\n",
      "batch 463: loss 0.231342\n",
      "batch 464: loss 0.219183\n",
      "batch 465: loss 0.467838\n",
      "batch 466: loss 0.470496\n",
      "batch 467: loss 0.351446\n",
      "batch 468: loss 0.155508\n",
      "batch 469: loss 0.341759\n",
      "batch 470: loss 0.227560\n",
      "batch 471: loss 0.275583\n",
      "batch 472: loss 0.113820\n",
      "batch 473: loss 0.110351\n",
      "batch 474: loss 0.193589\n",
      "batch 475: loss 0.169551\n",
      "batch 476: loss 0.208617\n",
      "batch 477: loss 0.266291\n",
      "batch 478: loss 0.224726\n",
      "batch 479: loss 0.175856\n",
      "batch 480: loss 0.360384\n",
      "batch 481: loss 0.163933\n",
      "batch 482: loss 0.275408\n",
      "batch 483: loss 0.330324\n",
      "batch 484: loss 0.244498\n",
      "batch 485: loss 0.223443\n",
      "batch 486: loss 0.337192\n",
      "batch 487: loss 0.409598\n",
      "batch 488: loss 0.335448\n",
      "batch 489: loss 0.297428\n",
      "batch 490: loss 0.358241\n",
      "batch 491: loss 0.216664\n",
      "batch 492: loss 0.251916\n",
      "batch 493: loss 0.136100\n",
      "batch 494: loss 0.291147\n",
      "batch 495: loss 0.107780\n",
      "batch 496: loss 0.298390\n",
      "batch 497: loss 0.276492\n",
      "batch 498: loss 0.091008\n",
      "batch 499: loss 0.231583\n",
      "batch 500: loss 0.273143\n",
      "batch 501: loss 0.158763\n",
      "batch 502: loss 0.296780\n",
      "batch 503: loss 0.246807\n",
      "batch 504: loss 0.346606\n",
      "batch 505: loss 0.205214\n",
      "batch 506: loss 0.126465\n",
      "batch 507: loss 0.184670\n",
      "batch 508: loss 0.264052\n",
      "batch 509: loss 0.119849\n",
      "batch 510: loss 0.406869\n",
      "batch 511: loss 0.401514\n",
      "batch 512: loss 0.279931\n",
      "batch 513: loss 0.353418\n",
      "batch 514: loss 0.212348\n",
      "batch 515: loss 0.109911\n",
      "batch 516: loss 0.357125\n",
      "batch 517: loss 0.193441\n",
      "batch 518: loss 0.205141\n",
      "batch 519: loss 0.316561\n",
      "batch 520: loss 0.279089\n",
      "batch 521: loss 0.113890\n",
      "batch 522: loss 0.361168\n",
      "batch 523: loss 0.250169\n",
      "batch 524: loss 0.306117\n",
      "batch 525: loss 0.273397\n",
      "batch 526: loss 0.149571\n",
      "batch 527: loss 0.200232\n",
      "batch 528: loss 0.242505\n",
      "batch 529: loss 0.387266\n",
      "batch 530: loss 0.358047\n",
      "batch 531: loss 0.216022\n",
      "batch 532: loss 0.285704\n",
      "batch 533: loss 0.148391\n",
      "batch 534: loss 0.177788\n",
      "batch 535: loss 0.138369\n",
      "batch 536: loss 0.185709\n",
      "batch 537: loss 0.139405\n",
      "batch 538: loss 0.226054\n",
      "batch 539: loss 0.117119\n",
      "batch 540: loss 0.124801\n",
      "batch 541: loss 0.189912\n",
      "batch 542: loss 0.316682\n",
      "batch 543: loss 0.201425\n",
      "batch 544: loss 0.211426\n",
      "batch 545: loss 0.328916\n",
      "batch 546: loss 0.163567\n",
      "batch 547: loss 0.405644\n",
      "batch 548: loss 0.091582\n",
      "batch 549: loss 0.267002\n",
      "batch 550: loss 0.282434\n",
      "batch 551: loss 0.303793\n",
      "batch 552: loss 0.315048\n",
      "batch 553: loss 0.079000\n",
      "batch 554: loss 0.161398\n",
      "batch 555: loss 0.274648\n",
      "batch 556: loss 0.134168\n",
      "batch 557: loss 0.226112\n",
      "batch 558: loss 0.311933\n",
      "batch 559: loss 0.294115\n",
      "batch 560: loss 0.061500\n",
      "batch 561: loss 0.050062\n",
      "batch 562: loss 0.126453\n",
      "batch 563: loss 0.151006\n",
      "batch 564: loss 0.083042\n",
      "batch 565: loss 0.172695\n",
      "batch 566: loss 0.271840\n",
      "batch 567: loss 0.209100\n",
      "batch 568: loss 0.066229\n",
      "batch 569: loss 0.303551\n",
      "batch 570: loss 0.428442\n",
      "batch 571: loss 0.204056\n",
      "batch 572: loss 0.195841\n",
      "batch 573: loss 0.264286\n",
      "batch 574: loss 0.192898\n",
      "batch 575: loss 0.345441\n",
      "batch 576: loss 0.219696\n",
      "batch 577: loss 0.255675\n",
      "batch 578: loss 0.237513\n",
      "batch 579: loss 0.235929\n",
      "batch 580: loss 0.443507\n",
      "batch 581: loss 0.354885\n",
      "batch 582: loss 0.222881\n",
      "batch 583: loss 0.128767\n",
      "batch 584: loss 0.180577\n",
      "batch 585: loss 0.212318\n",
      "batch 586: loss 0.182710\n",
      "batch 587: loss 0.407003\n",
      "batch 588: loss 0.247549\n",
      "batch 589: loss 0.206027\n",
      "batch 590: loss 0.397456\n",
      "batch 591: loss 0.086337\n",
      "batch 592: loss 0.375602\n",
      "batch 593: loss 0.190187\n",
      "batch 594: loss 0.282054\n",
      "batch 595: loss 0.375011\n",
      "batch 596: loss 0.300840\n",
      "batch 597: loss 0.158239\n",
      "batch 598: loss 0.199753\n",
      "batch 599: loss 0.287240\n",
      "batch 600: loss 0.186638\n",
      "batch 601: loss 0.138776\n",
      "batch 602: loss 0.353079\n",
      "batch 603: loss 0.193043\n",
      "batch 604: loss 0.210580\n",
      "batch 605: loss 0.147503\n",
      "batch 606: loss 0.290478\n",
      "batch 607: loss 0.318841\n",
      "batch 608: loss 0.148433\n",
      "batch 609: loss 0.263893\n",
      "batch 610: loss 0.235947\n",
      "batch 611: loss 0.311356\n",
      "batch 612: loss 0.170717\n",
      "batch 613: loss 0.172655\n",
      "batch 614: loss 0.184197\n",
      "batch 615: loss 0.426911\n",
      "batch 616: loss 0.160313\n",
      "batch 617: loss 0.193052\n",
      "batch 618: loss 0.333703\n",
      "batch 619: loss 0.165091\n",
      "batch 620: loss 0.148945\n",
      "batch 621: loss 0.134994\n",
      "batch 622: loss 0.084281\n",
      "batch 623: loss 0.176001\n",
      "batch 624: loss 0.399733\n",
      "batch 625: loss 0.206470\n",
      "batch 626: loss 0.087238\n",
      "batch 627: loss 0.233473\n",
      "batch 628: loss 0.315319\n",
      "batch 629: loss 0.133871\n",
      "batch 630: loss 0.300780\n",
      "batch 631: loss 0.243769\n",
      "batch 632: loss 0.267099\n",
      "batch 633: loss 0.278044\n",
      "batch 634: loss 0.156920\n",
      "batch 635: loss 0.435763\n",
      "batch 636: loss 0.128435\n",
      "batch 637: loss 0.160382\n",
      "batch 638: loss 0.167594\n",
      "batch 639: loss 0.211178\n",
      "batch 640: loss 0.103272\n",
      "batch 641: loss 0.235929\n",
      "batch 642: loss 0.121748\n",
      "batch 643: loss 0.397776\n",
      "batch 644: loss 0.239771\n",
      "batch 645: loss 0.220785\n",
      "batch 646: loss 0.290138\n",
      "batch 647: loss 0.208011\n",
      "batch 648: loss 0.055952\n",
      "batch 649: loss 0.202119\n",
      "batch 650: loss 0.159214\n",
      "batch 651: loss 0.159052\n",
      "batch 652: loss 0.114636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 653: loss 0.252376\n",
      "batch 654: loss 0.344962\n",
      "batch 655: loss 0.358595\n",
      "batch 656: loss 0.146270\n",
      "batch 657: loss 0.313496\n",
      "batch 658: loss 0.098009\n",
      "batch 659: loss 0.165361\n",
      "batch 660: loss 0.078737\n",
      "batch 661: loss 0.337035\n",
      "batch 662: loss 0.227241\n",
      "batch 663: loss 0.335263\n",
      "batch 664: loss 0.219010\n",
      "batch 665: loss 0.338599\n",
      "batch 666: loss 0.051352\n",
      "batch 667: loss 0.303068\n",
      "batch 668: loss 0.335663\n",
      "batch 669: loss 0.251104\n",
      "batch 670: loss 0.273023\n",
      "batch 671: loss 0.340206\n",
      "batch 672: loss 0.356594\n",
      "batch 673: loss 0.253473\n",
      "batch 674: loss 0.126542\n",
      "batch 675: loss 0.172617\n",
      "batch 676: loss 0.114551\n",
      "batch 677: loss 0.174991\n",
      "batch 678: loss 0.176480\n",
      "batch 679: loss 0.274750\n",
      "batch 680: loss 0.316108\n",
      "batch 681: loss 0.225242\n",
      "batch 682: loss 0.124631\n",
      "batch 683: loss 0.131444\n",
      "batch 684: loss 0.078113\n",
      "batch 685: loss 0.193813\n",
      "batch 686: loss 0.089483\n",
      "batch 687: loss 0.310202\n",
      "batch 688: loss 0.266326\n",
      "batch 689: loss 0.133890\n",
      "batch 690: loss 0.133496\n",
      "batch 691: loss 0.073909\n",
      "batch 692: loss 0.157979\n",
      "batch 693: loss 0.534136\n",
      "batch 694: loss 0.223448\n",
      "batch 695: loss 0.164932\n",
      "batch 696: loss 0.158622\n",
      "batch 697: loss 0.315435\n",
      "batch 698: loss 0.379482\n",
      "batch 699: loss 0.169587\n",
      "batch 700: loss 0.217368\n",
      "batch 701: loss 0.210690\n",
      "batch 702: loss 0.135937\n",
      "batch 703: loss 0.158893\n",
      "batch 704: loss 0.195073\n",
      "batch 705: loss 0.197380\n",
      "batch 706: loss 0.080871\n",
      "batch 707: loss 0.187596\n",
      "batch 708: loss 0.198234\n",
      "batch 709: loss 0.130248\n",
      "batch 710: loss 0.290624\n",
      "batch 711: loss 0.196594\n",
      "batch 712: loss 0.232987\n",
      "batch 713: loss 0.109343\n",
      "batch 714: loss 0.388815\n",
      "batch 715: loss 0.123863\n",
      "batch 716: loss 0.276053\n",
      "batch 717: loss 0.156157\n",
      "batch 718: loss 0.156164\n",
      "batch 719: loss 0.248004\n",
      "batch 720: loss 0.277269\n",
      "batch 721: loss 0.279233\n",
      "batch 722: loss 0.131404\n",
      "batch 723: loss 0.112687\n",
      "batch 724: loss 0.129882\n",
      "batch 725: loss 0.156838\n",
      "batch 726: loss 0.146711\n",
      "batch 727: loss 0.234950\n",
      "batch 728: loss 0.299199\n",
      "batch 729: loss 0.169164\n",
      "batch 730: loss 0.183960\n",
      "batch 731: loss 0.358046\n",
      "batch 732: loss 0.151406\n",
      "batch 733: loss 0.193983\n",
      "batch 734: loss 0.146324\n",
      "batch 735: loss 0.328920\n",
      "batch 736: loss 0.221708\n",
      "batch 737: loss 0.135764\n",
      "batch 738: loss 0.123940\n",
      "batch 739: loss 0.168182\n",
      "batch 740: loss 0.179563\n",
      "batch 741: loss 0.102150\n",
      "batch 742: loss 0.201725\n",
      "batch 743: loss 0.060968\n",
      "batch 744: loss 0.114331\n",
      "batch 745: loss 0.201129\n",
      "batch 746: loss 0.244936\n",
      "batch 747: loss 0.331291\n",
      "batch 748: loss 0.214234\n",
      "batch 749: loss 0.304262\n",
      "batch 750: loss 0.215464\n",
      "batch 751: loss 0.179197\n",
      "batch 752: loss 0.166202\n",
      "batch 753: loss 0.157965\n",
      "batch 754: loss 0.206761\n",
      "batch 755: loss 0.224188\n",
      "batch 756: loss 0.074800\n",
      "batch 757: loss 0.146362\n",
      "batch 758: loss 0.100808\n",
      "batch 759: loss 0.299833\n",
      "batch 760: loss 0.117997\n",
      "batch 761: loss 0.180096\n",
      "batch 762: loss 0.182027\n",
      "batch 763: loss 0.108452\n",
      "batch 764: loss 0.116859\n",
      "batch 765: loss 0.186459\n",
      "batch 766: loss 0.127836\n",
      "batch 767: loss 0.220703\n",
      "batch 768: loss 0.203675\n",
      "batch 769: loss 0.477111\n",
      "batch 770: loss 0.266233\n",
      "batch 771: loss 0.359814\n",
      "batch 772: loss 0.104516\n",
      "batch 773: loss 0.080843\n",
      "batch 774: loss 0.111556\n",
      "batch 775: loss 0.211812\n",
      "batch 776: loss 0.101332\n",
      "batch 777: loss 0.430670\n",
      "batch 778: loss 0.190015\n",
      "batch 779: loss 0.167133\n",
      "batch 780: loss 0.081849\n",
      "batch 781: loss 0.198120\n",
      "batch 782: loss 0.258626\n",
      "batch 783: loss 0.208683\n",
      "batch 784: loss 0.252553\n",
      "batch 785: loss 0.217716\n",
      "batch 786: loss 0.148838\n",
      "batch 787: loss 0.114299\n",
      "batch 788: loss 0.307873\n",
      "batch 789: loss 0.230717\n",
      "batch 790: loss 0.181269\n",
      "batch 791: loss 0.112205\n",
      "batch 792: loss 0.238753\n",
      "batch 793: loss 0.308010\n",
      "batch 794: loss 0.322193\n",
      "batch 795: loss 0.164076\n",
      "batch 796: loss 0.268146\n",
      "batch 797: loss 0.067994\n",
      "batch 798: loss 0.121816\n",
      "batch 799: loss 0.155094\n",
      "batch 800: loss 0.245814\n",
      "batch 801: loss 0.263717\n",
      "batch 802: loss 0.361318\n",
      "batch 803: loss 0.272371\n",
      "batch 804: loss 0.194649\n",
      "batch 805: loss 0.228653\n",
      "batch 806: loss 0.073997\n",
      "batch 807: loss 0.078434\n",
      "batch 808: loss 0.080558\n",
      "batch 809: loss 0.242290\n",
      "batch 810: loss 0.091292\n",
      "batch 811: loss 0.138285\n",
      "batch 812: loss 0.091180\n",
      "batch 813: loss 0.179218\n",
      "batch 814: loss 0.141747\n",
      "batch 815: loss 0.115830\n",
      "batch 816: loss 0.429279\n",
      "batch 817: loss 0.118517\n",
      "batch 818: loss 0.129214\n",
      "batch 819: loss 0.295995\n",
      "batch 820: loss 0.211915\n",
      "batch 821: loss 0.278601\n",
      "batch 822: loss 0.165029\n",
      "batch 823: loss 0.109803\n",
      "batch 824: loss 0.319858\n",
      "batch 825: loss 0.266920\n",
      "batch 826: loss 0.215252\n",
      "batch 827: loss 0.243970\n",
      "batch 828: loss 0.084518\n",
      "batch 829: loss 0.150371\n",
      "batch 830: loss 0.233750\n",
      "batch 831: loss 0.237565\n",
      "batch 832: loss 0.224506\n",
      "batch 833: loss 0.158934\n",
      "batch 834: loss 0.345590\n",
      "batch 835: loss 0.179551\n",
      "batch 836: loss 0.212910\n",
      "batch 837: loss 0.215758\n",
      "batch 838: loss 0.076378\n",
      "batch 839: loss 0.223863\n",
      "batch 840: loss 0.295404\n",
      "batch 841: loss 0.315761\n",
      "batch 842: loss 0.025703\n",
      "batch 843: loss 0.152289\n",
      "batch 844: loss 0.391957\n",
      "batch 845: loss 0.066346\n",
      "batch 846: loss 0.102425\n",
      "batch 847: loss 0.186318\n",
      "batch 848: loss 0.221481\n",
      "batch 849: loss 0.229764\n",
      "batch 850: loss 0.283102\n",
      "batch 851: loss 0.220147\n",
      "batch 852: loss 0.202496\n",
      "batch 853: loss 0.114962\n",
      "batch 854: loss 0.353847\n",
      "batch 855: loss 0.298502\n",
      "batch 856: loss 0.132925\n",
      "batch 857: loss 0.175246\n",
      "batch 858: loss 0.299651\n",
      "batch 859: loss 0.126259\n",
      "batch 860: loss 0.082699\n",
      "batch 861: loss 0.228258\n",
      "batch 862: loss 0.131619\n",
      "batch 863: loss 0.260977\n",
      "batch 864: loss 0.300338\n",
      "batch 865: loss 0.286650\n",
      "batch 866: loss 0.125282\n",
      "batch 867: loss 0.114974\n",
      "batch 868: loss 0.138582\n",
      "batch 869: loss 0.106849\n",
      "batch 870: loss 0.130448\n",
      "batch 871: loss 0.448693\n",
      "batch 872: loss 0.167275\n",
      "batch 873: loss 0.267536\n",
      "batch 874: loss 0.100267\n",
      "batch 875: loss 0.107952\n",
      "batch 876: loss 0.140116\n",
      "batch 877: loss 0.209630\n",
      "batch 878: loss 0.174602\n",
      "batch 879: loss 0.153426\n",
      "batch 880: loss 0.135727\n",
      "batch 881: loss 0.141014\n",
      "batch 882: loss 0.226104\n",
      "batch 883: loss 0.222789\n",
      "batch 884: loss 0.104761\n",
      "batch 885: loss 0.198724\n",
      "batch 886: loss 0.107865\n",
      "batch 887: loss 0.044810\n",
      "batch 888: loss 0.228679\n",
      "batch 889: loss 0.213417\n",
      "batch 890: loss 0.188364\n",
      "batch 891: loss 0.149705\n",
      "batch 892: loss 0.176128\n",
      "batch 893: loss 0.240036\n",
      "batch 894: loss 0.064803\n",
      "batch 895: loss 0.365131\n",
      "batch 896: loss 0.221119\n",
      "batch 897: loss 0.152608\n",
      "batch 898: loss 0.321049\n",
      "batch 899: loss 0.260147\n",
      "batch 900: loss 0.121138\n",
      "batch 901: loss 0.176446\n",
      "batch 902: loss 0.183046\n",
      "batch 903: loss 0.136904\n",
      "batch 904: loss 0.271941\n",
      "batch 905: loss 0.156814\n",
      "batch 906: loss 0.155831\n",
      "batch 907: loss 0.275119\n",
      "batch 908: loss 0.300710\n",
      "batch 909: loss 0.157268\n",
      "batch 910: loss 0.364333\n",
      "batch 911: loss 0.071783\n",
      "batch 912: loss 0.152644\n",
      "batch 913: loss 0.098575\n",
      "batch 914: loss 0.286499\n",
      "batch 915: loss 0.278813\n",
      "batch 916: loss 0.166170\n",
      "batch 917: loss 0.130474\n",
      "batch 918: loss 0.117130\n",
      "batch 919: loss 0.170786\n",
      "batch 920: loss 0.150422\n",
      "batch 921: loss 0.145503\n",
      "batch 922: loss 0.256243\n",
      "batch 923: loss 0.086782\n",
      "batch 924: loss 0.194477\n",
      "batch 925: loss 0.056321\n",
      "batch 926: loss 0.072807\n",
      "batch 927: loss 0.127069\n",
      "batch 928: loss 0.302527\n",
      "batch 929: loss 0.174031\n",
      "batch 930: loss 0.301922\n",
      "batch 931: loss 0.215195\n",
      "batch 932: loss 0.054122\n",
      "batch 933: loss 0.131741\n",
      "batch 934: loss 0.189028\n",
      "batch 935: loss 0.335182\n",
      "batch 936: loss 0.336513\n",
      "batch 937: loss 0.190710\n",
      "batch 938: loss 0.281214\n",
      "batch 939: loss 0.083474\n",
      "batch 940: loss 0.197001\n",
      "batch 941: loss 0.231478\n",
      "batch 942: loss 0.381923\n",
      "batch 943: loss 0.096404\n",
      "batch 944: loss 0.230474\n",
      "batch 945: loss 0.199346\n",
      "batch 946: loss 0.245169\n",
      "batch 947: loss 0.240132\n",
      "batch 948: loss 0.313651\n",
      "batch 949: loss 0.122265\n",
      "batch 950: loss 0.430037\n",
      "batch 951: loss 0.217166\n",
      "batch 952: loss 0.166316\n",
      "batch 953: loss 0.125987\n",
      "batch 954: loss 0.427378\n",
      "batch 955: loss 0.092473\n",
      "batch 956: loss 0.221199\n",
      "batch 957: loss 0.303022\n",
      "batch 958: loss 0.076928\n",
      "batch 959: loss 0.089373\n",
      "batch 960: loss 0.396412\n",
      "batch 961: loss 0.073119\n",
      "batch 962: loss 0.194308\n",
      "batch 963: loss 0.106913\n",
      "batch 964: loss 0.121268\n",
      "batch 965: loss 0.264452\n",
      "batch 966: loss 0.201560\n",
      "batch 967: loss 0.274042\n",
      "batch 968: loss 0.252801\n",
      "batch 969: loss 0.178279\n",
      "batch 970: loss 0.209873\n",
      "batch 971: loss 0.079560\n",
      "batch 972: loss 0.102765\n",
      "batch 973: loss 0.350852\n",
      "batch 974: loss 0.085586\n",
      "batch 975: loss 0.147780\n",
      "batch 976: loss 0.089989\n",
      "batch 977: loss 0.124897\n",
      "batch 978: loss 0.208113\n",
      "batch 979: loss 0.298695\n",
      "batch 980: loss 0.265634\n",
      "batch 981: loss 0.162299\n",
      "batch 982: loss 0.280519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 983: loss 0.193974\n",
      "batch 984: loss 0.116535\n",
      "batch 985: loss 0.162713\n",
      "batch 986: loss 0.259518\n",
      "batch 987: loss 0.252976\n",
      "batch 988: loss 0.216662\n",
      "batch 989: loss 0.246712\n",
      "batch 990: loss 0.096448\n",
      "batch 991: loss 0.069280\n",
      "batch 992: loss 0.219371\n",
      "batch 993: loss 0.064951\n",
      "batch 994: loss 0.110601\n",
      "batch 995: loss 0.179355\n",
      "batch 996: loss 0.209331\n",
      "batch 997: loss 0.133752\n",
      "batch 998: loss 0.091179\n",
      "batch 999: loss 0.121766\n"
     ]
    }
   ],
   "source": [
    "num_batches = 1000\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "log_dir = 'tensorboard'\n",
    "model = MLP()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)     # 实例化记录器\n",
    "tf.summary.trace_on(profiler=True)  # 开启Trace（可选）\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "        with summary_writer.as_default():                           # 指定记录器\n",
    "            tf.summary.scalar(\"loss\", loss, step=batch_index)       # 将当前损失函数的值写入记录器\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Administrator\\\\TensorFlow'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
